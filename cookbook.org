#+date: time-stamp: <2012-04-27 11:34>
#+Title: Procedures and Scripts for the Data Analysis
#+Author: Salvatore Enrico Paolo Indiogine

#+tags: noexport(n) chp(c) sec(s) subsec(b) temp(t)
#+latex_class: book
#+latex_header: \usepackage[margin=2.5cm]{geometry}
#+latex_header: \graphicspath{/home/henk/dissertation/graphic/}
#+language: en
#+description: dissertation procedures and computer scripts
#+todo: VOID(v) STRT(s) COMP(c) REVD(r) DONE(d)
#+options: h:3 num:t toc:1 \n:nil @:t ::t |:t ^:nil -:t f:t *:t <:t
#+options: tex:t latex:t skip:nil d:nil todo:nil pri:nil tags:nil
#+export_select_tags: export
#+export_exclude_tags: noexport

# confrontare con ultimo codice in dissertazione, e nel direttorio.

* Procedures and Computer code
** DONE Search FDsys and create URL lists				:chp:
The whole process begins with the search on the FDsys website for the terms "education", "achievement gap" and "math".  The web site created search results pages that I will save to one or more =HTML= files.  I then extract the URLs from these =HTML= pages and create my URL lists.  These lists were used by the scripts that will download the documents.  The repeating items will be removed and then sorted, and counted so that they can be checked with the totals given for the searches on the FDsys website.

Some searches resulted in more than one page.  Thus I needed to splice these pages into a single HTML file.  The scripts will perform the merges.

In the document collection congressional hearings this search produced 217 results, which I considered excessive.  Thus, I have made the search more restrictive for this collection.  I performed the following three searches

1. "education" and "achievement gap" and "math" and "TIMSS"
2. "education" and "achievement gap" and "math" and "NAEP"
2. "education" and "achievement gap" and "math" and "PISA"

The first search yielded 26 results, the second one 86, and the third one 24.  I merged the three results.  The total extracted URLs was 129 instead of 136 because the search results do partially overlap.  Thus, the script will remove duplicate URLs.

*** DONE Congressional hearings						:sec:

The search results are downloaded as =congr-hearings-url.html=. The log file is =congr-hearings-url.log= and the output file is =congr-hearings-url.lst=. The script will extract the URLs for the PDF files.  Then it converts it to the URL for the html files.  There were three searches resulted into one HTML pages each that have to be combined into a single one. Then, the duplicates will be removed.

#+srcname: congr-hearings-url
#+begin_src sh :tangle ~/Dropbox/dissertation/data/congr-hearings-url.sh :results output
#!/bin/bash

# define variables
home_dir=$home/Dropbox/dissertation/data
log_file=$home_dir/congr-hearings-url.log
imp_file=$home_dir/congr-hearings-url.html
out_file=$home_dir/congr-hearings-url.lst

# combine the 2 html pages into a single one temporary file
cd $home_dir
cat congr-hearings-url-1.html \
    congr-hearings-url-2.html \
    congr-hearings-url-3.html > file-1.tmp
  
# extract the PDF URLs  
cd $home_dir
echo "begin extraction of urls" | tee $log_file
grep -o 'http://www.gpo.gov/fdsys/pkg/.*pdf" ' file-1.tmp > file-2.tmp
grep -o 'http://www.gpo.gov/fdsys/pkg/.*htm" ' file-2.tmp > file-3.tmp
cat file-2.tmp file-3.tmp > file-4.tmp
echo "ended extraction of urls" | tee -a $log_file
  
# convert from pdf to html urls
# 1. replace first 'pdf' to 'html'
sed 's/pdf/html/' file-4.tmp > file-5.tmp

# 2. replace second 'pdf' to 'htm'
sed 's/pdf"/htm/' file-5.tmp > file-6.tmp

# 3. replace 'htm"' with 'htm'
sed 's/htm"/htm/' file-6.tmp > file-7.tmp
  
# count lines in list - before removing duplicates
echo "there are" `wc -l < file-7.tmp` "urls in the list" | tee -a $log_file

# remove duplicates and sort by file name, then delete temp files
cat file-7.tmp | sort | uniq > $out_file
rm file-1.tmp file-2.tmp file-3.tmp file-4.tmp file-5.tmp file-6.tmp file-7.tmp

# count lines in list - after removing duplicates
echo "the output file has" `wc -l < $out_file` "urls" | tee -a $log_file
#+end_src

#+results: congr-hearings-url
#+begin_example
begin extraction of urls
ended extraction of urls
there are 129 urls in the list
the output file has 89 urls
#+end_example

*** DONE Presidential documents						:sec:

The search results were downloaded as =pres-docs-url.html=. The log file is =pres-docs-url.log= and the output file is =pres-docs-url.lst=. The script will extract the URLs for the PDF files.  Then it converts it to the URL for the html files.   The search resulted into two HTML pages that have to be combined into a single one.

#+srcname: pres-docs-url
#+begin_src sh :tangle ~/Dropbox/dissertation/data/pres-docs-url.sh :results output
#!/bin/bash

# define variables
home_dir=$home/Dropbox/dissertation/data
log_file=$home_dir/pres-docs-url.log
imp_file=$home_dir/pres-docs-url.html
out_file=$home_dir/pres-docs-url.lst

# combine the 2 html pages into a single one
cd $home_dir
cat pres-docs-url-1.html pres-docs-url-2.html > $imp_file
  
# extract the pdf urls  
cd $home_dir
echo "begin extraction of urls" | tee $log_file
grep -o 'http://www.gpo.gov/fdsys/pkg/.*pdf" ' $imp_file > file-1.tmp
echo "ended extraction of urls" | tee -a $log_file
  
# convert from pdf to html urls
# 1. replace first 'pdf' to 'html'
sed 's/pdf/html/' file-1.tmp > file-2.tmp

# 2. replace second 'pdf' to 'htm'
sed 's/pdf"/htm/' file-2.tmp > file-3.tmp

# remove duplicates and sort by file name, then delete temp files
cat file-3.tmp | sort | uniq > $out_file
rm file-1.tmp file-2.tmp file-3.tmp
  
# count lines in list
echo "the output file has" `wc -l < pres-docs-url.lst` "urls" | tee -a $log_file
#+end_src

#+results: pres-docs-url
#+begin_example
begin extraction of urls
ended extraction of urls
the output file has 127 urls
#+end_example

*** DONE Congressional bills				       :noexport:sec:

The search results are downloaded as =congr-bills-url.html=. the log file is =congr-bills-url.log= and the output file is =congr-bills-url.lst=. the script will extract the urls for the pdf files.  then it converts it to the url for the html files.   the search resulted one html page.

#+srcname: congr-bills-url
#+begin_src sh :tangle ~/Dropbox/dissertation/data/congr-bills-url.sh :results output
#!/bin/bash

# define variables
home_dir=$home/Dropbox/dissertation/data
log_file=$home_dir/congr-bills-url.log
imp_file=$home_dir/congr-bills-url.html
out_file=$home_dir/congr-bills-url.lst
  
# extract the pdf urls  
cd $home_dir
echo "begin extraction of urls" | tee $log_file
grep -o 'http://www.gpo.gov/fdsys/pkg/.*pdf" ' $imp_file > file-1.tmp
echo "ended extraction of urls" | tee -a $log_file
  
# convert from pdf to html urls
# 1. replace first 'pdf' to 'html'
sed 's/pdf/html/' file-1.tmp > file-2.tmp

# 2. replace second 'pdf' to 'htm'
sed 's/pdf"/htm/' file-2.tmp > file-3.tmp

# remove duplicates and sort by file name, then delete temp files
cat file-3.tmp | sort | uniq > $out_file
rm file-1.tmp file-2.tmp file-3.tmp
  
# count lines in list
echo "the output file has" `wc -l < congr-bills-url.lst` "urls" | tee -a $log_file
#+end_src

#+results: congr-bills-url
#+begin_example
begin extraction of urls
ended extraction of urls
the output file has 62 urls
#+end_example

*** DONE ERIC collection of documents			       :noexport:sec:

The search results were downloaded as =eric-docs-url.html=. the log file is =eric-docs-url.log= and the output file is =eric-docs-url.lst=. the script will extract the urls for the pdf files. Then it converts it to the url for the html files.

#+srcname: eric-docs-url
#+begin_src sh :tangle ~/Dropbox/dissertation/data/eric-docs-url.sh :results output
#!/bin/bash

# define variables
home_dir=$home/Dropbox/dissertation/data
log_file=$home_dir/eric-docs-url.log
imp_file=$home_dir/eric-docs-url.html
out_file=$home_dir/eric-docs-url.lst

# combine the 2 html pages into a single one
cd $home_dir
cat eric-docs-url-1.html eric-docs-url-2.html > $imp_file
  
# extract the pdf urls  
cd $home_dir
echo "begin extraction of urls" | tee $log_file
grep -o 'http://www.gpo.gov/fdsys/pkg/.*pdf" ' $imp_file > file-1.tmp
echo "ended extraction of urls" | tee -a $log_file
  
# convert from pdf to html urls
# 1. replace first 'pdf' to 'html'
sed 's/pdf/html/' file-1.tmp > file-2.tmp

# 2. replace second 'pdf' to 'htm'
sed 's/pdf"/htm/' file-2.tmp > file-3.tmp

# remove duplicates and sort by file name, then delete temp files
cat file-3.tmp | sort | uniq > $out_file
rm file-1.tmp file-2.tmp file-3.tmp
  
# count lines in list
echo "the output file has" `wc -l < eric-docs-url.lst` "urls" | tee -a $log_file
#+end_src

** DONE Download source documents and convert to plain text		:chp:

The following are /bash/ UNIX scripts that perform the operations corresponding to the steps "download source files" and "convert to plain text" of the work flow diagram.

The /bash/ scripts download the data from FDsys, the federal government on-line database, place the files in the appropriate directory, and then to convert them into plain text files.  The scripts also create a log file that can be inspected.  The script will provide the number of downloaded files in each directory.  The scripts downloaded using a file with the list of all URLs.  This URL list file was created from the fdsys search results.

The scripts can be placed anywhere, but for convenience I keep all of them in =data= base directory.  The following table contains the number and type of downloaded source data files

# tangle: c-c c-v t
# evaluate: c-c c-c

*** DONE Congressional hearings						:sec:

The script =congr-hearings-download.sh= downloaded the HTML files to the appropriate directory and then it removed the HTML tags to convert the files into plain text. The original HTML files will be kept.  The log file =congr-hearings-download.log= was created.

Note: chrg-109shrg49104171.html and chrg-109shrg99869.html are duplicates.  They both correspond to hearing 109-284.  I deleted the second file.

#+srcname: congr-hearings-download
#+begin_src sh :tangle ~/Dropbox/dissertation/data/congr-hearings-download.sh :results output
#!/bin/bash
  
# define variables
home_dir=$home/Dropbox/dissertation/data
data_dir=$home_dir/congr-hearings
log_file=$home_dir/congr-hearings-download.log
imp_file=$home_dir/congr-hearings-url.lst
rc_file=$home_dir/html2textrc
    
echo "data directory" $data_dir | tee $log_file
echo "log file" $log_file | tee -a $log_file
    
echo "begin download of source files" | tee -a $log_file  
wget -nc -c --tries=75 --directory-prefix=$data_dir \
            --append-output=$log_file \
            --input-file=$imp_file
echo "ended download of source files" | tee -a $log_file
    
# convert html to plain text
cd $data_dir
echo "number of html files:" `ls -1 *.htm | wc -l` | tee -a $log_file
echo "beginning html -> txt conversion" | tee -a $log_file
for file in *.htm
do
  html2text -width 2000 -rcfile $rc_file -o "${file%.htm}.txt" "$file"
done
echo "ended html -> txt conversion" | tee -a $log_file
    
# count number of text files in directory
echo "number of text files:" `ls -1 *.txt | wc -l` | tee -a $log_file
#+end_src

#+results:
#+begin_example
data directory /home/henk/Dropbox/dissertation/data/congr-hearings
log file /home/henk/Dropbox/dissertation/data/congr-hearings-download.log
begin download of source files
ended download of source files
number of html files: 89
beginning html -> txt conversion
ended html -> txt conversion
number of text files: 89
#+end_example

#+results: congr-hearings-download
#+begin_example
data directory /home/henk/Dropbox/dissertation/data/congr-hearings
log file /home/henk/Dropbox/dissertation/data/congr-hearings-download.log
begin download of source files
ended download of source files
number of html files: 89
beginning html -> txt conversion
ended html -> txt conversion
number of text files: 89
#+end_example

# later change from "htm" to "txt"

#+srcname: congr-hearings-no
#+begin_src sh :results value
cd $home/Dropbox/dissertation/data/congr-hearings
echo `ls -l *.htm | wc -l`
#+end_src

#+results: congr-hearings-no
#+begin_example
89
#+end_example

*** DONE Presidential documents						:sec:

The script =pres_docs-download.sh= downloaded the HTML files to the appropriate directory and then it removed the HTML tags to convert the files into plain text.  The original HTML files will be kept.  The log file =pres_docs-download.log= was created.

#+srcname: pres-docs-download
#+begin_src sh :tangle ~/Dropbox/dissertation/data/pres-docs-download.sh :results output
#!/bin/bash
  
# define variables
home_dir=$home/Dropbox/dissertation/data
data_dir=$home_dir/pres-docs
log_file=$home_dir/pres-docs-download.log
imp_file=$home_dir/pres-docs-url.lst
rc_file=$home_dir/html2textrc
    
echo "data directory" $data_dir | tee $log_file
echo "log file" $log_file | tee -a $log_file
    
echo "begin download of source files" | tee -a $log_file  
wget -nc -c --tries=75 --directory-prefix=$data_dir \
            --append-output=$log_file \
            --input-file=$imp_file
echo "ended download of source files" | tee -a $log_file
    
# convert html to plain text
cd $data_dir
    
echo "beginning html -> txt conversion" | tee -a $log_file
for file in *.htm
do
  html2text -width 2000 -rcfile $rc_file -o "${file%.htm}.txt" "$file"
done
echo "ended html -> txt conversion" | tee -a $log_file
    
# count number of text files in directory
echo "number of text files:" `ls -1 *.txt | wc -l` | tee -a $log_file
#+end_src

#+results: pres-docs-download
#+begin_example
data directory /home/henk/Dropbox/dissertation/data/pres-docs
log file /home/henk/Dropbox/dissertation/data/pres-docs-download.log
begin download of source files
ended download of source files
beginning html -> txt conversion
ended html -> txt conversion
number of text files: 127
#+end_example

#+srcname: pres-docs-no
#+begin_src sh :results value
cd $home/Dropbox/dissertation/data/pres-docs
echo `ls -l *.htm | wc -l`
#+end_src

#+results: pres-docs-no
#+begin_example
127
#+end_example

** DONE Build tables of file descriptions 				:chp:

In the previous step I downloaded and converted into plain text files the data sources.  In this step I built the tables of file description of the downloaded data sources.  This step was necessary to start the analysis of the data sources.  These tables were later loaded into the QDA application using /R/ scripts. 

The resulting tables are shown later and the /bash/ UNIX scripts that helped me in building these tables are shown here below.

The output was used to build the description of files in the tables for the Presidential Documents.  Also another table was produced, but it is too large to be displayed here.  This table was then exported as a "tab separated value" file and used to populate the file attributes table in the /RQDA/ project database for the Presidential documents.

The same steps were done for the Congressional Hearings.  The output of another /bash/ script was used to build the description of files in later tables.  Also another large table was produced.  This table was then exported as a "tab separated value" file and used to populate the file attributes table in the /RQDA/ project database for the Congressional hearings.

The following sections contain /bash/ UNIX scripts used to build the tables shown later.

*** DONE Presidential documents						:sec:
  :PROPERTIES:
  :table_export_file: ~/Dropbox/dissertation/data/pres-docs.tsv
  :table_export_format: orgtbl-to-tsv
  :END:

# Alt-x org-table-export

The following script is used to extract the file IDs from the files in the =pres-docs= data directory

#+begin_src sh :results output
#!/bin/bash

home_dir=$home/Dropbox/dissertation/data
data_dir=$home_dir/pres-docs

cd $data_dir
for file in *.scrb
do
  echo ${file%.*}
done
#+end_src

The output is used to build the description of files table (\ref{tbl:pres-docs}).  The table is then exported as a "tab separated value" file and use to populate the file attributes table in the /RQDA/ project database.

#+tblname: pres-docs
#+caption: description of the presidential documents
#+label: tbl:pres-docs
| serial no.               |       date | author                        | title                                                                 |
|--------------------------+------------+-------------------------------+-----------------------------------------------------------------------|
| dcpd-200900575           | 2009-07-16 | Barack Obama                  | Remarks celebrating the 100th anniversary of the NAACP in NYC         |
| dcpd-200900595           | 2009-07-24 | Barack Obama                  | Remarks on education reform                                           |
| dcpd-200900884           | 2009-11-04 | Barack Obama                  | Remarks to students, faculty, and parents at James C. Wright ...      |
| dcpd-201000036           | 2010-01-19 | Barack Obama                  | Remarks at Graham Road Elementary School in Falls Church, VA          |
| dcpd-201000130           | 2010-02-26 | Barack Obama                  | Remarks on signing an executive order regarding historically ...      |
| dcpd-201000636           | 2010-07-29 | Barack Obama                  | Remarks at the national urban league centennial conference            |
| dcpd-201000812           | 2010-09-29 | Barack Obama                  | Remarsk and a question-and-answer session in Richmond, VA             |
| dcpd-201100172           | 2011-03-14 | Barack Obama                  | Remarks at Kenmore Middle School in Arlington, VA                     |
| wcpd-1999-05-31-pg964    | 1999-05-31 | William Clinton               | Message to congress transmitting the proposed 'educational ...        |
| wcpd-2000-06-19-pg1366-4 | 2000-01-15 | William Clinton               | Remarks at the White House strategy session on improving Hispanic ... |

| wcpd-2001-01-29-pg217    | 2001-01-23 | George W. Bush                | Remarks on submitting the education plan to congress                  |
| wcpd-2002-01-14-pg36     | 2002-01-09 | George W. Bush                | Remarks on implementation of No Child Left Behind Act of 2001         |
| wcpd-2002-04-08-pg551-2  | 2002-04-02 | George W. Bush                | Remarks at Pennsylvania State University, Delaware County, in media   |
| wcpd-2003-01-13-pg39     | 2003-01-08 | George W. Bush                | Remarks on the anniversary of No Child Left Behind Act                |
| wcpd-2003-08-04-pg984-2  | 2003-07-28 | George W. Bush                | Remarks to the National Urban League Conference in Pittsburgh, PA     |
| wcpd-2004-01-12-pg28     | 2004-01-08 | george w. bush                | remarks in a discussion at west view elementary school in knoxville,  |
| wcpd-2004-05-17-pg856    | 2004-05-11 | george w. bush                | remarks at butterfield junior high school in van buren, arkansas      |
| wcpd-2004-08-16-pg1561   | 2004-08-11 | george w. bush                | remarks in phoenix, arizona                                           |
| wcpd-2004-08-23-pg1587   | 2004-08-13 | george w. bush                | remarks in a discussion at southridge high school in beaverton, or    |
| wcpd-2004-08-23-pg1631   | 2004-08-17 | george w. bush                | remarks in hedgeville, west virginia                                  |
| wcpd-2004-08-23-pg1644-2 | 2004-08-18 | george w. bush                | remarks in discussion in hudson, wisconsin                            |
| wcpd-2004-08-30-pg1669   | 2004-08-21 | george w. bush                | the president's radio address                                         |
| wcpd-2004-08-30-pg1679   | 2004-08-26 | george w. bush                | remarks in farmington, new mexico                                     |
| wcpd-2004-09-06-pg1720   | 2004-08-28 | george w. bush                | remarks in troy, ohio                                                 |
| wcpd-2004-09-06-pg1727   | 2004-08-28 | george w. bush                | remarks in a discussion in lima, ohio                                 |
| wcpd-2004-09-06-pg1750   | 2004-08-29 | george w. bush                | remarks in wheeling, west virginia                                    |
| wcpd-2004-09-06-pg1757   | 2004-08-30 | george w. bush                | remarks in discussion in nashua, new hampshire                        |
| wcpd-2004-09-06-pg1773   | 2004-08-30 | george w. bush                | remarks in taylor, michigan                                           |
| wcpd-2004-09-06-pg1790   | 2004-09-01 | george w. bush                | remarks in columbus, ohio                                             |
| wcpd-2004-09-13-pg1819   | 2004-09-03 | george w. bush                | remarks in cedar rapids, iowa                                         |
| wcpd-2004-09-13-pg1839-2 | 2004-09-04 | george w. bush                | remarks in kirkland, ohio                                             |
| wcpd-2004-09-13-pg1851   | 2004-09-05 | george w. bush                | remarks in parkersburg, west virginia                                 |
| wcpd-2004-09-13-pg1863-2 | 2004-09-07 | george w. bush                | remarks in lee's summit, missouri                                     |

| wcpd-2004-09-13-pg1869   | 2004-09-07 | george w. bush                | remarks in a discussion in sedalia, missouri                          |
| wcpd-2004-09-20-pg2000   | 2004-09-16 | george w. bush                | remarks in st. cloud, minnesota                                       |
| wcpd-2004-09-20-pg2025   | 2004-09-17 | george w. bush                | remarks at a victory committee reception                              |
| wcpd-2004-09-27-pg2085   | 2004-09-22 | george w. bush                | remarks in a discussion on education in king of prussia, pennsylvania |
| wcpd-2004-09-27-pg2097   | 2004-09-22 | george w. bush                | remarks in latrobe, pennsylvania                                      |
| wcpd-2004-09-27-pg2126-2 | 2004-09-24 | george w. bush                | remarks in a discussion on education in janesville                    |
| wcpd-2004-10-04-pg2152-2 | 2004-09-27 | george w. bush                | remarks in a discussion on education in springfield, ohio             |
| wcpd-2004-10-11-pg2223   | 2004-10-02 | george w. bush                | remarks in a discussion in mansfield, ohio                            |
| wcpd-2004-10-11-pg2244   | 2004-10-04 | george w. bush                | clive, iowa                                                           |
| wcpd-2004-10-11-pg2276   | 2004-10-07 | george w. bush                | remarks in wausau, wisconsin                                          |
| wcpd-2004-10-18-pg2312   | 2004-10-09 | george w. bush                | remarks at a breakfast for gubernatorial candidate matt blunt ...     |
| wcpd-2004-10-18-pg2330   | 2004-10-11 | george w. bush                | remarks in hobbs, new mexico                                          |
| wcpd-2004-10-18-pg2338   | 2004-10-11 | george w. bush                | remarks at a luncheon for senatorial candidate pete coors in denver,  |
| wcpd-2004-10-18-pg2344   | 2004-10-11 | george w. bush                | remarks in morrison, colorado                                         |
| wcpd-2004-10-18-pg2364   | 2004-10-13 | George W. Bush and John Kerry | Presidential debate in Tempe, Arizona                                 |
| wcpd-2004-10-18-pg2387   | 2004-10-14 | George W. Bush                | Remarks in Las Vegas, Nevada                                          |
| wcpd-2004-10-18-pg2393   | 2004-10-14 | George W. Bush                | Remarks in Reno, Nevada                                               |
| wcpd-2004-10-18-pg2399   | 2004-10-14 | George W. Bush                | Remarks in Central Point, Oregon                                      |
| wcpd-2004-10-18-pg2405   | 2004-10-15 | George W. Bush                | Remarks in Cedar Rapids, Iowa                                         |
| wcpd-2004-10-25-pg2425   | 2004-10-16 | George W. Bush                | Remarks in Sunrise, Florida                                           |
| wcpd-2004-10-25-pg2455-2 | 2004-10-19 | George W. Bush                | Remarks in St. Petersburg, Florida                                    |
| wcpd-2004-10-25-pg2522-2 | 2004-10-22 | George W. Bush                | Remarks in Wilkes-Barre, Pennsylvania                                 |

| wcpd-2004-11-01-pg2543   | 2004-10-23 | George W. Bush                | Remarks in Fort Myers, Florida                                        |
| wcpd-2004-11-01-pg2549   | 2004-10-23 | george w. bush                | remarks in lakeland, florida                                          |
| wcpd-2004-11-01-pg2555   | 2004-10-23 | george w. bush                | remarks in melbourne, florida                                         |
| wcpd-2004-11-01-pg2561   | 2004-10-23 | george w. bush                | remarks in jacksonville, florida                                      |
| wcpd-2004-11-01-pg2567   | 2004-10-24 | george w. bush                | remarks in alamogordo, new mexico                                     |
| wcpd-2004-11-01-pg2628   | 2004-10-27 | george w. bush                | remarks in vienna, ohio                                               |
| wcpd-2004-11-01-pg2647   | 2004-10-28 | george w. bush                | remarks in saginaw, michigan                                          |
| wcpd-2004-11-01-pg2654   | 2004-10-28 | george w. bush                | remarks in dayton, ohio                                               |
| wcpd-2004-11-01-pg2660   | 2004-10-28 | george w. bush                | remarks in westlake, ohio                                             |
| wcpd-2004-11-01-pg2667   | 2004-10-28 | george w. bush                | remarks in yardley, pennsylvania                                      |
| wcpd-2004-11-01-pg2679   | 2004-10-29 | george w. bush                | remarks in portsmouth, new hampshire                                  |
| wcpd-2004-11-08-pg2689   | 2004-10-29 | george w. bush                | remarks in toledo, ohio                                               |
| wcpd-2004-11-08-pg2695-3 | 2004-10-29 | george w. bush                | remarks in columbus, ohio                                             |

| wcpd-2004-11-08-pg2708   | 2004-10-30 | george w. bush                | remarks in ashwaubenon, wisconsin                                     |
| wcpd-2004-11-08-pg2715   | 2004-10-30 | george w. bush                | remarks in minneapolis, minnesota                                     |
| wcpd-2004-11-08-pg2727   | 2004-10-31 | george w. bush                | remarks in miami. florida                                             |
| wcpd-2004-11-08-pg2732   | 2004-10-31 | george w. bush                | remarks in tampa, florida                                             |
| wcpd-2004-11-08-pg2737   | 2004-10-31 | george w. bush                | remarks in gainesville, florida                                       |
| wcpd-2004-11-08-pg2742   | 2004-10-31 | george w. bush                | remarks in cincinnati, ohio                                           |
| wcpd-2004-11-08-pg2747   | 2004-11-01 | george w. bush                | remarks in wilmington, ohio                                           |
| wcpd-2004-11-08-pg2752-2 | 2004-11-01 | george w. bush                | remarks in burgettstown, pennsylvania                                 |

| wcpd-2004-11-08-pg2758   | 2004-11-01 | george w. bush                | remarks in milwaukee, wisconsin                                       |
| wcpd-2004-11-08-pg2763   | 2004-11-01 | george w. bush                | remarks in des moines, iowa                                           |
| wcpd-2004-11-08-pg2768   | 2004-11-01 | george w. bush                | remarks in sioux city, iowa                                           |
| wcpd-2005-01-17-pg45     | 2005-01-12 | george w. bush                | remarks at j.e.b. stuart high school in falls church, virginia        |
| wcpd-2005-02-07-pg122-2  | 2005-01-31 | george w. bush                | remarks at a swearing-in ceremony for margaret spellings as ...       |
| wcpd-2005-02-14-pg187-2  | 2005-02-08 | george w. bush                | remarks to the detroit economic club in detroit, michigan             |
| wcpd-2005-03-07-pg340    | 2005-03-02 | george w. bush                | remarks in a discussion on job training in arnold, maryland           |
| wcpd-2005-03-21-pg440    | 2005-03-15 | george w. bush                | remarks at the national republican congressional committee dinner     |
| wcpd-2005-04-25-pg634    | 2005-04-20 | george w. bush                | remarks honoring the 2005 national and state teachers of the year     |
| wcpd-2005-06-27-pg1043   | 2005-06-22 | george w. bush                | remarks at calvert cliffs nuclear power plant in lusby, maryland      |
| wcpd-2005-07-18-pg1158   | 2005-07-14 | george w. bush                | remarks at the indiana black expo corporate luncheon in indianapolis, |
| wcpd-2005-10-24-pg1559   | 2005-10-19 | george w. bush                | remarks following a meeting with secretary of education margaret ...  |
| wcpd-2005-10-31-pg1600   | 2005-10-26 | george w. bush                | remarks to the economic club of washington, dc                        |

| wcpd-2006-01-09-pg12     | 2006-01-06 | george w. bush                | remarks to the economic club of chicago, illinois                     |
| wcpd-2006-01-16-pg26-2   | 2006-01-09 | george w. bush                | remarks on the nclb act in glen burnie, maryland                      |
| wcpd-2006-01-16-pg40-2   | 2006-01-11 | george w. bush                | remarks on the war on terror and a question-and-answer session ...    |
| wcpd-2006-01-23-pg80-2   | 2006-01-19 | george w. bush                | remarks on the national economy and a question-and-answer session ... |
| wcpd-2006-02-27-pg320    | 2006-02-22 | george w. bush                | remarks at a celebration of african american history month            |
| wcpd-2006-03-13-pg434    | 2006-03-10 | george w. bush                | remarks at the national newspaper association government affairs ...  |
| wcpd-2006-03-27-pg498    | 2006-03-20 | george w. bush                | remarks to the city club of cleveland and a question-and-answer ...   |
| wcpd-2006-04-24-pg725    | 2006-04-18 | george w. bush                | remarks at parkland magnet middle school for aerospace technology ... |
| wcpd-2006-04-24-pg734    | 2006-04-19 | george w. bush                | remarks at tuskegee university in tuskegee, alabama                   |
| wcpd-2006-05-01-pg751    | 2006-04-21 | george w. bush                | remarks in a discussion at cisco systems, inc., in san jose, ca       |

| wcpd-2006-05-01-pg769-2  | 2006-04-24 | george w. bush                | remarks on immigration reforms and a question-and-answer session ...  |
| wcpd-2006-05-01-pg798    | 2006-04-26 | george w. bush                | remarks honoring the 2006 national and state teachers of the year     |
| wcpd-2006-05-08-pg838    | 2006-05-03 | george w. bush                | remarks to the american council of engineering companies              |
| wcpd-2006-05-29-pg965-2  | 2006-05-19 | george w. bush                | remarks on american competitiveness in highland heights, kentucky     |
| wcpd-2006-07-31-pg1396   | 2006-07-27 | george w. bush                | remarks to the national association of manufacturers                  |
| wcpd-2006-10-09-pg1750   | 2006-10-05 | george w. bush                | remarks at woodridge elementary and middle campus                     |
| wcpd-2006-10-09-pg1758   | 2006-10-06 | george w. bush                | remarks at a reception celebrating hispanic heritage month            |
| wcpd-2006-10-16-pg1765   | 2006-10-07 | george w. bush                | the president's radio address                                         |
| wcpd-2006-10-23-pg1837-2 | 2006-10-18 | george w. bush                | remarks at waldo c. falkener elementary school in greensboro, nc      |

| wcpd-2006-11-06-pg1917-2 | 2006-10-30 | george w. bush                | remarks at a georgia victory 2006 rally in statesboro, georgia        |
| wcpd-2007-01-15-pg16     | 2007-01-06 | george w. bush                | the president's radio address                                         |
| wcpd-2007-02-05-pg99     | 2007-01-31 | george w. bush                | remarks on the national economy in new york city                      |
| wcpd-2007-03-05-pg238    | 2007-03-02 | george w. bush                | remarks at silver street elementary school in new albany              |
| wcpd-2007-03-19-pg338-2  | 2007-03-15 | george w. bush                | remarks at the national republican congressional committee dinner     |
| wcpd-2007-04-30-pg515    | 2007-04-24 | george w. bush                | remarks at the harlem village                                         |
| wcpd-2007-04-30-pg527    | 2007-04-26 | george w. bush                | remarks honoring the 2007 national and state teachers of the year     |
| wcpd-2007-06-04-pg715    | 2007-05-31 | george w. bush                | proclamation 8152 -- national child's day, 2007                       |
| wcpd-2007-07-02-pg858    | 2007-06-25 | george w. bush                | remarks to the 2007 presidential scholars                             |
| wcpd-2007-07-30-pg1011   | 2007-06-26 | george w. bush                | remarks to the american legislative exchange council in philadelphia, |
| wcpd-2007-10-01-pg1251-2 | 2007-09-25 | George W. Bush                | Statement on the National Assessment of Educational Progess           |

| wcpd-2007-10-01-pg1253   | 2007-09-26 | George W. Bush                | Remarks on the No Child Left Behind Act in New York City              |
| wcpd-2007-10-01-pg1255   | 2007-09-27 | George W. Bush                | Remarks on signing the College Cost Reduction and Access Act          |
| wcpd-2007-10-15-pg1318-2 | 2007-10-09 | George W. Bush                | Remarks on the No Child Left Behind Act reauthorization               |
| wcpd-2008-01-14-pg27     | 2008-01-07 | George W. Bush                | Remarks at Horace Greeley Elementary School in Chicago, Illinois      |
| wcpd-2008-04-28-pg587-2  | 2008-04-24 | George W. Bush                | Remarks at the White House Summit on inner-city children and ...      |
| wcpd-2008-05-05-pg622    | 2008-04-30 | George W. Bush                | Remarks honoring the 2008 national and state teachers of the year     |
| wcpd-2008-05-05-pg650-2  | 2008-05-02 | George W. Bush                | Proclamation 8251 -- National Charter Schools Week, 2008              |
| wcpd-2009-01-12-pg22-3   | 2009-01-08 | George W. Bush                | Remarks on the NCLB Act in Philadelphia, Pennsylvania                 |

*** DONE Congressional hearings						:sec:
  :PROPERTIES:
  :table_export_file: ~/Dropbox/dissertation/data/congr-hearings.tsv
  :table_export_format: orgtbl-to-tsv
  :END:

# alt-x org-table-export

#+tblname: congr-hearings
#+caption: description of the congressional hearings
#+label: tbl:congr-hearings
| serial no.  |       date | author | title                                                                                       |
|-------------+------------+--------+---------------------------------------------------------------------------------------------|
| sh-39-641   | 1997-01-23 | Senate | Ebonics                                                                                     |
| hh-59-654   | 1999-09-23 | House  | Fixing our schools from the bottom up                                                       |
| sh-70-756   | 2001-03-06 | Senate | Appropriations act for departments ... Education ... FY 2002                                |
| sh-78-480   | 2002-03-07 | Senate | Appropriations act for departments ... Education ... FY 2003                                |
| sh-79-324   | 2002-04-23 | Senate | Examining implementation of Elementary and Secondary Education Act                          |
| sh-79-941   | 2002-05-23 | Senate | America's schools: providing equal opportunity or still separate and unequal?               |
| sh-80-479   | 2002-06-25 | Senate | Reauthorization of the Office of Education Research and Improvement                         |
| sh-81-758   | 2002-09-10 | Senate | Successful implementation of Title I: state and local perspectives                          |
| hh-90-162   | 2003-10-30 | House  | Implementation of the math and science partnership program: Views from the field            |
| hh-91-364   | 2004-01-23 | House  | Fueling the high tech workforce with math and science education                             |
| hh-91-861   | 2004-02-11 | House  | Department of education budget priorities for FY 2005                                       |
| hh-92-309   | 2004-03-03 | House  | No Child Left Behind: Improving results for children with disabilities                      |
| hh-92-513   | 2004-03-18 | House  | The 2003 presidential awardees for excellence in math and science teaching ...              |
| hh-92-756   | 2004-03-30 | House  | H.R. 4030, congressional medal for outstanding contributions in math and ...                |
| hh-93-983   | 2004-05-27 | House  | Highly qualified teachers and raising student achievement                                   |
| hh-94-513   | 2004-06-23 | House  | No Child Left Behind: raising student achievement in America's big city schools             |
| sh-1910410  | 2003-03-27 | Senate | Depts. Labor, Health and Human Services, and Education ... appropriations FY 2004           |
| sh-85-932   | 2003-03-19 | Senate | Appropriations act for departments ... Education ... FY 2004                                |
| sh 94-491   | 2004-06-16 | Senate | Oversight hearing on implementation in Native American communities of the NCLBA             |
| sh 94-993   | 2004-07-15 | Senate | Pell grants for kids: It worked for colleges. Why not for K-12?                             |
| hh-20-424   | 2005-04-14 | House  | The 2004 presidential awardees for excellence in mathematics and science teaching           |
| hh-21-648   | 2005-06-09 | House  | The role of non-profit organizations in state and local high school reform efforts          |
| hh-23-691   | 2005-09-29 | House  | Closing the achievement gap in America's schools: the NCLB act                              |
| hh-26-125   | 2006-02-14 | House  | Member's Day                                                                                |
| hh-26-798   | 2006-03-30 | House  | K-12 science and math education across the federal agencies                                 |
| hh-27-978   | 2006-04-06 | House  | Building America's competitiveness: Examining what is needed to compete in a global economy |
| hh-27-985   | 2006-05-18 | House  | NCLB: how innovative educators are integrating subject matters to improve ....              |
| hh-28-431   | 2006-06-13 | House  | NCLB: disaggregating student achievement by subgroups to ensure all students are learning   |
| hh-28-839   | 2006-07-27 | House  | NCLB: can growth models ensure improved education for all students?                         |
| hh-29-626   | 2006-08-28 | House  | NCLB: successes and challenges of implementation in urban and suburban schools              |
| sh-20-732   | 2005-04-14 | Senate | Lifelong education opportunities                                                            |
| sh-21-951   | 2005-06-16 | Senate | Indian education                                                                            |
| sh-22-340   | 2005-06-30 | Senate | U.S. history: our worst subject?                                                            |
| sh-26-056   | 2006-02-09 | Senate | The role of education in global competitiveness                                             |
| sh-26-112   | 2006-02-14 | Senate | The president's FY 2007 budget request for Indian programs                                  |
| sh-26-353   | 2006-02-28 | senate | Protecting America's Competitive Edge Act (S. 2198): finding, training, and keeping ...     |
| sh-26-426   | 2006-03-01 | Senate | Protecting America's Competitive Edge Act (S. 2198): helping K-12 students learn math ...   |
| sh-27-036   | 2006-03-01 | Senate | Appropriations act for departments ... Education ... FY 2007                                |
| sh-27-768   | 2006-05-25 | Senate | Indian education                                                                            |
| sh-28-848   | 2006-04-26 | Senate | Fostering innovation in mathematics and science education                                   |
| sh-49104164 | 2005-12-31 | Senate | Appropriations act for departments ... Education ... FY 2006                                |
| sh-49104171 | 2005-03-02 | Senate | Appropriations act for departments ... Education ... FY 2006                                |
| sh-49104190 | 2006-03-01 | Senate | Appropriations act for departments ... Education ... FY 2007                                |
| sh-59104229 | 2006-12-31 | Senate | Appropriations act for departments ... Education ... FY 2007                                |
| sh-97-751   | 2005-01-06 | Senate | Nomination of Margaret Spellings                                                            |
| sh-99-869   | 2005-03-02 | Senate | Appropriations act for departments ... Education ... FY 2006                                |
| hh-33-801   | 2007-03-13 | House  | Science and technology leadership in a 21st century global economy                          |
| hh-34-015   | 2007-03-21 | house  | esea reauthorization: options for improving nclb's measures of progress                     |
| hh-34-016   | 2007-03-22 | House  | The Higher Education Act: Approaches to college preparation                                 |
| hh-34-017   | 2007-03-23 | House  | Impact of No Child Left Behind on English language learners                                 |
| hh-34-174   | 2007-03-29 | House  | How NCLB affects students with disabilities                                                 |
| hh-34-417   | 2007-04-12 | House  | Local perspectives on the No Child Left Behind Act                                          |
| hh-34-604   | 2007-04-27 | House  | Improving the No Child Left Behind Act's accountability system                              |
| hh-34-631   | 2007-04-23 | House  | NCLB: preventing dropouts and enhancing school safety                                       |
| hh-34-990   | 2007-05-11 | House  | ESEA reauthorization: boosting quality in the teaching profession                           |
| hh-35-233   | 2007-05-15 | House  | Federal stem education programs: educator's perspectives                                    |
| hh-35-664   | 2007-06-07 | House  | Reauthorization of the Elementary and Secondary Education Act: current and prospective ...  |
| hh-35-842   | 2007-06-28 | House  | Workforce investment act: recommendations to improve the effectiveness of job training      |
| hh-37-638   | 2007-09-10 | House  | Reauthorization of the Elementary and Secondary Education Act of 1965                       |
| hh-38-056   | 2007-10-10 | House  | Assessment of the national science board's action plan for stem education                   |
| hh-41-066   | 2008-03-12 | House  | Competitiveness and innovation on the committee's 50th anniversary with bill gates, ...     |
| hh-42-335   | 2008-05-21 | House  | The national mathematics advisory panel report: foundations for success                     |
| hh-43-311   | 2008-07-17 | House  | Mayor and superintendent partnerships in education: closing the achievement gap             |
| hh-43-470   | 2008-07-22 | House  | Innovation in education through business and educational stem partnerships                  |
| hh-44-214   | 2008-09-09 | House  | Challenges facing bureau of Indian education schools in improving student achievement       |
| jh-33-757   | 2007-03-13 | Joint  | Elementary and Secondary Act reauthorization: Improving NCLB to close the achievement gap   |
| sh-33-885   | 2007-03-07 | Senate | Strengthening American competitiveness in the 21st century                                  |
| sh-33-926   | 2007-03-19 | Senate | Appropriations for departments ... Education ... for FY 2008                                |
| sh-34-052   | 2007-03-06 | Senate | NCLB reauthorization: strategies for attracting, supporting and retaining high quality ...  |
| sh-35-072   | 2007-04-24 | Senate | NCLB reauthorization: Modernizing middle and high schools for the 21st century              |
| sh-35-329   | 2007-03-14 | Senate | Federal funding for the No Child Left Behind Act                                            |
| sh-37-293   | 2007-08-10 | Senate | No Child Left Behind: Improving education in Indian Country                                 |
| sh-45-589   | 2008-11-15 | Senate | Improving high school graduation rates and postsecondary success in Alaska and ...          |
| sh-69104283 | 2007-12-31 | Senate | Appropriation for departments ... Education ... for FY 2008                                 |
| hh-47-611   | 2009-02-26 | House  | Beyond the classroom: informal stem education                                               |
| hh-48-732   | 2009-04-29 | House  | Strengthening America's competitiveness through common academic standards                   |
| hh-49-499   | 2009-05-12 | House  | America's competitiveness through high school reform                                        |
| hh-52-859   | 2009-10-22 | House  | Engineering in K-12 education                                                               |
| hh-53-373   | 2009-11-19 | House  | Improving the literary skills of children and young adults                                  |
| hh-53-732   | 2009-12-08 | House  | Improving our competitiveness: Common core education standards                              |
| hh-55-304   | 2010-03-18 | House  | Elementary and Secondary Education Act reauthorization: Addressing the needs of ...         |
| hh-58-234   | 2010-03-04 | House  | Appropriation for departments ... Education .. for 2011                                     |
| sh-52-739   | 2009-08-24 | Senate | Stimulating Hawaii's economy: Impact of the American Recovery and Reinvestment Act of 2009  |
| sh-52-939   | 2009-09-16 | Senate | A review and assessment of the use, impact, and accomplishments of federal ...              |
| sh-55-474   | 2010-03-09 | Senate | ESEA reauthorization: The importance of a world-class K-12 education for our economic ...   |
| sh-67-045   | 2010-05-06 | Senate | America wins when America competes: Building a high-tech workforce                          |
| hh-64-229   | 2010-02-10 | House  | Education in the nation: examining the challenges and opportunities facing America's ...    |
| hh-64-657   | 2011-03-01 | House  | Education regulations: weighing the burden on schools and students                          |
| hh-64-795   | 2011-03-09 | House  | The budget and policy proposals of the U.S. Department of Education                         |

** DONE Text formatting and scrubbing 					:chp:
# tangle: c-c c-v t
# evaluate: c-c c-c

At this step of the investigation process I 'scrubbed' and formatted the text in the downloaded documents.  This step corresponds to the green box "extract statements" in the flow chart and was performed by /bash/ UNIX scripts shown here below, one for each of the two document collections.

The downloaded texts did not have proper paragraphs.  Each line ended in a line break/return character.  Thus, I included in the /bash/ scripts the instructions to build paragraphs based on blank lines and indentations of the texts.  In other cases I had to manually build paragraphs using an /Emacs/ function.

In the case of the Congressional Hearings the documents often contained both written statements and the reading of them.  Even though these two renditions were not identical, they basically duplicate all the content of the statement.  Thus, I have chosen the written statement and deleted the reading of it.  Sometimes the written statement was missing or corrupted and I was thus forced to use the version that was read.

In addition the hearings have also the transcripts of discussions and 'chit-chat'.  To reduce the text to only its `high content' portions, I extract only the written statements and letters if they are not read out.  Then all the pieces were recombined into a 'scrubbed' file that could then be imported into the QDA software.  I have excluded all discussions because they are much more intricate and complex and would require a different level of analysis.

The scripts in this chapter performed generic and file specific text scrubbing.  The generic part was the removal of redundant lines common to the specific document collection.  In addition, these scripts performed file-specific "scrubbing" of the files.  The original files were preserved for checking.  The statements were `extracted' and placed in a file with the same name except for the ".scrb" extension.

These /bash/ UNIX scripts perform the operations corresponding to the step "remove redundant text" of the work flow diagram.

The =congr-hearings= and =pres-docs= documents required some manual editing.

*** DONE Congressional hearings						:sec:

The following UNIX script removes redundant data from the congressional hearings.  Often the hearings will have both written statements and the reading of it. In addition the hearings have also the transcripts of 'chit-chat'. To reduce the text to only its 'high content' portions, I extract only the written statements and letters if they are not read out.  Each extracted portion ends with a blank line.  Then all the pieces are recombined into a 'scrubbed' file that can be imported into the QDA software.  I have excluded all discussions because they are much more intricate and complex and would require a different level of analysis.

#+srcname: congr-hearings-scrub
#+begin_src sh :tangle ~/Dropbox/dissertation/data/congr-hearings-scrub.sh :results output
#!/bin/bash

# define variables
home_dir=$home/Dropbox/dissertation/data
data_dir=$home_dir/congr-hearings
log_file=$home_dir/congr-hearings-scrub.log
  
# initial number of lines
cd $data_dir
echo "initial number of lines:"
for file in *.txt
do
  echo `wc -l $file` | tee $log_file
done

# remove old scrubbed files
echo "remove old scrubbed files" | tee -a $log_file
rm *.scrb

# extract prepared statements
echo "begin extracting statements and letters" | tee -a $log_file

sed -n -e   '195,339p' -e   '344,496p' -e   '654,899p' -e  '917,1505p' \
       -e '2058,2082p' -e '2176,2205p' -e '2290,2316p' -e '2469,2522p' \
       -e '2541,2556p' -e '2694,2766p' -e '2810,2924p' -e '3174,3397p' \
       -e '3534,3641p' -e '3756,3844p' -e '3968,4151p' -e '4333,5465p' \
       chrg-105shrg39641.txt > temp.txt
mv temp.txt chrg-105shrg39641.scrb

sed -n -e '302,405p'   -e '766,1177p'  -e '1460,1599p' -e '2171,2706p' \
       -e '3489,3809p' -e '4369,4523p' -e '4714,4821p' -e '5089,5220p' -e '5226,5244p' \
       -e '5259,5522p' -e '5604,5666p' -e '5722,5763p' \
       chrg-106hhrg59654.txt > temp.txt
mv temp.txt chrg-106hhrg59654.scrb

sed -n -e '16369,16491p' -e '16899,17160p' -e '17177,17222p' -e '17986,18870p' \
       -e '28312,28565p' -e '28933,29063p' -e '29094,29204p' \
       chrg-107shrg70756.txt > temp.txt
mv temp.txt chrg-107shrg70756.scrb

sed -n -e '4503,4603p'  -e '4617,4694p' -e '4733,4801p' -e '4819,4851p' \
       -e '4856,4994p'  -e '5007,5064p' -e '5196,5433p' -e '6155,7328p' \
       chrg-107shrg78480.txt > temp.txt
mv temp.txt chrg-107shrg78480.scrb

sed -n -e '129,213p' -e '215,296p' -e '299,327p' -e '334,496p' \
       -e '582,817p' -e '828,865p' \
       chrg-107shrg79324.txt > temp.txt
mv temp.txt chrg-107shrg79324.scrb

sed -n -e '136,308p'   -e '415,515p'   -e '537,586p'   -e '589,640p'   \
       -e '893,1059p'  -e '1123,1237p' -e '1902,2090p' -e '2248,2537p' \
       -e '2676,3370p' -e '3531,3665p' \
       chrg-107shrg79941.txt > temp.txt
mv temp.txt chrg-107shrg79941.scrb

sed -n -e '131,227p' -e '229,254p'   -e '258,466p'   -e '771,819p'   \
       -e '830,903p' -e '1155,1608p' -e '1806,2263p' -e '2387,2649p' \
       -e '2833,3061p' \
       chrg-107shrg80479.txt > temp.txt
mv temp.txt chrg-107shrg80479.scrb

sed -n -e '204,502p'   -e '628,761p'   -e '935,1263p' -e '1300,1683p' \
       -e '1787,2078p' \
       chrg-107shrg81758.txt > temp.txt
mv temp.txt chrg-107shrg81758.scrb

sed -n -e '220,232p'   -e '288,539p'   -e '691,774p'   -e '849,909p' \
       -e '968,1061p'  -e '1116,1208p' -e '1514,1574p' -e '1682,1784p' \
       -e '1942,2369p' -e '2541,2892p' -e '3985,4396p' \
       chrg-108hhrg90162.txt > temp.txt
mv temp.txt chrg-108hhrg90162.scrb

sed -n -e '168,175p'   -e '207,516p'   -e '677,730p'   -e '781,875p'   \
       -e '1053,1127p' -e '1308,1448p' -e '1615,1748p' -e '1874,2409p' \
       -e '2468,3664p' -e '3903,4029p' \
       chrg-108hhrg91364.txt > temp.txt
mv temp.txt chrg-108hhrg91364.scrb

sed -n -e '126,199p'   -e '202,295p'   -e '520,729p'   -e '1912,2231p' \
       -e '2243,2373p' -e '2493,2603p' -e '2955,3098p' \
       chrg-108hhrg91861.txt > temp.txt
mv temp.txt chrg-108hhrg91861.scrb

sed -n -e '279,418p'   -e '566,791p'   -e '923,1026p'  -e '1131,1226p' \
       -e '1339,1484p' -e '2589,2630p' -e '2987,3041p' -e '3043,3614p' \
       chrg-108hhrg92309.txt > temp.txt
mv temp.txt chrg-108hhrg92309.scrb

sed -n -e '184,193p'   -e '231,426p'   -e '500,542p' -e '586,631p'  \
       -e '655,672p'   -e '674,728p'   -e '754,766p' -e '772,797p'  \
       -e '805,811p'   -e '845,875p'   -e '877,911p' -e '1090,1176p' \
       -e '1718,1968p' -e '2254,2399p' -e '2595,2726p' \
       chrg-108hhrg92513.txt > temp.txt 
mv temp.txt chrg-108hhrg92513.scrb

sed -n -e '221,232p'   -e '271,426p'   -e '619,671p'   -e '675,699p'   \
       -e '701,743p'   -e '812,925p'   -e '1105,1396p' -e '1541,1745p' \
       -e '1863,2105p' -e '2247,2530p' -e '3158,3456p' \
       chrg-108hhrg92756.txt > temp.txt
mv temp.txt chrg-108hhrg92756.scrb

sed -n -e '324,433p'   -e '578,614p'   -e '803,1006p' -e '1243,1570p' \
       -e '1725,2157p' -e '2431,2868p' -e '3541,3590p' \
       chrg-108hhrg93983.txt > temp.txt
mv temp.txt chrg-108hhrg93983.scrb

sed -n -e '282,373p'   -e '434,493p'   -e '542,648p'   -e '775,1026p'  \
       -e '1134,1285p' -e '1396,1546p' -e '1560,1660p' -e '3494,3580p' \
       chrg-108hhrg94513.txt > temp.txt
mv temp.txt chrg-108hhrg94513.scrb

sed -n -e '32,67p'     -e '124,212p' -e '268,310p' -e '345,393p'   \
       -e '543,615p'   -e '653,908p' -e '952,991p' -e '1697,1780p' \
       -e '1817,2262p' -e '2402,2937p' \
       chrg-108shrg1910410.txt > temp.txt
mv temp.txt chrg-108shrg1910410.scrb

# ### duplicate !!!! ### find other copy - grep in directory ####
# ###
sed -n -e '6398,6661p' -e '6731,6834p' -e '6914,7450p' \
       chrg-108shrg85932.txt > temp.txt
mv temp.txt chrg-108shrg85932.scrb

sed -n -e '125,289p'   -e '1169,1346p' -e '1850,1958p' -e '2291,2492p' \
       -e '2561,2716p' -e '2728,2887p' \
       chrg-108shrg94491.txt > temp.txt
mv temp.txt chrg-108shrg94491.scrb

sed -n -e '397,637p'   -e '644,755p'   -e '784,850p'   -e '856,1193p'  \
       -e '1198,1632p' -e '1640,1728p' -e '1752,1840p' -e '1842,1968p' \
       -e '2122,2251p' -e '2338,2384p' -e '2499,2634p' -e '2719,2843p' \
       -e '3039,3330p' -e '3550,3812p' -e '3815,3953p' -e '4089,4188p' \
       -e '4304,4404p' -e '4877,5089p' \
       chrg-108shrg94993.txt > temp.txt
mv temp.txt chrg-108shrg94993.scrb

# ### duplicate of 108hhrg92513 ######
# "it is time to take action to ensure the best possible education
# for our children"
# ########################
sed -n -e '200,214p'   -e '238,496p'   -e '569,611p'   -e '668,710p'   \
       -e '715,755p'   -e '758,795p'   -e '798,830p'   -e '833,856p'   \
       -e '859,879p'   -e '882,947p'   -e '950,975p'   -e '1161,1247p' \
       -e '1423,1531p' -e '1704,1825p' -e '1956,2060p' -e '2087,2172p' \
       chrg-109hhrg20424.txt > temp.txt
mv temp.txt chrg-109hhrg20424.scrb

sed -n -e '258,307p'  -e '310,363p'   -e '369,431p' -e '558,752p' \
       -e '853,1144p' -e '1287,1726p' \
       chrg-109hhrg21648.txt > temp.txt
mv temp.txt chrg-109hhrg21648.scrb

sed -n -e '274,352p'   -e '354,394p'   -e '596,740p' -e '1856,1935p' \
       -e '1987,2025p' -e '2173,2519p' -e '2933,3094p' \
       chrg-109hhrg23691.txt > temp.txt
mv temp.txt chrg-109hhrg23691.scrb

sed -n -e '339,455p'   -e '582,671p'   -e '930,1151p'  -e '1282,1469p' \
       -e '1611,1722p' -e '1950,2127p' -e '2273,2366p' -e '2555,2753p' \
       -e '3012,3178p' -e '3405,3532p' -e '3735,3840p' -e '4147,4466p' \
       -e '4661,4756p' -e '4837,4933p' -e '5065,5156p' -e '5322,5406p' \
       -e '5566,5688p' -e '5801,5890p' -e '6038,6176p' -e '6255,6316p' \
       -e '6421,6501p' -e '6613,6704p' -e '6833,6929p' -e '6977,7093p' \
       -e '7184,7223p' -e '7225,7325p' -e '7329,7385p' \
       chrg-109hhrg26125.txt > temp.txt
mv temp.txt chrg-109hhrg26125.scrb

sed -n -e '191,197p'   -e '216,690p'   -e '769,831p'   -e '955,1029p'  \
       -e '1034,1089p' -e '1092,1144p' -e '1147,1183p' -e '1188,1214p' \
       -e '1218,1258p' -e '1403,1911p' -e '2042,2377p' -e '2498,2786p' \
       -e '2934,3268p' -e '3425,4054p' -e '5348,6406p' \
       chrg-109hhrg26798.txt > temp.txt
mv temp.txt chrg-109hhrg26798.scrb

sed -n -e '281,351p'   -e '461,555p'   -e '697,1080p'  -e '1277,1745p' \
       -e '3110,3234p' -e '3339,3738p' -e '3889,4130p' -e '4739,4773p' \
       -e '4776,5758p' \
      chrg-109hhrg27978.txt > temp.txt
mv temp.txt chrg-109hhrg27978.scrb

sed -n -e '245,301p'   -e '388,459p'   -e '786,982p'   -e '1240,1402p' \
       -e '1553,1734p' -e '1916,2085p' -e '2200,2392p' -e '3095,3148p' \
       -e '3151,3211p' -e '3214,3250p' -e '3254,3487p' \
       chrg-109hhrg27985.txt > temp.txt
mv temp.txt chrg-109hhrg27985.scrb

sed -n -e '224,278p'   -e '354,423p' -e '594,813p' -e '908,1073p' \
       -e '1165,1413p' -e '1711,2253p' \
       chrg-109hhrg28431.txt > temp.txt
mv temp.txt chrg-109hhrg28431.scrb

sed -n -e '238,290p'   -e '388,471p'   -e '653,1167p'  -e '1277,1429p' \
       -e '1578,1811p' -e '1873,2027p' -e '2143,2405p' -e '2514,2641p' \
       chrg-109hhrg28839.txt > temp.txt
mv temp.txt chrg-109hhrg28839.scrb

sed -n -e '268,334p'   -e '641,815p'   -e '978,1120p'  -e '1324,1508p' \
       -e '1620,1857p' -e '2034,2264p' -e '3167,3643p' -e '3764,3792p' \
       -e '3862,4419p' \
       chrg-109hhrg29626.txt > temp.txt
mv temp.txt chrg-109hhrg29626.scrb

sed -n -e '126,283p'   -e '386,693p'   -e '887,1175p'  -e '1334,1452p' \
       -e '1878,2970p' -e '3106,3351p' -e '3459,3864p' -e '3973,4211p' \
       -e '4335,4496p' -e '5129,5266p' \
       chrg-109shrg20732.txt > temp.txt
mv temp.txt chrg-109shrg20732.scrb

sed -n -e '122,147p'   -e '154,230p'   -e '1958,2274p' -e '1519,1636p' \
       -e '1365,1502p' -e '1506,1510p' -e '1245,1359p' -e '1064,1239p' \
       -e '846,905p'   -e '332,491p'   -e '239,327p' \
       chrg-109shrg21951.txt > temp.txt
mv temp.txt chrg-109shrg21951.scrb

sed -n -e '153,290p'   -e '343,437p'   -e '439,765p'   -e '767,789p' \
       -e '1099,1463p' -e '1574,1727p' -e '1834,1939p' -e '2539,2577p' \
       chrg-109shrg22340.txt > temp.txt
mv temp.txt chrg-109shrg22340.scrb
 
sed -n -e '118,217p' -e '290,388p' -e '1914,2394p' -e '2399,3637p' \
       chrg-109shrg26056.txt > temp.txt
mv temp.txt chrg-109shrg26056.scrb

sed -n -e '172,267p'   -e '269,280p'   -e '282,324p'   -e '327,414p'   \
       -e '449,462p'   -e '468,516p'   -e '520,606p'   -e '611,713p'   \
       -e '721,812p'   -e '816,891p'   -e '1594,1757p' -e '1762,1850p' \
       -e '1856,1976p' -e '1981,2091p' -e '2095,2282p' -e '2529,2774p' \
       chrg-109shrg26112.txt > temp.txt 
mv temp.txt chrg-109shrg26112.scrb
 
sed -n -e '177,302p'   -e '310,332p'   -e '419,665p'   -e '1142,1305p' \
       -e '1146,1599p' -e '1738,2191p' -e '2467,2723p' -e '2843,2928p' \
       -e '3097,3287p' -e '3771,3900p' -e '3961,4222p' \
       chrg-109shrg26353.txt > temp.txt
mv temp.txt chrg-109shrg26353.scrb
 
sed -n -e '229,283p'   -e '288,375p'   -e '377,512p'   -e '626,811p' \
       -e '911,1279p'  -e '1281,1535p' -e '2137,2281p' -e '3575,2642p' \
       -e '2742,3082p' -e '3236,3382p' -e '3992,4715p' \
       chrg-109shrg26426.txt > temp.txt
mv temp.txt chrg-109shrg26426.scrb
 
sed -n -e '335,365p'    -e '376,438p'   -e '553,702p'     -e '810,891p' \
       -e '1310,1336p'  -e '1619,1701p' -e '2199,2255p'   -e '2261,2305p' \
       -e '2313,3725p'  -e '3739,4273p' -e '38678,38780p' -e '39717,39873p' \
       chrg-109shrg27036.txt > temp.txt
mv temp.txt chrg-109shrg27036.scrb
 
sed -n -e '161,184p'   -e '188,248p'   -e '252,291p'   -e '1937,2062p' \
       -e '2344,2451p' -e '333,401p'   -e '407,515p'   -e '959,1041p' \
       -e '1049,1147p' -e '1267,1304p' -e '1311,1426p' \
       chrg-109shrg27768.txt > temp.txt
mv temp.txt chrg-109shrg27768.scrb
 
sed -n -e '135,382p'   -e '472,1088p'  -e '1376,1457p' -e '1586,1737p' \
       -e '1906,2056p' -e '2222,2316p' -e '2735,3063p' -e '3119,3241p' \
       chrg-109shrg28848.txt > temp.txt
mv temp.txt chrg-109shrg28848.scrb
 
sed -n -e '23474,23491p' -e '23501,23975p' \
       chrg-109shrg49104164.txt > temp.txt
mv temp.txt chrg-109shrg49104164.scrb
 
sed -n -e '266,479p'   -e '487,509p' -e '566,1514p' -e '1543,1666p' \
       -e '2407,2506p' -e '2515,5356p' \
       chrg-109shrg49104171.txt > temp.txt
mv temp.txt chrg-109shrg49104171.scrb
 
sed -n -e '86,117p'    -e '129,186p'   -e '305,464p'   -e '901,951p' \
       -e '1076,1104p' -e '1387,1471p' -e '1663,1766p' -e '1971,2029p' \
       -e '2035,2081p' -e '2089,3597p' -e '3623,4185p' \
       chrg-109shrg49104190.txt > temp.txt
mv temp.txt chrg-109shrg49104190.scrb
 
sed -n -e '19011,19177p' -e '19257,19360p' -e '19628,20471p' \
       chrg-109shrg59104229.txt > temp.txt
mv temp.txt chrg-109shrg59104229.scrb
 
sed -n -e '297,486p'   -e '489,504p'   -e '506,656p'   -e '663,694p' \
       -e '696,758p'   -e '972,1146p'  -e '1158,3789p' -e '4131,4183p' \
       -e '4379,4521p' -e '5143,5211p' -e '5507,5596p' -e '5621,7834p' \
       chrg-109shrg97751.txt > temp.txt
mv temp.txt chrg-109shrg97751.scrb

sed -n -e '213,224p'   -e '250,401p'   -e '643,1520p'  -e '1595,1650p' \
       -e '1684,1713p' -e '1723,1775p' -e '1899,2158p' -e '2417,2624p' \
       -e '2847,3672p' -e '3997,4096p' -e '4237,4507p' -e '4710,5102p' \
       -e '6652,7421p' \
       chrg-110hhrg33801.txt > temp.txt
mv temp.txt chrg-110hhrg33801.scrb
 
sed -n -e '200,333p'   -e '277,333p'   -e '508,923p'   -e '1034,1121p' \
       -e '1216,1536p' -e '1698,1852p' -e '1969,2093p' -e '3932,3955p' \
       -e '3959,4112p' -e '4142,4658p' -e '4660,4740p' \
       chrg-110hhrg34015.txt > temp.txt
mv temp.txt chrg-110hhrg34015.scrb
 
sed -n -e '180,237p'   -e '241,281p'   -e '496,909p' -e '1021,1114p' \
       -e '1228,1652p' -e '1754,1897p' \
       chrg-110hhrg34016.txt > temp.txt 
mv temp.txt chrg-110hhrg34016.scrb
 
sed -n -e '207,265p'  -e '268,304p'   -e '356,456p'   -e '619,856p' \
       -e '963,1562p' -e '1649,1767p' -e '1867,2174p' -e '3235,3999p' \
       chrg-110hhrg34017.txt > temp.txt
mv temp.txt chrg-110hhrg34017.scrb
 
sed -n -e '204,324p'   -e '476,650p'   -e '772,1766p'  -e '1770,1881p' \
       -e '2068,2437p' -e '2445,2533p' -e '2789,2861p' -e '2967,3046p' \
       -e '3639,3766p' -e '3804,3818p' -e '3850,4227p' -e '4443,4507p' \
       chrg-110hhrg34174.txt > temp.txt 
mv temp.txt chrg-110hhrg34174.scrb
 
sed -n -e '276,473p'   -e '667,787p'   -e '898,983p' -e '1107,1475p' \
       -e '1594,1694p' -e '1846,2003p' -e '3018,3893p' \
       chrg-110hhrg34417.txt > temp.txt 
mv temp.txt chrg-110hhrg34417.scrb
 
sed -n -e '220,416p'   -e '630,823p'   -e '941,1081p' -e '1184,1418p' \
       -e '1539,1632p' -e '1734,1852p' -e '2763,3112p' \
       chrg-110hhrg34604.txt > temp.txt
mv temp.txt chrg-110hhrg34604.scrb
 
sed -n -e '329,389p'   -e '392,458p'   -e '641,1105p'  -e '1222,1456p' \
       -e '1575,1810p' -e '1921,2067p' -e '2225,2843p' -e '4636,4671p' \
       -e '4679,5178p' \
      chrg-110hhrg34631.txt > temp.txt 
mv temp.txt chrg-110hhrg34631.scrb
 
sed -n -e '296,392p'   -e '473,544p'   -e '791,1044p'  -e '1148,1304p' \
       -e '1409,1589p' -e '1716,1828p' -e '1947,2330p' -e '2410,2663p' \
       -e '2803,2997p' -e '3101,3256p' -e '4934,4958p' -e '4965,5119p' \
       -e '5147,5929p' \
       chrg-110hhrg34990.txt > temp.txt
mv temp.txt chrg-110hhrg34990.scrb
 
sed -n -e '299,315p'   -e '331,466p'   -e '550,623p'   -e '687,729p' \
       -e '735,766p'   -e '886,1081p'  -e '1206,1432p' -e '1561,1722p' \
       -e '1872,2221p' -e '2359,2758p' -e '3698,4152p' -e '4189,4208p' \
       -e '4224,4524p' -e '4598,4657p' -e '4726,4759p' -e '4766,4793p' \
       -e '4795,4822p' -e '4947,5177p' -e '5334,5798p' -e '5956,6229p' \
       -e '6368,6705p' -e '7321,7597p' -e '7606,7819p' \
       chrg-110hhrg35233.txt > temp.txt
mv temp.txt chrg-110hhrg35233.scrb
 
sed -n -e '275,336p'   -e '415,483p'   -e '490,961p'   -e '965,1119p' \
       -e '1135,1505p' -e '1658,1885p' -e '2063,2302p' -e '2406,2583p' \
       -e '2693,2830p' -e '2934,3196p' -e '3313,3677p' \
       chrg-110hhrg35664.txt > temp.txt 
mv temp.txt chrg-110hhrg35664.scrb
 
sed -n -e '262,320p'   -e '373,408p'   -e '488,600p'   -e '763,1335p' \
       -e '1439,1873p' -e '2108,2520p' -e '2529,2646p' -e '2763,3147p' \
       -e '3698,3730p' -e '4042,4366p' -e '4384,4611p' \
       chrg-110hhrg35842.txt > temp.txt
mv temp.txt chrg-110hhrg35842.scrb
 
sed -n -e '512,640p'     -e '681,724p'     -e '731,777p'     -e '780,817p' \
       -e '934,1062p'    -e '1161,1494p'   -e '1601,1723p'   -e '1725,2058p' \
       -e '2172,2217p'   -e '2233,2947p'   -e '3112,3326p'   -e '3447,3794p' -e '4030,4294p' \
       -e '4666,4786p'   -e '4901,5049p'   -e '5154,5253p'   -e '5349,5501p' \
       -e '5649,5902p'   -e '6053,6572p'   -e '6682,6890p'   -e '7330,7690p' \
       -e '7797,8026p'   -e '8212,8417p'   -e '8419,8783p'   -e '9025,9589p' \
       -e '9700,10054p'  -e '10179,10493p' -e '10613,10801p' -e '10920,11032p' \
       -e '11396,11686p' -e '11795,11891p' -e '12005,12284p' -e '12402,12509p' \
       -e '12613,12793p' -e '12880,12991p' -e '13094,13242p' \
       -e '13753,14012p' -e '14129,14199p' -e '14281,14548p' -e '14662,14962p' \
       -e '15076,15190p' -e '15297,15619p' -e '15738,15924p' -e '16176,16553p' \
       -e '17017,17244p' -e '17345,18253p' -e '18352,18616p' -e '18727,18814p' \
       -e '18960,19210p' -e '19333,19475p' -e '19572,19596p' -e '19599,19794p' \
       -e '19796,20154p' -e '20156,20325p' \
       chrg-110hhrg37638.txt > temp.txt
mv temp.txt chrg-110hhrg37638.scrb
 
sed -n -e '262,273p'   -e '292,434p'   -e '621,685p'   -e '750,788p'   -e '924,1231p' \
       -e '1362,1486p' -e '1616,1783p' -e '1907,2146p' -e '2291,2594p' \
       -e '2744,2963p' -e '3931,4586p' \
       chrg-110hhrg38056.txt > temp.txt 
mv temp.txt chrg-110hhrg38056.scrb
 
sed -n -e '144,202p' -e '271,327p'  -e '328,423p' -e '425,453p' \
       -e '455,499p' -e '741,1608p' \
       chrg-110hhrg41066.txt > temp.txt
mv temp.txt chrg-110hhrg41066.scrb
 
sed -n -e '266,355p'   -e '407,453p'   -e '684,945p'   -e '1104,1333p' \
       -e '1486,1685p' -e '1836,1983p' -e '2107,2676p' -e '2787,2883p' \
       -e '4797,5006p' -e '5011,5033p' -e '5036,5079p' \
       chrg-110hhrg42335.txt > temp.txt
mv temp.txt chrg-110hhrg42335.scrb
 
sed -n -e '265,335p'   -e '414,484p'   -e '809,892p'   -e '1143,1332p' \
       -e '1543,1625p' -e '1779,1849p' -e '1996,2162p' -e '2382,2538p' \
       -e '4219,4384p' -e '4388,5976p' \
       chrg-110hhrg43311.txt > temp.txt
mv temp.txt chrg-110hhrg43311.scrb
 
sed -n -e '247,322p'   -e '392,457p'   -e '463,487p'   -e '491,550p' \
       -e '810,1025p'  -e '1124,1234p' -e '1333,1460p' -e '1568,1706p' \
       -e '1823,2162p' -e '2267,2556p' -e '2651,2883p' -e '2994,3475p' \
       -e '4869,5147p' \
       chrg-110hhrg43470.txt > temp.txt
mv temp.txt chrg-110hhrg43470.scrb
 
sed -n -e '270,326p'   -e '374,414p'   -e '529,626p'   -e '768,1227p' \
       -e '1397,1610p' -e '1747,1939p' -e '1943,2114p' \
       chrg-110hhrg44214.txt > temp.txt
mv temp.txt chrg-110hhrg44214.scrb
 
sed -n -e '249,337p'   -e '338,420p'   -e '423,517p'   -e '520,586p' \
       -e '669,767p'   -e '928,1076p'  -e '1187,1415p' -e '1419,1534p' \
       -e '1686,1846p' -e '1951,4486p' -e '4570,4732p' -e '6260,6290p' \
       -e '6294,6411p' -e '6415,6540p' -e '6560,6752p' -e '6756,7174p' \
       -e '7176,7413p' -e '7417,7808p' -e '7813,8022p' -e '8063,8147p' \
       -e '8151,8581p' \
       chrg-110jhrg33757.txt > temp.txt
mv temp.txt chrg-110jhrg33757.scrb
 
sed -n -e '181,241p' -e '281,429p' -e '443,487p' -e '740,1402p' \
       -e '3053,3242p' \
       chrg-110shrg33885.txt > temp.txt
mv temp.txt chrg-110shrg33885.scrb
 
sed -n -e '46823,46888p' -e '46395,46556p' \
       chrg-110shrg33926.txt > temp.txt
mv temp.txt chrg-110shrg33926.scrb
 
sed -n -e '209,384p'   -e '495,1378p'  -e '1382,1483p' -e '1585,2112p' \
       -e '2211,2470p' -e '2602,2804p' -e '2889,3026p' -e '3132,3478p' \
       -e '3488,3647p' -e '3780,4338p' -e '4480,5037p' -e '5140,5340p' \
       -e '6128,6704p' \
       chrg-110shrg34052.txt > temp.txt
mv temp.txt chrg-110shrg34052.scrb
 
sed -n -e '192,264p'   -e '351,443p'   -e '616,923p'   -e '1087,1530p' \
       -e '1715,2127p' -e '2309,2563p' -e '2703,2923p' -e '3794,4116p' \
       chrg-110shrg35072.txt > temp.txt
mv temp.txt chrg-110shrg35072.scrb

# this hearing does not have long statements 
sed -n -e '330,532p'   -e '663,899p'   -e '1147,1190p' -e '1396,1418p' \
       -e '4809,5116p' -e '5255,5440p' -e '5570,5732p' -e '5873,5996p' \
       -e '6115,6323p' \
       chrg-110shrg35329.txt > temp.txt
mv temp.txt chrg-110shrg35329.scrb
 
sed -n -e '128,155p'   -e '158,160p'   -e '163,175p'   -e '207,307p' \
       -e '425,586p'   -e '768,902p'   -e '1069,1522p' -e '1694,1836p' \
       -e '2035,2100p' -e '2116,2220p' \
       chrg-110shrg37293.txt > temp.txt 
mv temp.txt chrg-110shrg37293.scrb
 
sed -n -e '196,394p'   -e '643,1020p'  -e '1154,1438p' -e '1631,1849p' \
       -e '1956,2396p' -e '2612,3042p' -e '3158,3220p' -e '3331,3493p' \
       -e '3613,3713p' -e '3869,3996p' -e '4122,4337p' -e '5338,6963p' \
       chrg-110shrg45589.txt > temp.txt 
mv temp.txt chrg-110shrg45589.scrb
 
sed -n -e '2404,2434p' -e '6819,7032p' \
       -e '10984,11066p' \
       chrg-110shrg69104283.txt > temp.txt
mv temp.txt chrg-110shrg69104283.scrb
 
sed -n -e '225,236p'   -e '256,508p'   -e '578,634p'   -e '709,736p' -e '741,772p' \
       -e '914,1321p'  -e '1471,1899p' -e '2012,2535p' -e '2661,2914p' \
       -e '3492,3895p' -e '4767,5601p' -e '6322,6365p' -e '6375,7125p' \
       chrg-111hhrg47611.txt > temp.txt
mv temp.txt chrg-111hhrg47611.scrb
 
sed -n -e '307,389p'   -e '441,494p'   -e '707,940p'   -e '1080,1205p' \
       -e '1311,1549p' -e '1648,1772p' -e '1886,1998p' -e '2924,3075p' \
       -e '3080,3149p' \
       chrg-111hhrg48732.txt > temp.txt
mv temp.txt chrg-111hhrg48732.scrb
 
sed -n -e '304,393p'   -e '438,483p'   -e '611,731p'   -e '829,931p' \
       -e '1043,1127p' -e '1235,1318p' -e '1582,1849p' -e '1971,2299p' \
       -e '2416,3047p' -e '3263,3389p' -e '3522,3741p' -e '3863,4210p' \
       chrg-111hhrg49499.txt > temp.txt 
mv temp.txt chrg-111hhrg49499.scrb
 
sed -n -e '216,221p'   -e '242,512p'   -e '555,587p'   -e '637,658p'   -e '663,710p' \
       -e '981,1373p'  -e '1510,1765p' -e '2226,2822p' -e '3083,3331p' \
       -e '3501,3922p' -e '4715,4816p' \
       chrg-111hhrg52859.txt > temp.txt
mv temp.txt chrg-111hhrg52859.scrb
 
sed -n -e '303,349p'   -e '423,483p'   -e '757,953p'   -e '1086,1284p' \
       -e '1453,2167p' -e '2274,2531p' -e '2639,2730p' -e '2827,3022p' \
       -e '3581,3686p' -e '4237,4358p' -e '4362,4438p' -e '4447,5456p' \
       -e '5464,5731p' -e '5770,5994p' -e '6009,6052p' -e '6133,6159p' \
       -e '6184,6236p' \
       chrg-111hhrg53373.txt > temp.txt
mv temp.txt chrg-111hhrg53373.scrb
 
sed -n -e '279,352p'   -e '357,427p'   -e '581,744p' -e '1651,1892p' \
       -e '1992,2088p' -e '2198,2374p' -e '3301,4164p' -e '4174,4797p' \
       chrg-111hhrg53732.txt > temp.txt
mv temp.txt chrg-111hhrg53732.scrb
 
sed -n -e '225,297p'   -e '300,364p'   -e '665,1051p'  -e '1159,1523p' \
       -e '1618,1926p' -e '2037,2628p' -e '2728,2863p' -e '2973,3205p' \
       -e '4147,4377p' -e '4381,4460p' \
       chrg-111hhrg55304.txt > temp.txt
mv temp.txt chrg-111hhrg55304.scrb
 
sed -n -e '5236,5411p' -e '5428,5549p' \
       chrg-111hhrg58234.txt > temp.txt
mv temp.txt chrg-111hhrg58234.scrb
 
sed -n -e '212,301p'   -e '347,378p'    -e '563,1019p'  -e '1309,1478p' \
       -e '1786,1865p' -e '2080,2204p'  -e '2400,2744p' -e '2991,3236p' \
       -e '4484,4620p' -e '4889,5129p'  -e '5334,5495p' -e '9158,9273p' \
       -e '10246,10297p' \
       chrg-111shrg52739.txt > temp.txt
mv temp.txt chrg-111shrg52739.scrb
 
sed -n -e '217,406p'   -e '408,447p'   -e '449,974p'   -e '977,1074p' \
       -e '1082,1117p' -e '1121,1178p' -e '1380,1628p' -e '2034,2207p' -e '2624,2879p' \
       -e '4517,4561p' -e '4607,4646p' -e '4972,5094p' -e '5096,5200p' -e '5202,5492p' -e '5494,5555p' \
       -e '5557,5624p' -e '5626,5866p' -e '5868,6152p' -e '6200,6308p' -e '6353,6468p' -e '6779,6847p' \
       -e '7090,7179p' \
       -e '7339,7815p' \
       -e '7952,8053p'\
       chrg-111shrg52939.txt > temp.txt
mv temp.txt chrg-111shrg52939.scrb
 
sed -n -e '174,262p'   -e '265,369p'   -e '492,1139p' -e '1242,2557p' \
       -e '2650,2761p' -e '2846,3230p' -e '4610,5316p' \
       chrg-111shrg55474.txt > temp.txt
mv temp.txt chrg-111shrg55474.scrb
 
sed -n -e '257,320p'   -e '405,480p'   -e '578,946p'   -e '1052,1155p' \
       -e '1281,1513p' -e '1630,1944p' -e '2004,2107p' -e '2414,2563p' \
       -e '3450,4083p' \
       chrg-111shrg67045.txt > temp.txt
mv temp.txt chrg-111shrg67045.scrb
 
sed -n -e '263,321p'   -e '415,496p'   -e '694,902p' -e '1009,1147p' \
       -e '1266,1435p' -e '1440,1535p' \
       chrg-112hhrg64229.txt > temp.txt
mv temp.txt chrg-112hhrg64229.scrb
 
sed -n -e '242,312p'   -e '424,502p'   -e '687,793p'   -e '916,1032p' \
       -e '1143,1373p' -e '1467,1813p' -e '3889,3927p' -e '3944,4060p' \
       chrg-112hhrg64657.txt > temp.txt
mv temp.txt chrg-112hhrg64657.scrb
 
sed -n -e '271,350p'   -e '452,543p'   -e '554,599p' -e '857,1090p' \
       -e '3417,3433p' -e '4738,5083p' \
       chrg-112hhrg64795.txt > temp.txt
mv temp.txt chrg-112hhrg64795.scrb

# remove lines with "tiff"
echo "deleting lines with tiff" | tee -a $log_file
for file in *.scrb
do
  sed '/tiff/d' $file > $file.tmp
  mv $file.tmp $file
done
echo "finished deleting lines with tiff" | tee -a $log_file


# change 10-space indents into 4-space indents
echo "begin changing 10-space indents into 4-space indents" | tee -a $log_file
for file in *.scrb
do
  sed 's/          /    /' $file > $file.tmp
  mv $file.tmp $file
done
echo "finished changing 10-space indents into 4-space indents" | tee -a $log_file

# change 8-space indents into 0-space indents
echo "begin changing 8-space indent into 0-space indents" | tee -a $log_file
for file in *.scrb
do
  sed 's/        //' $file > $file.tmp
  mv $file.tmp $file
done
echo "finished changing 8-space indents into 0-space indents" | tee -a $log_file

# change 10-space indent into 4-space indents
echo "begin changing 10-space indent into 4-space indents" | tee -a $log_file
for file in *.scrb
do
  sed 's/          /    /' $file > $file.tmp
  mv $file.tmp $file
done
echo "finished changing 4-space indents into blank lines" | tee -a $log_file

# change 4-space indents into a blank line
echo "begin changing 4-space indents into blank lines" | tee -a $log_file
for file in *.scrb
do
  sed 's/    / \n/' $file > $file.tmp
  mv $file.tmp $file
done
echo "finished changing 4-space indents into blank lines" | tee -a $log_file

# change 12-space indents into a blank line
echo "begin changing 12-space indents into blank lines" | tee -a $log_file
for file in *.scrb
do
  sed 's/            / \n/' $file > $file.tmp
  mv $file.tmp $file
done
echo "finished changing 12-space indents into blank lines" | tee -a $log_file

# change 13-space indents into a blank line
echo "begin changing 13-space indents into blank lines" | tee -a $log_file
for file in *.scrb
do
  sed 's/             / \n/' $file > $file.tmp
  mv $file.tmp $file
done
echo "finished changing 13-space indents into blank lines" | tee -a $log_file

# change 30-space indents into a blank line
echo "begin changing 30-space indents into blank lines" | tee -a $log_file
for file in *.scrb
do
  sed 's/                              / \n/' $file > $file.tmp
  mv $file.tmp $file
done
echo "finished changing 30-space indents into blank lines" | tee -a $log_file

# change 14-space indents into a blank line
echo "begin changing 14-space indents into blank lines" | tee -a $log_file
for file in *.scrb
do
  sed 's/              / \n/' $file > $file.tmp
  mv $file.tmp $file
done
echo "finished changing 14-space indents into blank lines" | tee -a $log_file

# change 2-space plus dash indents into a blank line
echo "begin changing 2-space plus dash indents into blank lines" | tee -a $log_file
for file in *.scrb
do
  sed 's/  --/ \n  --/' $file > $file.tmp
  mv $file.tmp $file
done
echo "finished changing 2-space plus dash indents into blank lines" | tee -a $log_file

# change empty line into a blank line with one space
echo "begin changing empty line into blank lines with one space" | tee -a $log_file
for file in *.scrb
do
  sed 's/^$/ /' $file > $file.tmp
  mv $file.tmp $file
done
echo "finished changing empty line into blank lines with one space" | tee -a $log_file

# insert blank line before dash line
echo "begin insert blank line before dash line" | tee -a $log_file
for file in *.scrb
do
  sed 's/---------------------------------------------------------------------------/ \n--------------------------------------------------------------------------- /' $file > $file.tmp
  mv $file.tmp $file
done
echo "finished inserting blank line before dash line" | tee -a $log_file

# remove empty lines
# echo "begin removal of empty lines" | tee -a $log_file
# for file in *.scrb
# do
#   sed '/^$/d' $file > $file.tmp
#   mv $file.tmp $file
# done
# echo "finished deleting empty lines" | tee -a $log_file

# unwrap using an emacs function
echo "begin emacs function emacs" | tee -a $log_file
for file in *.scrb
do
  emacs --batch -l $home/.emacs --file $file --funcall unwrap --funcall save-buffer
done
echo "finished emacs function unwrap" | tee -a $log_file
  
# count final number of lines
echo "final number of lines:"
for file in *.scrb
do
  echo `wc -l $file` | tee -a $log_file
done
#+end_src

#+results:
#+begin_example
initial number of lines:
5455 chrg-105shrg39641.txt
6100 chrg-106hhrg59654.txt
50926 chrg-107shrg70756.txt
52888 chrg-107shrg78480.txt
2653 chrg-107shrg79324.txt
4512 chrg-107shrg79941.txt
3085 chrg-107shrg80479.txt
2749 chrg-107shrg81758.txt
4413 chrg-108hhrg90162.txt
5089 chrg-108hhrg91364.txt
3421 chrg-108hhrg91861.txt
3617 chrg-108hhrg92309.txt
4345 chrg-108hhrg92513.txt
3498 chrg-108hhrg92756.txt
3592 chrg-108hhrg93983.txt
3582 chrg-108hhrg94513.txt
2881 chrg-108shrg1910410.txt
35917 chrg-108shrg85932.txt
2892 chrg-108shrg94491.txt
5092 chrg-108shrg94993.txt
3204 chrg-109hhrg20424.txt
3010 chrg-109hhrg21648.txt
3075 chrg-109hhrg23691.txt
7344 chrg-109hhrg26125.txt
6409 chrg-109hhrg26798.txt
5762 chrg-109hhrg27978.txt
3489 chrg-109hhrg27985.txt
3389 chrg-109hhrg28431.txt
3864 chrg-109hhrg28839.txt
4329 chrg-109hhrg29626.txt
5269 chrg-109shrg20732.txt
2278 chrg-109shrg21951.txt
2560 chrg-109shrg22340.txt
3642 chrg-109shrg26056.txt
3131 chrg-109shrg26112.txt
4236 chrg-109shrg26353.txt
4725 chrg-109shrg26426.txt
43186 chrg-109shrg27036.txt
2768 chrg-109shrg27768.txt
3245 chrg-109shrg28848.txt
24655 chrg-109shrg49104164.txt
5323 chrg-109shrg49104171.txt
4185 chrg-109shrg49104190.txt
23014 chrg-109shrg59104229.txt
7838 chrg-109shrg97751.txt
50865 chrg-109shrg99869.txt
7718 chrg-110hhrg33801.txt
4734 chrg-110hhrg34015.txt
2846 chrg-110hhrg34016.txt
4027 chrg-110hhrg34017.txt
4412 chrg-110hhrg34174.txt
3901 chrg-110hhrg34417.txt
3119 chrg-110hhrg34604.txt
5193 chrg-110hhrg34631.txt
5902 chrg-110hhrg34990.txt
7830 chrg-110hhrg35233.txt
4573 chrg-110hhrg35664.txt
4618 chrg-110hhrg35842.txt
20332 chrg-110hhrg37638.txt
4589 chrg-110hhrg38056.txt
3180 chrg-110hhrg41066.txt
5085 chrg-110hhrg42335.txt
5983 chrg-110hhrg43311.txt
5173 chrg-110hhrg43470.txt
3131 chrg-110hhrg44214.txt
8616 chrg-110jhrg33757.txt
3249 chrg-110shrg33885.txt
60188 chrg-110shrg33926.txt
6772 chrg-110shrg34052.txt
4132 chrg-110shrg35072.txt
6753 chrg-110shrg35329.txt
2561 chrg-110shrg37293.txt
6922 chrg-110shrg45589.txt
23774 chrg-110shrg69104283.txt
7129 chrg-111hhrg47611.txt
3161 chrg-111hhrg48732.txt
5524 chrg-111hhrg49499.txt
4834 chrg-111hhrg52859.txt
6161 chrg-111hhrg53373.txt
4804 chrg-111hhrg53732.txt
4468 chrg-111hhrg55304.txt
9154 chrg-111hhrg58234.txt
10416 chrg-111shrg52739.txt
9886 chrg-111shrg52939.txt
5323 chrg-111shrg55474.txt
4084 chrg-111shrg67045.txt
3387 chrg-112hhrg64229.txt
4089 chrg-112hhrg64657.txt
5172 chrg-112hhrg64795.txt
remove old scrubbed files
begin extracting statements and letters
deleting lines with tiff
finished deleting lines with tiff
begin changing 10-space indents into 4-space indents
finished changing 10-space indents into 4-space indents
begin changing 8-space indent into 0-space indents
finished changing 8-space indents into 0-space indents
begin changing 10-space indent into 4-space indents
finished changing 4-space indents into blank lines
begin changing 4-space indents into blank lines
finished changing 4-space indents into blank lines
begin changing 12-space indents into blank lines
finished changing 12-space indents into blank lines
begin changing 13-space indents into blank lines
finished changing 13-space indents into blank lines
begin changing 30-space indents into blank lines
finished changing 30-space indents into blank lines
begin changing 14-space indents into blank lines
finished changing 14-space indents into blank lines
begin changing 2-space plus dash indents into blank lines
finished changing 2-space plus dash indents into blank lines
begin changing empty line into blank lines with one space
finished changing empty line into blank lines with one space
begin insert blank line before dash line
finished inserting blank line before dash line
begin emacs function emacs
finished emacs function unwrap
final number of lines:
1268 chrg-105shrg39641.scrb
853 chrg-106hhrg59654.scrb
617 chrg-107shrg70756.scrb
664 chrg-107shrg78480.scrb
214 chrg-107shrg79324.scrb
526 chrg-107shrg79941.scrb
601 chrg-107shrg80479.scrb
552 chrg-107shrg81758.scrb
632 chrg-108hhrg90162.scrb
970 chrg-108hhrg91364.scrb
416 chrg-108hhrg91861.scrb
424 chrg-108hhrg92309.scrb
308 chrg-108hhrg92513.scrb
693 chrg-108hhrg92756.scrb
530 chrg-108hhrg93983.scrb
302 chrg-108hhrg94513.scrb
574 chrg-108shrg1910410.scrb
304 chrg-108shrg85932.scrb
288 chrg-108shrg94491.scrb
1018 chrg-108shrg94993.scrb
380 chrg-109hhrg20424.scrb
376 chrg-109hhrg21648.scrb
304 chrg-109hhrg23691.scrb
1178 chrg-109hhrg26125.scrb
1346 chrg-109hhrg26798.scrb
900 chrg-109hhrg27978.scrb
450 chrg-109hhrg27985.scrb
432 chrg-109hhrg28431.scrb
564 chrg-109hhrg28839.scrb
718 chrg-109hhrg29626.scrb
1224 chrg-109shrg20732.scrb
466 chrg-109shrg21951.scrb
468 chrg-109shrg22340.scrb
611 chrg-109shrg26056.scrb
565 chrg-109shrg26112.scrb
787 chrg-109shrg26353.scrb
876 chrg-109shrg26426.scrb
824 chrg-109shrg27036.scrb
310 chrg-109shrg27768.scrb
611 chrg-109shrg28848.scrb
192 chrg-109shrg49104164.scrb
1638 chrg-109shrg49104171.scrb
945 chrg-109shrg49104190.scrb
433 chrg-109shrg59104229.scrb
2933 chrg-109shrg97751.scrb
1099 chrg-109shrg99869.scrb
1614 chrg-110hhrg33801.scrb
732 chrg-110hhrg34015.scrb
479 chrg-110hhrg34016.scrb
945 chrg-110hhrg34017.scrb
1087 chrg-110hhrg34174.scrb
739 chrg-110hhrg34417.scrb
444 chrg-110hhrg34604.scrb
761 chrg-110hhrg34631.scrb
904 chrg-110hhrg34990.scrb
1537 chrg-110hhrg35233.scrb
788 chrg-110hhrg35664.scrb
872 chrg-110hhrg35842.scrb
3882 chrg-110hhrg37638.scrb
745 chrg-110hhrg38056.scrb
445 chrg-110hhrg41066.scrb
608 chrg-110hhrg42335.scrb
1268 chrg-110hhrg43311.scrb
980 chrg-110hhrg43470.scrb
382 chrg-110hhrg44214.scrb
2533 chrg-110jhrg33757.scrb
356 chrg-110shrg33885.scrb
73 chrg-110shrg33926.scrb
1705 chrg-110shrg34052.scrb
666 chrg-110shrg35072.scrb
524 chrg-110shrg35329.scrb
458 chrg-110shrg37293.scrb
1764 chrg-110shrg45589.scrb
103 chrg-110shrg69104283.scrb
1355 chrg-111hhrg47611.scrb
452 chrg-111hhrg48732.scrb
810 chrg-111hhrg49499.scrb
897 chrg-111hhrg52859.scrb
1428 chrg-111hhrg53373.scrb
835 chrg-111hhrg53732.scrb
799 chrg-111hhrg55304.scrb
145 chrg-111hhrg58234.scrb
924 chrg-111shrg52739.scrb
1359 chrg-111shrg52939.scrb
1234 chrg-111shrg55474.scrb
851 chrg-111shrg67045.scrb
319 chrg-112hhrg64229.scrb
478 chrg-112hhrg64657.scrb
331 chrg-112hhrg64795.scrb
#+end_example

*** DONE Presidential documents						:sec:

The following script remove redundant lines from the presidential documents.  There is no file-specific scrubbing.  Files starting with WCPD in their file name have line breaks at the end of each line and thus have no proper paragraph formatting.  I had to manually reconstruct the paragraphs using an Emacs function (http://www.gnu.org/software/emacs).

# use Alt-x unwrap

#+srcname: pres-docs-scrub
#+begin_src sh :tangle ~/Dropbox/dissertation/data/pres-docs-scrub.sh :results output
#!/bin/bash

# define variables
home_dir=$home/Dropbox/dissertation/data
data_dir=$home_dir/pres-docs
log_file=$home_dir/pres-docs-scrub.log

cd $data_dir
echo "removing old scrubbed files" | tee $log_file
rm *.scrb

echo "initial number of lines"
for file in *.txt
do
  echo `wc -l $file` | tee -a $log_file
done

echo "begin scrubbing of presidential documents"
for file in *.txt
do
  sed -e '/names: /,$d' \
      -e '/note: /,$d' \
      -e '/\[filed with /,4d' \
      -e '1,2d' \
      -e '/government printing office/d' \
      -e '/r04/d' \
      -e '/\[pages /d' \
      -e '/./,$!d' \
      -e '/\[\[page /d' $file > $file.tmp
done

for file in *.txt.tmp
do
  cp $file ${file%.txt.tmp}.scrb
done

# remove leftover html formatting
echo "removing leftover html formatting" | tee -a $log_file
for file in *.scrb
do
  sed -e 's/&mdash;/ /g' \
      -e 's/&quot;/ /g'  \
      -e 's/\[applause\]/ /g' \
      -e 's/\[applause\]/ /g' \
      -e 's/\[laughter\]/ /g' \
      -e 's/\[laughter\]/ /g' \
      -e 's/\[inaudible\]/ /g' $file > $file.tmp
  mv $file.tmp $file
done

# remove lines starting with "audience members"
echo "deleting lines starting with audience members" | tee -a $log_file
for file in *.scrb
do
  sed '/audience members./d' $file > $file.tmp
  mv $file.tmp $file
done
echo "finished deleting lines with tiff" | tee -a $log_file

# remove temp files
rm *.tmp

echo "final number of lines:"
for file in *.scrb
do 
  echo `wc -l $file` | tee -a $log_file
done
#+end_src

** DONE Import text files into /RQDA/					:chp:
# generate scripts: C-c C-v t
*** DONE Introduction							:sec:
  :PROPERTIES:
  :table_export_file: ~/dropbox/dissertation/data/file-categories.tsv
  :table_export_format: orgtbl-to-tsv
  :END:

# alt-x org-table-export

In this step I began the computer assisted qualitative data analysis.  The software used is the /R/ package /RQDA/.

Once the data sources were "scrubbed" they were ready to be imported into /RQDA/, the QDA software.  /RQDA/ organizes the data into 'projects', each project has as back-end a SQLite database file (http://www.sqlite.org).  The text of each file is inserted into a database field 'file' of the table 'source'.  In addition each project has a description which is stored in the 'project' table.  During later steps several other tables in the databases were populated.  Sometimes I could use scripts to enter data and sometimes I had to do this manually.  This will be shown in the following subsections of this chapter.

I wrote a script for each of the two document collections, Presidential Documents and Congressional Hearings.  These scripts (1) loaded the scrubbed plain text files and (2) inserted a project description.  These \emph{R} scripts were based on code given to me by the developer of /RQDA/, Wincent Ronggui Huang (http://homepage.fudan.edu.cn/rghuang/cv/).  The /R/ scripts themselves are shown here below.  This appendix also shows the /R/ commands used to create and open the /RQDA/ projects and then run the /R/ scripts.

These /R/ scripts and command performed the operations corresponding to the red box "import into RQDA" of the work flow diagram.

In this appendix are the /R/ scripts and commands used to (1) create the /RQDA/ project, (2) open the project, (3) load the text from the scrubbed files, (4) apply a file category id to the files in the project, and (4) add a description of the project.

The file category is only relevant if we have different types of files in the same project.  This is not the case in this research.  I have only implemented this feature for completeness.  Table \ref{tbl:file-categories} was used by the \emph{R} scripts to implement this feature.

#+tblname: file-categories
#+label: tbl:file-categories
#+caption: file categories
| catID | Name           | Description                  |
|-------+----------------+------------------------------|
|     1 | congr-hearings | congressional hearings       |
|     2 | pres-docs      | presidential documents       |

*** DONE Congressional hearings						:sec:

# generate scripts: c-c c-v t
To create the =congr-hearings= project and to import the congressional hearings into this project type the following at the /R/ prompt

#+begin_src R
library("RQDA")
setwd("~/Dropbox/dissertation/data")
rqda:::new_proj("congr-hearings.rqda")
openproject("~/Dropbox/dissertation/data/congr-hearings.rqda", updateGUI = true)
source("congr-hearings-imp-rqda.R")
#+end_src

#+begin_latex
\noindent
#+end_latex
This will run the following /R/ script that imports the appropriate text files into the newly created /RQDA/ project

#+srcname: congr-hearings-imp-rqda
#+begin_src R :tangle ~/Dropbox/dissertation/data/congr-hearings-imp-rqda.R
# remove old objects
#
rm(list = ls())

# define variables and set working directory
#
project.name <- "congr-hearings"
home.dir     <- "~/Dropbox/dissertation/data/"
data.dir     <- paste(home.dir, project.name, "/", sep = "")
sql          <- "update project set memo='congressional hearings collection'"
fcontent     <- list()
user         <- Sys.getenv("USER")
today        <- Sys.Date()
drv          <- dbDriver("sqlite")
con          <- dbConnect(drv, dbname = paste(project.name, ".rqda", sep = ""))

setwd(data.dir)

fnames <- list.files(pattern = ".scrb$")

for (i in 1:length(fnames)) fcontent[[i]] <- 
  paste(readlines(con = fnames[i]), collapse = "\n")
names(fcontent) <- fnames
write.filelist(fcontent)

# add project description
#
RQDAQuery(sql)

setwd(home.dir)

# read into a temporary data frame the names and memos from tsv file
#
temp <- read.delim("file-categories.tsv", header = TRUE,
                   colclasses = c("numeric","character","character"),
                   fill = FALSE)

# build the "filecat" data frame
#
DF.1 <- data.frame(name   = temp$name,
                   fid    = NA,
                   catid  = as.integer(temp$catid),
                   owner  = user,
                   date   = as.character(today),
                   datem  = NA,
                   memo   = temp$description,
                   status = as.integer(1), 
                   StringsAsFactors = FALSE)

# load file categories into database table
#
dbWriteTable(con, "filecat", DF.1, row.names = FALSE, overwrite = TRUE)

# build the "treefile" data frame
#
temp <- RQDAQuery("select id from source")

DF.2 <- data.frame(fid    = temp$id,
                   catid  = as.integer(1),
                   date   = as.character(today),
                   datem  = NA,
                   memo   = NA,
                   status = as.integer(1),
                   owner  = user,
                   StringsAsFactors = FALSE)

# populate the "treefile" table
#
dbWriteTable(con, "treefile", DF.2, row.names = FALSE, overwrite = TRUE)
dbDisconnect(con)
#+end_src

*** DONE Presidential documents						:sec:

To create the =pres-docs= project and to import the presidential documents into this project type the following at the /R/ prompt

#+begin_src R
library("RQDA")
setwd("~/Dropbox/dissertation/data")
rqda:::new_proj("pres-docs.rqda")
openproject("~/Dropbox/dissertation/data/pres-docs.rqda", updateGUI = TRUE)
source("pres-docs-imp-rqda.R")
#+end_src

#+begin_latex
\noindent
#+end_latex
This will run the following /R/ script that imports the appropriate text files into the newly created /RQDA/ project

#+srcname: pres-docs-imp-rqda
#+begin_src R :tangle ~/Dropbox/dissertation/data/pres-docs-imp-rqda.R
# remove old objects
#
rm(list = ls())

# define variables and set working directory
#
project.name <- "pres-docs"
home.dir     <- "~/Dropbox/dissertation/data/"
data.dir     <- paste(home.dir, project.name, sep = "")
sql          <- "update project set memo='presidential documents collection'"
fcontent     <- list()
user         <- Sys.getenv("USER")
today        <- Sys.Date()
drv          <- dbdriver("sqlite")
con          <- dbconnect(drv, dbname = paste(project.name, ".rqda", sep = ""))

setwd(data.dir)

fnames <- list.files(pattern = ".scrb$")

for (i in 1:length(fnames)) fcontent[[i]] <- 
  paste(readlines(con = fnames[i]), collapse = "\n")
names(fcontent) <- fnames
write.filelist(fcontent)

# add project description
#
RQDAQuery(sql)

setwd(home.dir)

# read into a temporary data frame the names and memos from tsv file
#
temp <- read.delim("file-categories.tsv", header = TRUE,
                   colclasses = c("numeric","character","character"),
                   fill = FALSE)

# build the "filecat" data frame
#
DF.1 <- data.frame(name   = temp$name,
                   fid    = NA,
                   catid  = as.integer(temp$catid),
                   owner  = user,
                   date   = as.character(today),
                   datem  = NA,
                   memo   = temp$description,
                   status = as.integer(1), 
                   StringsAsFactors = FALSE)

# load file categories into database table
#
dbWriteTable(con, "filecat", DF.1, row.names = FALSE, overwrite = TRUE)

# build the "treefile" data frame
#
temp <- RQDAQuery("select id from source")

DF.2 <- data.frame(fid    = temp$id,
                   catid  = as.integer(3),
                   date   = as.character(today),
                   datem  = NA,
                   memo   = NA,
                   status = as.integer(1),
                   owner  = user,
                   StringsAsFactors = FALSE)

# populate the "treefile" table
#
dbWriteTable(con, "treefile", DF.2, row.names = FALSE, overwrite = TRUE)
dbDisconnect(con)
#+end_src

** DONE Apply attributes to files in /RQDA/				:chp:
# C-c C-v t to generate the R scripts

*** DONE Introduction							:sec:

There were three levels of analysis of the document collections as described in table \ref{tbl:level-analysis}.

#+tblname: level-analysis
#+LABEL: tbl:level-analysis
#+CAPTION: Levels of analysis of the data
#+Attr_LaTeX: placement=[htbp]
| Level | Unit of Analysis | Descriptors     |
|-------+------------------+-----------------|
|     1 | File             | File attributes |
|     2 | Statement        | Case attributes |
|     3 | Paragraph        | QDA Codes       |
|     4 | Word             | text mining     |
|-------+------------------+-----------------|

The first is at the level of document itself.  In this stage I assigned attributes to the plain text files that were imported into the *qda* software program, /RQDA/. This chapter corresponds to the step "Apply attributes to files". 

The second is the level of statement.  I define statement as a part of text that is from the same author and stems from the same written source.  In the Presidential Documents usually each file is one statement.  The exceptions are the debates, where there are two statements, one for each debater.  In /RQDA/ I use a structure called *case* to analyse these statements.

The third is the level of paragraph, defined as a portion of text between two empty lines or starting with an indentation.  In /RQDA/ I use *codes* to analyse relevant paragraphs.

The fourth and lowest level are the words themselves.  I use *text mining* and the /R/ package /tm/ to perform analysis at this level.

In this chapter I describe the method for the *file* level analysis. I wrote collection specific /R/ scripts to first populate the "attributes" database table in the /RQDA/ project database. this database table contains the names and descriptions of the file attributes and is common to all /RQDA/ projects (see table \ref{tbl:file-attributes}). The table is exported as a tab separated values file, =attributes.tsv=. the /R/ script will read this file and populate the "attributes" table. then the script will load the actual attribute values for each of the files of this project into the "fileAttr" table that contains the file specific attributes. some of the data will be extracted from the specific =<collection>-url.lst= file that contains the URLs of the documents that were searched for in FDsys (see [[search-url][Search FDsys]]). the missing information will be the author, title, and date of the documents, which are very difficult to automatically extract from the text of the document files.

The following table describes the file attributes used for all document collections

#+tblname: file-attributes
#+label: tbl:file-attributes
#+caption: file attributes
| name        | description                                          |
|-------------+------------------------------------------------------|
| DocDate     | File: date of release of document                    |
| DocType     | File: type of document, i.e. collection              |
| DocId       | File: serial or identification number of document    |
| Title       | File: title of document                              |
| Author      | File: name of author, person or institution          |
| AuthorCat   | File: category of author, e.g. Senator, House        |
| Institution | File: name of institution that released the document |

Notice that all the descriptions begin with "file: ...". This is to distinguish them from case attributes that will be loaded later (see [[case-attributes][Use RQDA cases for discourse analysis]]).

To see the loaded file attributes start /R/, then /RQDA/, and open the /RQDA/ project and click on the "attributes" tab. you will see the loaded attributes in alphabetic order. It is also possible to check the file attributes by entering at the /R/ prompt the following statement after having opened the /RQDA/ project

#+srcname: check-attributes
#+begin_src R
RQDAQuery("select name, memo from attributes where status==1 and memo like 'File:%'")
#+end_src

See each document collection section for details.  These /R/ scripts perform the operations corresponding to the step "Apply attributes to file" of the work flow diagram.

*** DONE Table of file and case attributes		       :noexport:sec:
  :PROPERTIES:
  :table_export_file: ~/Dropbox/dissertation/data/attributes.tsv
  :table_export_format: orgtbl-to-tsv
  :END:

# alt-x org-table-export

Table with file attributes in upper panel and case attributes in lower panel. Both types of attributes are loaded into the same table, "attributes".

#+tblname: attributes
#+label: tbl:attributes
#+caption: File and Case attributes
| Name              | Description                                                        | Type      |
|-------------------+--------------------------------------------------------------------+-----------|
| Author            | File: name of author, person or institution [text]                 | character |
| AuthorCat         | File: category of author, e.g. Senator, House [text]               | character |
| DocDate           | File: date of release of document [yyyy-mm-dd]                     | character |
| DocId             | File: serial or identification number of document [text]           | character |
| DocType           | File: type of document, i.e. collection [text]                     | character |
| Institution       | File: name of institution that released the document [text]        | character |
| Title             | File: title of document [text]                                     | character |
|-------------------+--------------------------------------------------------------------+-----------|
| CaseAuthor        | Case: author of the statement                                      | character |
| AuthorAudience    | Case: audience of the statement                                    | character |
| AuthorAffiliation | Case: political party or organization of the author                | character |
| AuthorMessage     | Case: message of the author                                        | character |
| AuthorAssIdeas    | Case: ideas are taken for granted as if there were no alternatives | character |
| CaseDate          | Case: date of statement [yyyy-mm-dd]  (from DocDate)               | character |

*** DONE Presidential documents						:sec:

To populate the "attributes" and "fileattr" tables in the /RQDA/ project =pres-docs= type the following at the /R/ prompt

#+begin_src R
library("RQDA")
openproject("~/Dropbox/dissertation/data/pres-docs.rqda", updateGUI = TRUE)
setwd("~/Dropbox/dissertation/data")
source("pres-docs-attr.R")
#+end_src

#+begin_latex
\noindent
#+end_latex
This will run the following /R/ script

#+srcname: pres-docs-attr
#+begin_src R :tangle ~/Dropbox/dissertation/data/pres-docs-attr.R
# remove old objects
#
rm(list = ls())

# define variables
#
project.name <- "pres-docs"
home.dir     <- "~/Dropbox/dissertation/data"
user         <- Sys.getenv("USER")
today        <- Sys.Date()
doc.type     <- project.name
author.cat   <- "president"
institution  <- "presidency"
sql          <- "select name from attributes where memo like 'File%'"

setwd(home.dir)
drv <- dbDriver("sqlite")
con <- dbConnect(drv, dbname = paste(project.name, ".rqda", sep = ""))

# read into a temporary data frame the names and memos from tsv file
temp <- read.delim("attributes.tsv", header = TRUE,
                   colclasses = c("character","character","character"),
                   fill = FALSE)

# build the "attributes" data frame
DF.1 <- data.frame(name   = temp$name,
                   status = as.integer(1), 
                   date   = as.character(today), 
                   datem  = NA, 
                   owner  = user, 
                   memo   = temp$description,
                   class  = as.character(temp$type),
                   StringsAsFactors = FALSE)

# load attributes into database table
#
dbWriteTable(con, "attributes", DF.1, row.names = FALSE, overwrite = TRUE)

# ###############################################
# define variables to populate table "fileattr"
# ###############################################
temp <- RQDAQuery(sql)

my.variables  <- temp$name
num.var       <- length(my.variables)
 
# read into a temporary data frame from pres-docs tsv file
temp <- read.delim("pres-docs.tsv", header = TRUE,
                    colclasses = rep("character", times = 4), fill = FALSE)
names(temp)
 
num.row <- nrow(temp)        # 127
tot.row <- num.row * num.var # 889 = 127 * 7

# ##########################################
# create vector of value and then populate
# ##########################################
my.values <- vector(mode = "list", tot.row)

for (i in 1:num.row)   # author
{
  my.values[i*num.var-6] <- temp[i,3]
}
for (i in 1:num.row)   # author_cat
{
  my.values[i*num.var-5] <- author.cat
}
for (i in 1:num.row)   # doc_date
{
  my.values[i*num.var-4] <- temp[i,2]
}
for (i in 1:num.row)   # doc_id
{
  my.values[i*num.var-3] <- temp[i,1] 
}
for (i in 1:num.row)   # doc_type
{
  my.values[i*num.var-2] <- doc.type
}
for (i in 1:num.row)   # institution
{
  my.values[i*num.var-1] <- institution
}
for (i in 1:num.row)    # title
{
  my.values[i*num.var-0] <- temp[i,4]
}
   
# make num.row copies of each row, here 127
rep.variables <- rep(my.variables, times = num.row)

# build the data frame
DF.2 <- data.frame(cbind(variable = as.character(rep.variables),
                         value    = as.character(my.values),
                         fileid   = as.integer(rep(1:num.row, each=num.var)),
                         date     = as.character(today),
                         datem    = NA,
                         owner    = as.character(user),
                         status   = as.integer(1)),
                         StringsAsFactors = FALSE)

dbWriteTable(con, "fileAttr", DF.2, row.names = FALSE, overwrite = TRUE)
dbDisconnect(con)
#+end_src

#+begin_latex
\noindent
#+end_latex
To check the values for a certain attribute (e.g. "title") run the following at the /R/ prompt

#+begin_src R
library("RQDA")
openProject("~/Dropbox/dissertation/data/pres-docs.rqda", updateGUI = TRUE)
getAttr(type = "file", attrs = "<attribute name>")
#+end_src

*** DONE Congressional hearings						:sec:

# C-c C-v t to generate the R script
# C-c C-c to run the bash script

To populate the "attributes" and "fileAttr" tables in the /RQDA/ project =congr-hearings= type the following at the /R/ prompt

#+begin_src R
library("RQDA")
openProject("~/Dropbox/dissertation/data/congr-hearings.rqda", updateGUI = TRUE)
setwd("~/Dropbox/dissertation/data")
source("congr-hearings-attr.R")
#+end_src

#+begin_latex
\noindent
#+end_latex
This will run the following /R/ script

#+srcname: congr-hearings-attr
#+begin_src R :tangle ~/Dropbox/dissertation/data/congr-hearings-attr.R
# remove old objects
#
rm(list = ls())

project.name <- "congr-hearings"
home.dir     <- "~/Dropbox/dissertation/data"
user         <- Sys.getenv("USER")
today        <- Sys.Date()
doc.type     <- project.name
author       <- "various"
author.cat   <- "various"
sql          <- "select name from attributes where memo like 'File%'"

setwd(home.dir)
drv <- dbDriver("sqlite")
con <- dbConnect(drv, dbname = paste(project.name, ".rqda", sep = ""))

# read into a temporary data frame the names, memos, and types from tsv file
temp <- read.delim("attributes.tsv", header = TRUE,
                   colclasses = c("character","character","character"),
                   fill = FALSE)

# build the "attributes" data frame
DF.1 <- data.frame(name   = temp$name,
                   status = as.integer(1), 
                   date   = as.character(today), 
                   datem  = NA, 
                   owner  = user, 
                   memo   = temp$description,
                   class  = as.character(temp$type),
                   StringsAsFactors = FALSE)

# load attributes into "attributes" table
#
dbWriteTable(con, "attributes", DF.1, row.names = FALSE, overwrite = TRUE)

# ###############################################
# define variables to populate table "fileattr"
# ###############################################
temp <- RQDAQuery(sql)

my.variables  <- temp$name
num.var       <- length(my.variables)
 
# read into a temporary data frame from congr-hearings tsv file
#
temp <- read.delim("congr-hearings.tsv", header = TRUE,
                   colclasses = rep("character", times = 4), fill = FALSE)

names(temp)
 
num.row <- nrow(temp)        # xxx
tot.row <- num.row * num.var # xxx * x = xxx 

# ########################################### 
# create vector of values and then populate
# ###########################################
my.values <- vector(mode = "list", tot.row)
 
for (i in 1:num.row)    # author
{
  my.values[i*num.var-6] <- author
}
for (i in 1:num.row)    # author_cat
{
  my.values[i*num.var-5] <- author.cat
}
for (i in 1:num.row)    # doc_date
{
  my.values[i*num.var-4] <- temp[i,2]
}
for (i in 1:num.row)    # doc_id
{
  my.values[i*num.var-3] <- temp[i,1]
}
for (i in 1:num.row)    # doc_type
{ 
  my.values[i*num.var-2] <- doc.type
}
for (i in 1:num.row)    # institution
{
  my.values[i*num.var-1] <- temp[i,3]
}
for (i in 1:num.row)    # title
{
  my.values[i*num.var-0] <- temp[i,4]
}
   
# make num.row copies of each row, here 127
rep.variables <- rep(my.variables, times = num.row)

# build the data frame
DF.2 <- data.frame(cbind(variable = as.character(rep.variables),
                         value    = as.character(my.values),
                         fileid   = as.integer(rep(1:num.row, each = num.var)),
                         date     = as.character(today),
                         datem    = NA,
                         owner    = as.character(user),
                         status   = as.integer(1)),
                         StringsAsFactors = FALSE)

dbWriteTable(con, "fileAttr", DF.2, row.names = FALSE, overwrite = TRUE)
dbDisconnect(con)
#+end_src

#+begin_latex
\noindent
#+end_latex
To check the values for a certain attribute (e.g. "title") run the following at the /R/ prompt

#+begin_src R
library("RQDA")
openProject("~/Dropbox/dissertation/data/congr-hearings.rqda", updateGUI = TRUE)
getAttr(type = "file", attrs = "<attribute name>")
#+end_src

** STRT Define and describe statements in /RQDA/			:chp:
# C-c C-v t to generate the R scripts
# ####################################################################
# case = statement
# single-statement = congr-docs, pres-docs non debates
# multi-statement  = congr-hearings, eric-docs, pres-docs debates
# ####################################################################

*** STRT Introduction							:sec:

This is the 3rd level of analysis, the level of statement.  Each of the documents contained one or more statements.  In the case of the Presidential Documents, each file corresponded to only one statement.  On the contrary, the Congressional Hearings were comprised of several statements.  I was only interested in statements with coded paragraphs.  Thus, procedurally I had to code the texts before describing the statements.

\emph{RQDA} allowed me this level of analysis by using constructs that it calls "cases".  The author of \emph{RQDA} implemented this feature to perform Qualitative Comparative Analysis (QCA, http://www.u.arizona.edu/~cragin/fsQCA/index.shtml).  QCA is an analytic technique that uses Boolean algebra.  It did not perform QCA, but took advantage of these "cases" to assist me in the analysis of the statements.  Simply, each statement with coded paragraphs was assigned to a case and each case was assigned attributes that described these statements.  How to do so is described in the on-line /RQDA/ manual, part 7 (http://rqda.r-forge.r-project.org/documentation_2.html\#manual).

Each statement has a set of attributes, an author, an audience, and a date, that were assigned.  The statement date, "CaseDate", is obviously the same as the date of the document.  The author has an affiliation with a political party or a public or private institution, school, company or organization.  In addition we can abstract a "message" of the author.  Unlike the other attributes, this one is not unambiguous.  I read the statement and would look for one or few sentence that would "jump out" to me.  I would pay special attention to the introduction or the conclusion of the statement and the coded parts of the text.  For these attributes see Table \ref{tbl:case-attributes}.

#+tblname: case-attributes
#+LABEL: tbl:case-attributes
#+CAPTION: Statement (case) attributes
| Statement attribute | Attribute description                         |
|---------------------+-----------------------------------------------|
| CaseAuthor          | Author of the statement                       |
| CaseDate            | Date of the statement, same of the document   |
| AuthorAudience      | Audience of the statement                     |
| AuthorAffiliation   | Political party or organization of the author |
| AuthorMessage       | Message of the author                         |
|---------------------+-----------------------------------------------|

In the =congr-bills= and =pres-docs= projects each statement corresponds to one file, while in the =congr-hearings= and =eric-docs= each file contains more than one statement (case).  I have written scripts for each project that loads all the cases into the project database by populating the "caseAttr", "cases", and "caselinkage" tables.  The case attributes were loaded already with the file attributes previously (see [[File-attributes][Apply attributes to files in RQDA]]).  The descriptions (memos) of all case attributes start with "Case: ...".   A complication exists for one =pres-docs= document.  That is, this document consists of a presidential debate, thus there are two authors for these documents.

The only attribute that can be assigned automatically is the "CaseDate" because its value corresponds to the "DocDate" of the file where the statement was derived.

From this point on, the process was interactive where I read the statements and answered the questions (entering the values of the case attributes).  Operationally, in the /RQDA/ GUI I proceeded as follows (1) click on "cases" tab and open a case, (2) select a portion of text, (3) click on the "link" button, (4) right-click on the case and select "add/modify attributes" and enter the answer to the cda question.

The results can be displayed by entering the following commands at the /R/ prompt

#+begin_src R
library("RQDA")
openProject("~/Dropbox/dissertation/data/<project>.rqda", updateGUI = TRUE)
sql  <- "select name from attributes where memo like 'Case:%' and status = 1'"
temp <- RQDAQuery(sql)
getAttr(type = "case", attrs = temp$name)
#+end_src

where <project> is the name of the project and <case attribute> is the name of one of the case attributes (surrounded by double quotes) corresponding to one of the answers to CDA questions #4 to #16.

*** DONE Presidential documents						:sec:
In the =pres-docs= collection each document corresponds to a single statement and thus corresponds to a single case.  The case attributes were already loaded into the "attributes" table together with the file attributes. 

Note that document WCPD-2004-10-18-Pg2364 is a debate between President George Bush and Senator John Kerry and is thus the exception in having 2 statements.

The only attribute that can be assigned automatically are the "CaseDate" because that corresponds to the "DocDate".

To add the case attributes to the "caseAttr", "cases", and "caselinkage" tables in this /RQDA/ project type the following at the /R/ prompt

#+begin_src R
library("RQDA")
openProject("~/Dropbox/dissertation/data/pres-docs.rqda", updateGUI = TRUE)
setwd("~/Dropbox/dissertation/data")
source("pres-docs-cases.R")
#+end_src

#+begin_latex
\noindent
#+end_latex
This will run the following /R/ script

#+srcname: pres-docs-cases
#+begin_src R :tangle ~/Dropbox/dissertation/data/pres-docs-cases.R
# remove old objects
#
rm(list = ls())

# define variables
#
project.name <- "pres-docs"
home.dir     <- "~/Dropbox/dissertation/data/"
user         <- Sys.getenv("USER")
today        <- Sys.Date()
sql          <- "select id, name, memo, file from source"
drv          <- dbDriver("SQLite")
con          <- dbConnect(drv, dbname = paste(project.name, ".rqda", sep = ""))

setwd(home.dir)

##################################
# single-file case
##################################
temp <- RQDAQuery(sql)

# build and load "cases" table
#
DF.1 <- data.frame(name   = gsub("\\.scrb", "", temp$name),
                   memo   = temp$memo,
                   owner  = user,
                   date   = as.character(today), 
                   dateM  = NA,
                   id     = as.integer(temp$id),
                   status = as.integer(1),
                   stringsAsFactors = FALSE)

dbWriteTable(con, "cases", DF.1, row.names = FALSE, overwrite = TRUE)

# build and load "caselinkage" table
# make "caseid" equal to "fid"
#
DF.2 <- data.frame(caseID   = as.integer(temp$id),
                   fid      = as.integer(temp$id),
                   selfirst = as.integer(1),
                   selend   = nchar(temp$file),
                   status   = as.integer(1),
                   owner    = user,
                   date     = as.character(today),
                   memo     = NA,
                   stringsAsFactors = FALSE)

dbWriteTable(con, "caselinkage", DF.2, row.names = FALSE, overwrite = TRUE)

# build and populate "caseAttr" table
#
sql <- "select fileID, value from fileattr where variable='DocDate'"
temp <- RQDAQuery(sql)

DF.3 <- data.frame(variable = "CaseDate",
                   value    = as.character(temp$value),
                   caseID   = as.integer(temp$fileID),
                   date     = as.character(today),
                   dateM    = NA,
                   owner    = user,
                   status   = as.integer(1),
                   stringsasfactors = FALSE)
 
dbWriteTable(con, "caseAttr", DF.3, row.names = FALSE, overwrite = TRUE)

DF.4 <- data.frame(variable = "DegreeCertainty",
                   value    = as.integer(3),
                   caseID   = as.integer(temp$fileID),
                   date     = as.character(today),
                   dateM    = NA,
                   owner    = user,
                   status   = as.integer(1),
                   stringsAsFactors = FALSE)

dbWriteTable(con, "caseAttr", DF.4, row.names = FALSE, overwrite = FALSE, append = TRUE)

DF.5 <- data.frame(variable = "DegreeAuthority",
                   value    = as.integer(3),
                   caseID   = as.integer(temp$fileID),
                   date     = as.character(today),
                   dateM    = NA,
                   owner    = user,
                   status   = as.integer(1),
                   stringsAsFactors = FALSE)

dbWriteTable(con, "caseAttr", DF.5, row.names = FALSE, overwrite = FALSE, append = TRUE)

###############################
cat("populate 'CaseAuthor'\n")
###############################
sql  <- "select fileID, value from fileattr where variable='Author'"
temp <- RQDAQuery(sql)

DF.6 <- data.frame(variable = "CaseAuthor",
                   value    = temp$value,
                   caseID   = as.integer(temp$fileID),
                   date     = as.character(today),
                   dateM    = NA,
                   owner    = user,
                   status   = as.integer(1),
                   stringsAsFactors = FALSE)

dbWriteTable(con, "caseAttr", DF.6, row.names = FALSE, overwrite = FALSE, append = TRUE)
dbDisconnect(con)
#+end_src

*** DONE Congressional hearings						:sec:
In the =congr-hearings= collection each document contains more than one statement, each with its own author.  This being the situation, nothing can be automated.  Cases were assigned according to the instructions on http://rqda.r-forge.r-project.org/documentation_2.html#manual.  Then, values were assigned to Case Attributes manually using the GUI.

# Each of these statements was assigned to a case. to add the case attributes to the "caseAttr", "cases" and "caselinkage" tables in this /RQDA/ project type the following at the /R/ prompt

# #+begin_src R
# setwd("~/Dropbox/dissertation/data")
# source("congr-hearings-cases.R")
# #+end_src

# #+begin_latex
# \noindent
# #+end_latex
# This will run the following /R/ script

# #+srcname: congr-hearings-cases
# #+begin_src R :tangle ~/Dropbox/dissertation/data/congr-hearings-cases.R
# # remove old objects
# #
# rm(list = ls())

# # define variables
# #
# project.name <- "congr-hearings"
# home.dir     <- "~/Dropbox/dissertation/data/"
# user         <- Sys.getenv("USER")
# today        <- Sys.Date()
# drv          <- dbDriver("SQLite")
# con          <- dbConnect(drv, dbname = paste(project.name, ".rqda", sep = ""))

# setwd(home.dir)
# dbDisconnect(con)
# #+end_src

** STRT Summary results of statement analysis in /RQDA/			:chp:
In this chapter are the scripts used to provide summary statistics of the statement descriptions.

*** STRT Presidential Documents
The following script will give some aggregate results of the attribution to statements

#+srcname: pres-docs-case-results
#+begin_src r :tangle ~/Dropbox/dissertation/data/pres-docs-case-results.R
# remove old objects
#
rm(list = ls())

# define variables
#
project.name <- "pres-docs"
home.dir     <- "~/Dropbox/dissertation/data/"
user         <- Sys.getenv("USER")
today        <- Sys.Date()
sql          <- "select name from attributes where memo like 'Case:%' and status = 1"
drv          <- dbDriver("SQLite")
con          <- dbConnect(drv, dbname = paste(project.name, ".rqda", sep = ""))

setwd(home.dir)

#######################
# Number of statements
#######################
sql <- "select count(id) from cases where status == 1;"
RQDAQuery(sql)

###################################
# printout of all attribute values
###################################
temp <- RQDAQuery(sql)
getAttr(type = "case", attrs = temp$name)

###############################
# list of statement authors
###############################
sql <- "select distinct value from caseAttr where variable = 'CaseAuthor'"
RQDAQuery(sql)

##################################
# number of statements per author
##################################
RQDAQuery("select count(value) from caseAttr where value = 'Barack Obama'")
RQDAQuery("select count(value) from caseAttr where value = 'William Clinton'")
RQDAQuery("select count(value) from caseAttr where value = 'George W. Bush'")
RQDAQuery("select count(value) from caseAttr where value = 'John Kerry'")

############################################
# messages of the statement for each author
############################################
sql <- "select value from caseAttr where variable = 'AuthorMessage'
 and caseID in (select caseID from caseAttr where value = 'Barack Obama')"
RQDAQuery(sql)

sql <- "select value from caseAttr where variable = 'AuthorMessage'
 and caseID in (select caseID from caseAttr where value = 'William Clinton')"
RQDAQuery(sql)

sql <- "select value from caseAttr where variable = 'AuthorMessage'
 and caseID in (select caseID from caseAttr where value = 'George W. Bush')"
RQDAQuery(sql)

sql <- "select value from caseAttr where variable = 'AuthorMessage'
 and caseID in (select caseID from caseAttr where value = 'John Kerry')"
RQDAQuery(sql)
#+end_src

*** STRT Congressional Hearings

The following script will give some aggregate results of the attribution to statements

#+srcname: congr-hearings-case-results
#+begin_src r :tangle ~/Dropbox/dissertation/data/congr-hearings-case-results.R
# remove old objects
#
rm(list = ls())

# load library
#
library("sqldf")

# define variables
#
project.name <- "congr-hearings"
home.dir     <- "~/Dropbox/dissertation/data/"
user         <- Sys.getenv("USER")
today        <- Sys.Date()

setwd(home.dir)

###################################
# all attribute values
###################################
sql  <- "select name from attributes where memo like 'Case:%' and status = 1"
case.names <- RQDAQuery(sql)
temp   <- getAttr(type = "case", attrs = case.names$name)
mydata <- data.frame(ID          = temp$case, 
                     author      = temp$CaseAuthor,
                     affiliation = temp$AuthorAffiliation,
                     audience    = temp$AuthorAudience,
                     message     = temp$AuthorMessage,
                     stringsAsFactors=FALSE)
str(mydata)

#######################
# number of statements
#######################
sqldf("select count(*) as NumberOfStatements from mydata")

####################
# number of authors
####################
sqldf("select count(distinct author) as NumberOfAuthors from mydata")

#######################################
# list author and number of statements
#######################################
my.cases <- sqldf("select author as Author, count(author) NumberOfStatements
  from mydata group by author")
write.table(my.cases, file="congr-hearings-case-authors.tsv", 
  row.names=FALSE, quote=FALSE, sep="\t")

####################################
# list of authors and statement IDs
####################################
cases.list <- sqldf("select author as Author, ID from mydata
  order by author, ID")
write.table(cases.list, file="congr-hearings-case-list.tsv", row.names=FALSE,
  quote=FALSE, sep="\t")

#########################################
# list audience and number of statements
#########################################
sqldf("select audience as Audience, count(audience) as NumberOfStatements
  from mydata group by audience")

#########################
# list messages by ID
#########################
messages.list <- sqldf("select ID, message from mydata order by ID")
write.table(messages.list, file="congr-hearings-case-messages.tsv",
 row.names=FALSE, quote=FALSE, sep="\t")

#####################################
# list statements by specific author
#####################################
sqldf("select ID from mydata where author = '<author name>'")
#+end_src
** DONE Apply QDA codes to paragraphs in /RQDA/				:chp:
# use coding memo (case?) to record the author in a multi-author document, 
# e.g. congressional hearings

*** DONE Introduction							:sec:

This chapter and the next one are the "core" of the QDA process.  Here we define the QDA codes and code categories and add them to the QDA software. Then the codes are applied to the unit of analysis, the paragraph. In contrast, the unit of analysis of the cases are the statements. In QDA the interpretation and thoughts of the researcher are much more preeminent than in quantitative analysis, Thus any automatic application of codes is a preliminary step that has to be followed by a careful reading of the texts that will confirm or remove the applied codes.

In each of the document collection sections is an /R/ script. initially, the script populates the "freecode" table in the appropriate /RQDA/ project database.  The codes will be read from a tab separated value file, =codes.tsv=. Then the /R/ script will load the code categories (also called themes) into the "codecat" table. After that it will assign codes to the categories by populating the "treecode" table. Finally, the script will assign tentative codes to paragraphs in the documents based on key words according in table \ref{tbl:code-patterns} in the section [[code-patterns][code patterns]].  

Many paragraphs will be auto-coded.  However, by reading through the document I only kept the codes on paragraphs coded with /EducGap/ and related paragraphs.  Sometimes I added /EducGap/ when it is not auto-coded, but the meaning of the text demands it. If the label /EducGap/ is added because of a simple mention of "dissaggredated" then the code is not kept.  Differential drop-out is manually coded for /EducGap/, as are comparisons of graduation rates with other countries.

In the /RQDA/ gui click on the "files" tab and open and read each file.  Then confirm or remove the code.  other coded can be added.  Also the boundaries of the coding areas can be changed by first removing the coding and then adding it. The automatic coding will by necessity generate many 'false positives'.   For example, just the term 'achievement' will also label many paragraphs that have nothing to do with school or academic achievement.  I removed all codes applied to post-secondary education and early childhood education that have no reference to K-12 education. In addition I removed codings in section titles and references. these code removals and changes will create inactive records in the database table "coding". These inactive records can be removed by the following /R/ statements

#+begin_src R
library("RQDA")
openProject("~/Dropbox/dissertation/data/<project>.rqda", updateGUI = TRUE)
CleanProject()
#+end_src

The =CleanProject()= statement will remove all unused files, codes, code categories, and cases.

*** DONE Code categories				       :noexport:sec:
  :PROPERTIES:
  :table_export_file: ~/Dropbox/dissertation/data/themes.tsv
  :table_export_format: orgtbl-to-tsv
  :END:

# alt-x org-table-export

The code categories are based on foucault's three definitions of /governmentality/

#+label: fig:governmentality
#+caption: data analysis spiral
#+attr_latex: placement=[h!]
[[/home/henk/Dropbox/dissertation/graphic/governmentality.jpg]]

#+begin_quote
1. The ensemble formed by the /institutions/, /procedures/, /analyses and reflections/, the /calculations and tactics/ that allow the exercise of this very specific albeit complex form of power, which has as its /target/ population, as its principal /form of knowledge/ political economy, and as its essential /technical means/ apparatuses of security.

2. The /tendency/ which, over a long period and throughout the west, has steadily led towards the pre-eminence over all other forms (sovereignty, discipline, etc) of this type of power which may be termed government, resulting, on the one hand, in /formation of a whole series of specific governmental apparatuses/, and, on the other, in the development of a whole /complex of savoirs/.

3. The process, or rather the /result of the process/, through which the state of justice of the middle ages, transformed into the /administrative state/ during the fifteenth and sixteenth centuries, gradually becomes 'governmentalized'.

1. Tout d’abord, l’ensemble constitué par les /institutions/, les /procédures/, les /analyses et les réflexions/, les /calculs et les tactiques qui permet d’exercer un pouvoir ayant pour cible principale la population, pour forme majeure de savoir l’économie politique (cf. le mercantilisme) et pour instrument technique essentiel les dispositifs de sécurité (la « police »). 

2. Ensuite, la tendance qui dans tout l’occident conduit vers la prééminence d’un type de pouvoir particulier parmi d’autres : le gouvernement (les autres étant : la souveraineté, la discipline, etc.). 

3. Enfin, le processus ou plutôt le résultat du processus par lequel l’état de justice du moyen age, qui est devenu vers 1500, un état administratif, devient progressivement gouvernementalisé, c’est-à-dire s’est vu adjoindre de plus en plus de prérogatives sur la population
#+end_quote

The code categories (themes) listed in table \ref{tbl:themes} were defined /a priori/ based on the theory of /governmentality/.  GovGoal stands for "goals of governmentality", govknowledge for "knowledge of governmentality", govmeans for "means of governmentality", and histtendency for "historical tendency, outcome of historical process".  These categories are common for all document collections.

#+tblname: themes
#+label: tbl:themes
#+caption: code categories
| catid | name          | description                                              |
|-------+---------------+----------------------------------------------------------|
|     1 | nationgoal    | goal of the nation, society to be prosperous and stable  |
|     2 | rationality   | rationality, conceptualization of government, of society |
|     3 | techpower     | technology of power, procedures, institutions            |
|     4 | techself      | technology of self, conduct of one self                  |
|     5 | admintendency | historical tendency towards an administrative state      |

refer to the [[glossary][glossary]] for details on the code categories.  this classification of themes is based on my personal 'distillation' of foucault's treatment of /governmentality/ and that of his successors (e.g. dean, burchell, and rose).

in the /rqda/ gui the theme/code connections are graphically displayed by right-clicking a code category and selecting "plot selected code categories".  in figure \ref{fig:nation-goal} is the diagram for the code category /nationgoal/, in figure \ref{fig:rationality} for code category /rationality/, in figure \ref{fig:tech-power} for code category /techpower/, and in figure \ref{fig:tech-self} for code category /techself/.

#+label: fig:nation-goal
#+caption: codes for category nationgoal
#+attr_latex: placement=[h!]
[[/home/henk/dropbox/dissertation/graphic/nationgoal.jpg]]

#+label: fig:rationality
#+caption: codes for category rationality
#+attr_latex: placement=[h!]
[[/home/henk/dropbox/dissertation/graphic/rationality.jpg]]

#+label: fig:tech-power
#+caption: codes for category techpower
#+attr_latex: placement=[h!]
[[/home/henk/dropbox/dissertation/graphic/techpower.jpg]]

#+label: fig:tech-self
#+caption: codes for category techself
#+attr_latex: placement=[h!]
[[/home/henk/dropbox/dissertation/graphic/techself.jpg]]

*** DONE QDA codes							:sec:
  :PROPERTIES:
  :table_export_file: ~/Dropbox/dissertation/data/qda-codes.tsv
  :table_export_format: orgtbl-to-tsv
  :END:

# M-x org-export-table -> qda-codes.tsv
#
The application of codes to paragraphs is accomplished in two steps each actuated by an /R/ script.  The first one loads the names and descriptions of the codes from tables \ref{tbl:qda-codes} and \ref{tbl:code-descr} into the "freecode" database table of the /rqda/ project. These codes are in common among all collections. the codes were obtained /inductively/ from a pilot study of the documents in conjunction with a /deductive/ process based on the theory of governmentality.  The QDA codes are shown in tables \ref{tbl:qda-codes} and \ref{tbl:code-descr}.

The second step is the application of these codes to the paragraphs in the files of the document collections based the content of the text unit, the paragraph. This is two-step process accomplished by an /R/ script that populates the "coding" database table in the /RQDA/ project with "tentative" codes based on "text patterns", followed up by a careful reading of the texts that 'fine-tunes' the code application process.

#+tblname: qda-codes
#+label: tbl:qda-codes
#+attr_latex: longtable
#+caption: QDA codes
| Id | Name               | Catid |
|----+--------------------+-------|
|  1 | ControlFed         |     2 |
|  2 | ControlLocal       |     2 |
|  3 | ControlState       |     2 |
|----+--------------------+-------|
|  4 | EducAchiev         |     2 |
|  5 | EducEquity         |     2 |
|  6 | EducGap            |     2 |
|  7 | EducFunding        |     3 |
|  8 | EducMathSci        |     2 |
|  9 | EducResearch       |     2 |
| 10 | EducStandard       |     2 |
|----+--------------------+-------|
| 11 | NationBestFirst    |     2 |
| 12 | NationDuty         |     2 |
| 13 | NationEcon         |     2 |
| 14 | NationInterest     |     2 |
| 15 | NationInternComp   |     2 |
| 16 | NationProsperity   |     1 |
| 17 | NationTech         |     2 |
|----+--------------------+-------|
| 18 | ParentInvolve      |     4 |
|----+--------------------+-------|
| 19 | SchoolAccount      |     3 |
| 20 | SchoolBizInput     |     2 |
| 21 | SchoolCharter      |     3 |
| 22 | SchoolChoice       |     3 |
| 23 | SchoolDiversity    |     2 |
| 24 | SchoolFixclose     |     3 |
| 25 | SchoolQuality      |     2 |
| 26 | SchoolReform       |     3 |
|----+--------------------+-------|
| 27 | StudentAll         |     2 |
| 28 | StudentAssess      |     3 |
| 29 | StudentCareer      |     4 |
| 30 | studentCollege     |     4 |
| 31 | StudentExpectation |     4 |
| 32 | StudentGraduation  |     4 |
| 33 | StudentPoverty     |     1 |
|----+--------------------+-------|
| 34 | TeacherApprec      |     2 |
| 35 | TeacherAssess      |     3 |
| 36 | TeacherCert        |     3 |
| 37 | TeacherProfdev     |     3 |
| 38 | TeacherQuality     |     2 |
| 39 | TeacherReplace     |     3 |

#+tblname: code-descr
#+label: tbl:code-descr
#+attr_latex: longtable
#+caption: qda code descriptions
| Id | Description                                                                                 |
|----+---------------------------------------------------------------------------------------------|
|  1 | Control at federal level of education                                                       |
|  2 | Control at school and school district level of education                                    |
|  3 | Control at state level of education                                                         |
|----+---------------------------------------------------------------------------------------------|
|  4 | Academic achievement, success in school, quality of education                               |
|  5 | Equity in education, or the lack of it                                                      |
|  6 | Gap in educational achievement or related metrics, e.g. graduation or college enrolment     |
|  7 | Funding of education, federal, state or local sources, for schools, students, tutors, etc   |
|  8 | Instruction in math and/or science                                                          |
|  9 | Research-based school interventions                                                         |
| 10 | Standards and standardization in education, local, state or federal                         |
|----+---------------------------------------------------------------------------------------------|
| 11 | Being, remaining, or becoming again the best in the world                                   |
| 12 | National duty, responsibility, mandate, related to education                                |
| 13 | National economy, related to education                                                      |
| 14 | National interest, goal, objective or priority, national success, related to education      |
| 15 | International or global competition, related to education                                   |
| 16 | Prosperity of the nation, society and its components, especially students                   |
| 17 | Technological society and required skills to participate therewith                          |
|----+---------------------------------------------------------------------------------------------|
| 18 | Parent involvement in the education of their children                                       |
|----+---------------------------------------------------------------------------------------------|
| 19 | Accountability of schools                                                                   |
| 20 | Interventions and inputs from business in education                                         |
| 21 | Conversion into charter school or ex novo                                                   |
| 22 | The ability of changing schools by the parents                                              |
| 23 | Diversity of the school with respect to students and/or teachers                            |
| 24 | Drastic school interventions up to closure                                                  |
| 25 | Quality of schools, perceived or measured                                                   |
| 26 | School reform, at local, state or federal level, not at school level                        |
|----+---------------------------------------------------------------------------------------------|
| 27 | All students, all children, etc.                                                            |
| 28 | The testing of students or other means of assessment                                        |
| 29 | The career of students after completing K-12 school, work-force, employment, employability  |
| 30 | Admission to college and ability to graduate from college, related to K-12 education        |
| 31 | Social and individual expectations about the worth of education, motivation, responsibility |
| 32 | Graduation from K-12 or lack of it, dropping out                                            |
| 33 | Unsatisfactory financial situation of the students, their families, their schools           |
|----+---------------------------------------------------------------------------------------------|
| 34 | Teacher appreciation, by value in society, financial means, or autonomy                     |
| 35 | Teacher assessment or evaluation                                                            |
| 36 | Teacher certification                                                                       |
| 37 | Professional development of teachers                                                        |
| 38 | The perceived worth of a teacher and thus his or her classification in society              |
| 39 | The replacement or firing of "bad" teachers with "good" ones, replacing principals          |

First we add the codes to the database. It is possible to check the available codes for a certain /RQDA/ project by clicking on the "codes" tab in the GUI or by entering the following at the /R/ prompt

#+srcname: check-codes
#+begin_src R
library("RQDA")
openProject("~/Dropbox/dissertation/data/<project>.rqda", updateGUI = TRUE)
RQDAQuery("select id, name, memo from freecode where status==1")
#+end_src

*** DONE /RQDA/ codes					       :noexport:sec:
  :PROPERTIES:
  :table_export_file: ~/Dropbox/dissertation/data/codes.tsv
  :table_export_format: orgtbl-to-tsv
  :END:

# alt-x org-table-export

# not exported to latex because table is too large

#+tblname: codes
#+label: tbl:codes
#+attr_latex: longtable
#+caption: codes
| Id | Name               | Description                                                                                 | catid |
|----+--------------------+---------------------------------------------------------------------------------------------+-------|
|  1 | ControlFed         | control at federal level of education                                                       |     2 |
|  2 | ControlLocal       | control at school and school district level of education                                    |     2 |
|  3 | ControlState       | control at state level of education                                                         |     2 |
|----+--------------------+---------------------------------------------------------------------------------------------+-------|
|  4 | EducAchiev         | academic achievement, success in school, quality of education                               |     2 |
|  5 | EducEquity         | equity in education, or the lack of it                                                      |     2 |
|  6 | EducGap            | gap in educational achievement or related metrics, e.g. graduation                          |     2 |
|  7 | EducFunding        | funding of education, federal, state or local sources, for schools, students, tutors, etc   |     3 |
|  8 | EducMathSci        | instruction in math and/or science                                                          |     2 |
|  9 | EducResearch       | research-based school interventions                                                         |     2 |
| 10 | EducStandard       | standards and standardization in education, local, state or federal                         |     2 |
|----+--------------------+---------------------------------------------------------------------------------------------+-------|
| 11 | NationBestFirst    | being, remaining, or becoming again the best in the world                                   |     2 |
| 12 | NationDuty         | national duty, responsibility, mandate, related to education                                |     2 |
| 13 | NationEcon         | national economy, related to education                                                      |     2 |
| 14 | NationInterest     | national interest, goal, objective or priority, national success, related to education      |     2 |
| 15 | NationInternComp   | international or global competition, related to education                                   |     2 |
| 16 | NationProsperity   | prosperity of the nation, society and its components, especially students                   |     1 |
| 17 | NationTech         | technological society and required skills to participate therewith                          |     2 |
|----+--------------------+---------------------------------------------------------------------------------------------+-------|
| 18 | ParentInvolve      | parent involvement in the education of their children                                       |     4 |
|----+--------------------+---------------------------------------------------------------------------------------------+-------|
| 19 | SchoolAccount      | accountability of schools                                                                   |     3 |
| 20 | SchoolBizInput     | interventions and inputs from business in education                                         |     2 |
| 21 | SchoolCharter      | conversion into charter school or ex novo                                                   |     3 |
| 22 | SchoolChoice       | the ability of changing schools by the parents                                              |     3 |
| 23 | SchoolDiversity    | diversity of the school with respect to students and/or teachers                            |     2 |
| 24 | SchoolFixClose     | drastic school interventions up to closure                                                  |     3 |
| 25 | SchoolQuality      | quality of schools, perceived or measured                                                   |     2 |
| 26 | SchoolReform       | school reform, at local, state or federal level, not at school level                        |     3 |
|----+--------------------+---------------------------------------------------------------------------------------------+-------|
| 27 | StudentAll         | all students, all children, etc.                                                            |     2 |
| 28 | StudentAssess      | the testing of students or other means of assessment                                        |     3 |
| 29 | StudentCareer      | the career of students after completing k12 school, work-force, employment, employability   |     4 |
| 30 | StudentCollege     | admission to college and ability to graduate from college, related to k12 education         |     4 |
| 31 | StudentExpectation | social and individual expectations about the worth of education, motivation, responsibility |     4 |
| 32 | StudentGraduation  | graduation from k12 or lack of it, dropping out                                             |     4 |
| 33 | StudentPoverty     | unsatisfactory financial situation of the students, their families, their schools           |     1 |
|----+--------------------+---------------------------------------------------------------------------------------------+-------|
| 34 | TeacherApprec      | teacher appreciation, by value in society, financial means, or autonomy                     |     2 |
| 35 | TeacherAssess      | teacher assessment or evaluation                                                            |     3 |
| 36 | TeacherCert        | teacher certification                                                                       |     3 |
| 37 | TeacherProfDev     | professional development of teachers                                                        |     3 |
| 38 | TeacherQuality     | the perceived worth of a teacher and thus his or her classification in society              |     2 |
| 39 | TeacherReplace     | the replacement or firing of "bad" teachers with "good" ones, replacing principals          |     3 |

*** DONE Code patterns							:sec:
  :PROPERTIES:
  :table_export_file: ~/Dropbox/dissertation/data/code-patterns.tsv
  :table_export_format: orgtbl-to-tsv
  :END:

# alt-x org-table-export

This section presents the code descriptions and the word patterns that identify them.  The patterns follow the /regular expression/ syntax as per e.g. http://en.wikipedia.org/wiki/regular_expression

The ideal situation is non-overlapping (orthogonal) word patterns. that is, each word pattern should correspond to only one code.  However, sometimes this is impossible. For instance the pattern "fire bad teacher" is common to both codes /TeacherQuality/ and /TeacherReplace/, "a nation at risk" is a pattern for both codes /NationInterest/ and /EducGap/, "ayp" and "adequate yearly progress" are a pattern for both codes /SchoolQuality/ and /StudentAssess/, "parental choice" is a pattern for both codes /ParentInvolve/ and /SchoolChoice/, "highly qualified teacher requirement" is a pattern for both codes /EducEquity/ and /TeacherQuality/.

#+tblname: code-patterns
#+label: tbl:code-patterns
#+caption: code patterns
#+attr_latex: longtable
| cid | pattern                                                                 |
|-----+-------------------------------------------------------------------------|
|   1 | [Ff]ederal [Ii]ntervention                                              |
|   1 | [Ff]ederal control                                                      |
|   1 | [Ff]ederal role                                                         |
|-----+-------------------------------------------------------------------------|
|   2 | [Ll]ocal control                                                        |
|   2 | trust local people                                                      |
|-----+-------------------------------------------------------------------------|
|   3 | [Ss]tate [Cc]ontrol                                                     |
|   3 | [Ff]lexibility for [Ss]tate                                             |
|   3 | [Ss]tate leadership                                                     |
|-----+-------------------------------------------------------------------------|
|   4 | [Ss]tudent achiev                                                       |
|   4 | [Ss]tudents achiev                                                      |
|   4 | [Ss]tudent outcome                                                      |
|   4 | [Ss]tudent preparation                                                  |
|   4 | [Ss]tudent's achievement                                                |
|   4 | [Ii]mproving achievement                                                |
|   4 | [Ii]mprove the achievement                                              |
|   4 | low achievement                                                         |
|   4 | raise achievement                                                       |
|   4 | raising achievement                                                     |
|   4 | [Ii]mprove achievement                                                  |
|   4 | [Mm]ath achievement                                                     |
|   4 | [Aa]cademic achiev                                                      |
|   4 | meet challenging state content standard                                 |
|   4 | [Aa]cademic performance                                                 |
|   4 | [Aa]cademic enrichment                                                  |
|   4 | [Aa]cademic excellence                                                  |
|   4 | [Aa]cademic success                                                     |
|   4 | success of our student                                                  |
|   4 | success for our student                                                 |
|   4 | student success                                                         |
|   4 | [Aa]cademic progress                                                    |
|   4 | [Ee]ducational excellence                                               |
|   4 | [Ee]ducational achiev                                                   |
|   4 | [Ee]ducational attain                                                   |
|   4 | [Ee]xcellence in our children                                           |
|   4 | [Ee]xcellence in the classroom                                          |
|   4 | do well in school                                                       |
|   4 | [Ee]xcel in school                                                      |
|   4 | [Rr]aise the quality of education                                       |
|   4 | [Ss]tudent [pp]erformance                                               |
|   4 | [Qq]uality of education                                                 |
|   4 | [Qq]uality of our education                                             |
|   4 | [Ee]ducational result                                                   |
|   4 | [Aa]chievement level                                                    |
|   4 | [Aa]chieve proficiency                                                  |
|   4 | child is performing                                                     |
|   4 | children are performing                                                 |
|   4 | improve their performance                                               |
|   4 | performance of student                                                  |
|   4 | children's achievement                                                  |
|   4 | [Ss]chool [pp]erformance                                                |
|   4 | [Ee]ffective education                                                  |
|   4 | first-class education                                                   |
|   4 | conceptual understanding                                                |
|   4 | educational outcome                                                     |
|   4 | educational performance                                                 |
|   4 | able to learn                                                           |
|   4 | proficient student                                                      |
|   4 | low-achieving student                                                   |
|   4 | student proficiency                                                     |
|   4 | student learning                                                        |
|-----+-------------------------------------------------------------------------|
|   5 | [ee]quit                                                                |
|   5 | [cc]ivil right                                                          |
|   5 | [ff]undamental right                                                    |
|   5 | brown v. board of education                                             |
|   5 | brown vs. board of education                                            |
|   5 | fair chance                                                             |
|   5 | equal chance                                                            |
|   5 | opportunity gap                                                         |
|   5 | have the same chance                                                    |
|   5 | [ee]qual education                                                      |
|   5 | same high quality education                                             |
|   5 | educational opportunity                                                 |
|   5 | educational opportunities                                               |
|   5 | educational inequalit                                                   |
|   5 | educational equalit                                                     |
|   5 | [hh]ighly [qq]ualified [tt]eacher [rr]equirement                        |
|   5 | teacher distribution gap                                                |
|   5 | teacher quality gap                                                     |
|   5 | teacher quality provision                                               |
|   5 | teacher quality requirement                                             |
|-----+-------------------------------------------------------------------------|
|   6 | [gg]ap                                                                  |
|   6 | lag behind                                                              |
|   6 | furthest behind                                                         |
|   6 | fallen behind                                                           |
|   6 | [aa]t.risk student                                                      |
|   6 | [nn]ation [aa]t [rr]isk                                                 |
|   6 | [ss]ubgroup                                                             |
|   6 | america lag                                                             |
|   6 | [dd]isaggregate                                                         |
|   6 | [dd]isaggregation                                                       |
|   6 | u.s.a. lag                                                              |
|   6 | usa lag                                                                 |
|   6 | [ss]tudent at risk                                                      |
|   6 | [ss]tudents at risk                                                     |
|   6 | [oo]uteducate                                                           |
|   6 | start school already behind                                             |
|   6 | [uu]nderachievement                                                     |
|   6 | [uu]nderachiving student                                                |
|   6 | [uu]nderachiving children                                               |
|   6 | racial differences                                                      |
|   6 | racial disparities                                                      |
|-----+-------------------------------------------------------------------------|
|   7 | [Ff]und                                                                 |
|   7 | [Gg]rant                                                                |
|   7 | [Aa]ward                                                                |
|   7 | [Tt]axation                                                             |
|   7 | [Ss]chool finance                                                       |
|   7 | [Ss]chool tax                                                           |
|   7 | [Mm]oney                                                                |
|   7 | [Tt]itle [Ii] allocation                                                |
|   7 | [Rr]esource.                                                            |
|   7 | [Bb]udget                                                               |
|   7 | education spending                                                      |
|   7 | [Ff]ederal spending                                                     |
|   7 | [Ff]ederal dollar                                                       |
|   7 | [Ff]ederal investment                                                   |
|   7 | billion initiative                                                      |
|   7 | [Ff]ederal [Gg]overnment will pay                                       |
|   7 | [Ss]cholarship                                                          |
|   7 | [Tt]ax [Dd]ollar                                                        |
|   7 | [Vv]aucher                                                              |
|   7 | million                                                                 |
|   7 | billion                                                                 |
|-----+-------------------------------------------------------------------------|
|   8 | [mm]ath                                                                 |
|   8 | stem                                                                    |
|   8 | [cc]alculus                                                             |
|   8 | [aa]lgebra                                                              |
|   8 | [aa]rithmetic                                                           |
|   8 | add and subtract                                                        |
|   8 | adds and subtract                                                       |
|   8 | adding and subtracting                                                  |
|-----+-------------------------------------------------------------------------|
|   9 | [rr]esearch base                                                        |
|   9 | [rr]esearch-based                                                       |
|   9 | [ss]cientifically-based program                                         |
|   9 | [ss]cientifically based program                                         |
|   9 | [ss]cientifically-based research                                        |
|   9 | [ss]cientifically based research                                        |
|   9 | [ss]cientifically based instruction                                     |
|   9 | [ee]vidence base                                                        |
|   9 | [ee]vidence-base                                                        |
|   9 | [dd]ata-driven results                                                  |
|   9 | [ee]ducation research                                                   |
|   9 | [ee]ducational research                                                 |
|   9 | [ss]tudies show                                                         |
|   9 | [bb]est available evidence                                              |
|   9 | [bb]est evidence available                                              |
|   9 | [ee]ducation [ee]xpert                                                  |
|   9 | proven instructional method                                             |
|   9 | proven practices                                                        |
|-----+-------------------------------------------------------------------------|
|  10 | [ss]chool expectation                                                   |
|  10 | [nn]ational [ee]ducation [gg]oal                                        |
|  10 | [gg]rade [ll]evel                                                       |
|  10 | [aa]cademic [ss]tandard                                                 |
|  10 | [aa]cademic rigor                                                       |
|  10 | [ee]ducational [ss]tandard                                              |
|  10 | [ee]ducation [ss]tandard                                                |
|  10 | [tt]eaching [ss]tandard                                                 |
|  10 | [ss]tandards of achievement                                             |
|  10 | [pp]erformance [ss]tandard                                              |
|  10 | [nn]ational [ss]tandard                                                 |
|  10 | [gg]oals for performance                                                |
|  10 | [cc]urriculum                                                           |
|  10 | [cc]urricula                                                            |
|  10 | [ss]tate standard                                                       |
|  10 | high standard                                                           |
|  10 | rigorous academic program                                               |
|  10 | rigorous courses                                                        |
|  10 | standards-based instruction                                             |
|-----+-------------------------------------------------------------------------|
|  11 | [gg]lobal leader                                                        |
|  11 | best education                                                          |
|  11 | best prepared                                                           |
|  11 | envy of the world                                                       |
|  11 | [ll]eader in innovation                                                 |
|  11 | [ll]eading the world                                                    |
|  11 | lead the world                                                          |
|  11 | best schools in the world                                               |
|  11 | best in the world                                                       |
|  11 | always led the way                                                      |
|  11 | [rr]emaining superpower                                                 |
|  11 | [rr]ank number one                                                      |
|  11 | [ww]orld.class education                                                |
|  11 | america to lead                                                         |
|  11 | world leader                                                            |
|  11 | world class                                                             |
|  11 | be number one again                                                     |
|  11 | first in the world                                                      |
|  11 | no. 1 in the world                                                      |
|  11 | envy of the world                                                       |
|  11 | [ll]eader in the world                                                  |
|  11 | preeminence in the world                                                |
|  11 | competitive advantage                                                   |
|  11 | competitive edge                                                        |
|-----+-------------------------------------------------------------------------|
|  12 | [nn]ational duty                                                        |
|  12 | [nn]ational demand                                                      |
|  12 | [nn]ational responsibility                                              |
|  12 | [pp]atriotic responsibility                                             |
|  12 | [rr]esponsibility as leaders                                            |
|  12 | [rr]esponsibility of the united states                                  |
|  12 | [rr]esponsibility of the us                                             |
|  12 | [rr]esponsibility of the u.s.                                           |
|  12 | [ii]mperative                                                           |
|  12 | [ee]ducation problem                                                    |
|  12 | America is counting on                                                  |
|  12 | America cannot afford                                                   |
|  12 | America can not afford                                                  |
|  12 | American people are counting on                                         |
|  12 | [Oo]bligation as a country                                              |
|  12 | [Nn]ational mission                                                     |
|  12 | we've got an obligation                                                 |
|  12 | not acceptable to the country                                           |
|  12 | [Oo]ur [nn]ation has the responsibility                                 |
|  12 | [Oo]ur [nn]ation has a responsibility                                   |
|  12 | moral outrage                                                           |
|  12 | no longer acceptable                                                    |
|-----+-------------------------------------------------------------------------|
|  13 | [Bb]usiness climate                                                     |
|  13 | [Ee]ducation is an economic                                             |
|-----+-------------------------------------------------------------------------|
|  14 | [nn]ational interest                                                    |
|  14 | [nn]ation's interest                                                    |
|  14 | [nn]ational objective                                                   |
|  14 | [nn]ational priorit                                                     |
|  14 | [nn]ation's priorit                                                     |
|  14 | [Nn]ational import                                                      |
|  14 | [Ee]ssential for our country                                            |
|  14 | [Dd]omestic priority                                                    |
|  14 | [Nn]ational tragedy                                                     |
|  14 | America's future                                                        |
|  14 | America's priority                                                      |
|  14 | [Pp]riority of a nation                                                 |
|  14 | [Ii]nterest. of our children                                            |
|  14 | [nn]ation's goal                                                        |
|  14 | [nn]ation's concern                                                     |
|  14 | [nn]ational [gg]oal                                                     |
|  14 | [nn]ational [ii]ssue                                                    |
|  14 | [nn]ational [uu]rgency                                                  |
|  14 | [nn]ational crisis                                                      |
|  14 | [nn]ational agenda                                                      |
|  14 | [nn]ational [pp]roblem                                                  |
|  14 | [nn]ational [ss]ecurity                                                 |
|  14 | important to the country                                                |
|  14 | essential to the country                                                |
|  14 | essential to this country                                               |
|  14 | [gg]oal of this [aa]dministration                                       |
|  14 | [gg]oal of our [nn]ation                                                |
|  14 | [gg]oals of our [nn]ation                                               |
|  14 | [Pp]riority of this [Aa]dministration                                   |
|  14 | not right for America                                                   |
|  14 | our number one priority                                                 |
|  14 | to the future of this country                                           |
|  14 | [hh]ighest [pp]riority                                                  |
|  14 | [nn]ation [aa]t [rr]isk                                                 |
|  14 | [ff]uture of the [nn]ation                                              |
|  14 | [ii]t is vital                                                          |
|  14 | critical need                                                           |
|-----+-------------------------------------------------------------------------|
|  15 | [gg]lobal competition                                                   |
|  15 | [ww]orldwide [ee]conomy                                                 |
|  15 | [ii]nternational competit                                               |
|  15 | [cc]ompete internat                                                     |
|  15 | [cc]ompete against other countries                                      |
|  15 | [gg]lobal compete                                                       |
|  15 | [gg]lobal competitiveness                                               |
|  15 | [cc]ompete global                                                       |
|  15 | [cc]ompete in a global                                                  |
|  15 | outcompete us                                                           |
|  15 | global economy                                                          |
|  15 | globalization                                                           |
|  15 | [cc]ompetitive world                                                    |
|  15 | [cc]ompete in a global economy                                          |
|  15 | [cc]ompete in this global economy                                       |
|  15 | [ww]orld competition                                                    |
|  15 | [aa]cademic [cc]ompetitiveness [cc]oucil                                |
|  15 | american competitiveness initiative                                     |
|  15 | american competitiveness                                                |
|  15 | economic competitiveness                                                |
|  15 | competitive challenge                                                   |
|  15 | compete in a global                                                     |
|-----+-------------------------------------------------------------------------|
|  16 | [qq]uality of life                                                      |
|  16 | [pp]ath to opportunity                                                  |
|  16 | [pp]ath to success                                                      |
|  16 | American [dd]ream                                                       |
|  16 | [pp]rosperity                                                           |
|  16 | fulfill their potential                                                 |
|  16 | their full potential                                                    |
|  16 | realize their dream                                                     |
|  16 | make most of their lives                                                |
|  16 | provide families                                                        |
|  16 | bright future                                                           |
|  16 | succeed in their lives                                                  |
|  16 | better future                                                           |
|  16 | [ff]uture of our [nn]ation                                              |
|  16 | change lives                                                            |
|  16 | future citizenry                                                        |
|  16 | the good of everyone                                                    |
|  16 | the good of everybody                                                   |
|  16 | improve lives                                                           |
|  16 | nation prosper                                                          |
|  16 | nation's well-being                                                     |
|  16 | productive citizen                                                      |
|  16 | prosperous future                                                       |
|  16 | ticket to success                                                       |
|-----+-------------------------------------------------------------------------|
|  17 | 21st century                                                            |
|  17 | [tt]wenty.first century                                                 |
|  17 | [tt]echnological [ii]nnovation                                          |
|  17 | [nn]ew economy                                                          |
|  17 | [kk]nowledge economy                                                    |
|  17 | [jj]obs of the future                                                   |
|  17 | [hh]igh.[tt]ech job                                                     |
|  17 | [ss]killed work                                                         |
|  17 | [ii]nformation age                                                      |
|-----+-------------------------------------------------------------------------|
|  18 | [pp]arents are involved                                                 |
|  18 | [pp]arents are move involved                                            |
|  18 | [pp]arental [ii]nvolvement                                              |
|  18 | [pp]arental [cc]ontrol                                                  |
|  18 | [pp]arents more involved                                                |
|  18 | [ee]ncourage their children                                             |
|  18 | parents asking their children                                           |
|  18 | [pp]arent-[tt]eacher                                                    |
|  18 | push their children                                                     |
|  18 | [pp]arents should                                                       |
|  18 | [pp]arents decide                                                       |
|  18 | [pp]arent's [rr]esponsibility                                           |
|  18 | [pp]arental [rr]esponsibility                                           |
|  18 | [ee]mpowering parent                                                    |
|  18 | [ee]mpower parent                                                       |
|  18 | [ee]mpowerment of parent                                                |
|  18 | [pp]arent involvement                                                   |
|  18 | [pp]arental involvement                                                 |
|  18 | [pp]arental choice                                                      |
|  18 | [pp]arental support                                                     |
|  18 | [pp]arental engagement                                                  |
|  18 | [oo]rganize parent                                                      |
|  18 | [ii]nvolve parent                                                       |
|  18 | [ii]nvolving parent                                                     |
|  18 | [ii]nvolved parent                                                      |
|  18 | [gg]ive parents the information                                         |
|  18 | [gg]ive the parents the information                                     |
|  18 | [gg]ive the parents information                                         |
|  18 | [gg]ive parent the information                                          |
|  18 | [pp]arent are inform                                                    |
|  18 | [pp]arent were inform                                                   |
|-----+-------------------------------------------------------------------------|
|  19 | [Aa]ccountable                                                          |
|  19 | hold people to account                                                  |
|  19 | [Hh]old schools responsible                                             |
|  19 | [Aa]ccountability                                                       |
|-----+-------------------------------------------------------------------------|
|  20 | [Bb]usiness leader                                                      |
|  20 | [Bb]usiness involve                                                     |
|-----+-------------------------------------------------------------------------|
|  21 | Harlem Children's Zone                                                  |
|  21 | [cc]harter [ss]chool                                                    |
|  21 | KIPP                                                                    |
|  21 | K.I.P.P                                                                 |
|-----+-------------------------------------------------------------------------|
|  22 | [pp]arents have options                                                 |
|  22 | [pp]arents can choose                                                   |
|  22 | [pp]arent can choose                                                    |
|  22 | there are options                                                       |
|  22 | [pp]arents can change                                                   |
|  22 | [ss]chool [cc]hoice                                                     |
|  22 | [ee]ducational [cc]hoice                                                |
|  22 | [ee]ducational [oo]pportunit                                            |
|  22 | [oo]ptions for parents                                                  |
|  22 | [vv]oucher                                                              |
|  22 | school of their choice                                                  |
|  22 | [pp]arental choice                                                      |
|-----+-------------------------------------------------------------------------|
|  23 | [ss]chool [dd]iversity                                                  |
|  23 | [dd]iversity in schools                                                 |
|  23 | [ss]tudent [dd]iversity                                                 |
|  23 | [dd]iversity among students                                             |
|  23 | [ss]chool [ss]egregation                                                |
|  23 | [ss]chool [dd]esegregation                                              |
|  23 | [ss]egregated school                                                    |
|  23 | recruit minority teacher                                                |
|  23 | under-represented                                                       |
|  23 | diverse student                                                         |
|  23 | minority participation                                                  |
|  23 | minority-serving                                                        |
|  23 | minority school                                                         |
|  23 | desegregation                                                           |
|-----+-------------------------------------------------------------------------|
|  24 | [Cc]lose school                                                         |
|  24 | sanction                                                                |
|  24 | [Cc]lose the school                                                     |
|  24 | [Cc]losing the school                                                   |
|  24 | [Ff]ixing our school                                                    |
|  24 | [Ff]ixing school                                                        |
|  24 | [Ii]mprove our school                                                   |
|  24 | [Ii]mprove school                                                       |
|  24 | [Ii]mproving our school                                                 |
|  24 | [Ii]mproving school                                                     |
|  24 | [Ss]chool takeover                                                      |
|  24 | [Ss]chool [Ii]mprove                                                    |
|  24 | [Cc]orrective [Aa]ction                                                 |
|  24 | schools in restructuring                                                |
|-----+-------------------------------------------------------------------------|
|  25 | [qq]uality of school                                                    |
|  25 | [qq]uality of the school                                                |
|  25 | [qq]uality of our public school                                         |
|  25 | [qq]uality of our education                                             |
|  25 | high.quality public school                                              |
|  25 | grading of the school                                                   |
|  25 | [ss]chool [qq]uality                                                    |
|  25 | [gg]ood school                                                          |
|  25 | bad school                                                              |
|  25 | drop.out factory                                                        |
|  25 | [qq]uality [ss]chool                                                    |
|  25 | excellent school                                                        |
|  25 | underperforming school                                                  |
|  25 | troubled school                                                         |
|  25 | better school                                                           |
|  25 | effective school                                                        |
|  25 | low-performing school                                                   |
|  25 | low- performing school                                                  |
|  25 | low performing school                                                   |
|  25 | underperforming school                                                  |
|  25 | underachieving school                                                   |
|  25 | high-need school                                                        |
|  25 | highest-need school                                                     |
|  25 | highest-needs school                                                    |
|  25 | highest needs school                                                    |
|  25 | high-needs school                                                       |
|  25 | underserved school                                                      |
|  25 | underserved student                                                     |
|  25 | lowest performing school                                                |
|  25 | failing school                                                          |
|  25 | [Ss]uccessful school                                                    |
|  25 | effective high school                                                   |
|  25 | school's problem                                                        |
|  25 | quality education                                                       |
|  25 | quality of education                                                    |
|  25 | quality education                                                       |
|  25 | good education                                                          |
|  25 | ayp                                                                     |
|  25 | AYP                                                                     |
|  25 | Adequate Yearly Progress                                                |
|  25 | adequate yearly progress                                                |
|  25 | excellent education                                                     |
|  25 | school performance                                                      |
|  25 | school effectiveness                                                    |
|  25 | school report card                                                      |
|-----+-------------------------------------------------------------------------|
|  26 | ESEA                                                                    |
|  26 | E.S.E.A                                                                 |
|  26 | [ss]chool [rr]eform                                                     |
|  26 | [aa]cademic [rr]eform                                                   |
|  26 | [ee]ducation [rr]eform                                                  |
|  26 | [ee]ducational [rr]eform                                                |
|  26 | [ii]mproving our schools                                                |
|  26 | [rr]eform our schools                                                   |
|  26 | [rr]eforming our schools                                                |
|  26 | [rr]eforming our public schools                                         |
|  26 | [rr]eforming our high schools                                           |
|  26 | [rr]eform your schools                                                  |
|  26 | [tt]ransform our education                                              |
|  26 | reformed their education                                                |
|  26 | [rr]eform [ee]ducation                                                  |
|  26 | fix our school                                                          |
|  26 | no child left behind                                                    |
|  26 | [rr]ace [tt]o [tt]he [tt]op                                             |
|  26 | NCLB                                                                    |
|  26 | N.C.L.B                                                                 |
|  26 | RTTT                                                                    |
|  26 | rttt                                                                    |
|  26 | [Ss]chool.[rr]eform                                                     |
|  26 | [Rr]edesigning school                                                   |
|  26 | [Rr]edesig school                                                       |
|-----+-------------------------------------------------------------------------|
|  27 | [aa]ll student                                                          |
|  27 | [ee]very student                                                        |
|  27 | [uu]niversal access                                                     |
|  27 | [ee]ach and every child                                                 |
|  27 | [ee]very child                                                          |
|  27 | [ee]very last child                                                     |
|  27 | [ee]very single child                                                   |
|  27 | not a single child                                                      |
|  27 | not one single child                                                    |
|  27 | [aa]ll pupil                                                            |
|  27 | [aa]ll youth                                                            |
|  27 | [aa]ll young people                                                     |
|  27 | [aa]ll children                                                         |
|  27 | [aa]ll our children                                                     |
|  27 | [aa]ll of our children                                                  |
|  27 | [aa]ll of god's children                                                |
|  27 | [aa]ll of our students                                                  |
|  27 | [aa]ll our students                                                     |
|  27 | [ee]very high school student                                            |
|  27 | [ee]very young person                                                   |
|  27 | no child is left behind                                                 |
|  27 | everybody gets an education                                             |
|  27 | [ee]very classroom                                                      |
|  27 | for each child                                                          |
|  27 | [aa]ll kinds of children                                                |
|  27 | [aa]ll american children                                                |
|-----+-------------------------------------------------------------------------|
|  28 | [Ss]tandardized test                                                    |
|  28 | PISA                                                                    |
|  28 | P.I.S.A                                                                 |
|  28 | [Pr]rogramme [Ff]or [Ii]nternational [Ss]tudent [Aa]ssessment           |
|  28 | NAEP                                                                    |
|  28 | N.A.E.P                                                                 |
|  28 | national assessment of educational progress                             |
|  28 | national assessment of education progress                               |
|  28 | national assessment for education progress                              |
|  28 | TIMSS                                                                   |
|  28 | T.I.M.S.S                                                               |
|  28 | [tt]rends [ii]n [ii]nternational [mm]athematics and [ss]cience [ss]tudy |
|  28 | assessment                                                              |
|  28 | [pp]erformance of student                                               |
|  28 | [pp]erformance assessment                                               |
|  28 | [mm]easure student                                                      |
|  28 | [mm]easuring student                                                    |
|  28 | [mm]easurement system                                                   |
|  28 | [aa]ssessing [pp]rogress                                                |
|  28 | [aa]ssess [pp]rogress                                                   |
|  28 | [ee]valuate [ss]chool                                                   |
|  28 | [aa]ssess [ss]chool                                                     |
|  28 | [aa]ssess [ss]tudent                                                    |
|  28 | ayp                                                                     |
|  28 | adequate yearly pogress                                                 |
|  28 | adequate yearly progress                                                |
|  28 | [tt]est [ss]core                                                        |
|  28 | [nn]ation's [rr]eport [cc]ard                                           |
|  28 | sat                                                                     |
|  28 | [ss]cholastic [aa]ptitude [tt]est                                       |
|  28 | act                                                                     |
|  28 | taas                                                                    |
|  28 | taks                                                                    |
|  28 | academic test                                                           |
|  28 | state test                                                              |
|  28 | measure academic achievement                                            |
|  28 | [gg]rowth model                                                         |
|  28 | [dd]ata [rr]eporting                                                    |
|  28 | [dd]ata [ss]ystem                                                       |
|  28 | [ss]tudent [ii]nformation [ss]ystem                                     |
|  28 | achievement data                                                        |
|-----+-------------------------------------------------------------------------|
|  29 | [mm]arketable                                                           |
|  29 | obtain a job                                                            |
|  29 | future job                                                              |
|  29 | find a job                                                              |
|  29 | obtain a job                                                            |
|  29 | prepare for a job                                                       |
|  29 | train for a job                                                         |
|  29 | [cc]areer                                                               |
|  29 | [ss]uccessful member                                                    |
|  29 | [ff]uture workforce                                                     |
|  29 | [ff]uture employee                                                      |
|  29 | [ee]ducated workforce                                                   |
|  29 | workforce of tomorrow                                                   |
|  29 | [jj]obs of the future                                                   |
|  29 | [jj]obs of the 21st century                                             |
|  29 | [pp]repare them for the workplace                                       |
|-----+-------------------------------------------------------------------------|
|  30 | [aa]dmission to college                                                 |
|  30 | [aa]dmission into college                                               |
|  30 | [pp]athway to college                                                   |
|  30 | [rr]eady for college                                                    |
|  30 | [cc]ollege preparation                                                  |
|  30 | [pp]repared for college                                                 |
|  30 | readiness for college                                                   |
|  30 | [pp]repare our kids for college                                         |
|  30 | [pp]repare students for college                                         |
|  30 | go to college                                                           |
|  30 | succeed in college                                                      |
|  30 | college-ready                                                           |
|  30 | [gg]raduate from college                                                |
|  30 | require bachelor's degree                                               |
|  30 | require a bachelor's degree                                             |
|  30 | require a college degree                                                |
|  30 | earn a degree                                                           |
|  30 | earn a college degree                                                   |
|  30 | [pp]repare community college student                                    |
|  30 | send their kids to college                                              |
|  30 | [ss]ucceed in higher education                                          |
|  30 | [aa]dvanced [pp]lacement                                                |
|  30 | ap                                                                      |
|  30 | [i]international [bb]accalaureate                                       |
|  30 | access to post-secondary education                                      |
|  30 | access to postsecondary education                                       |
|  30 | trio                                                                    |
|  30 | gear up                                                                 |
|  30 | postsecondary education                                                 |
|-----+-------------------------------------------------------------------------|
|  31 | high expectation                                                        |
|  31 | expect student                                                          |
|  31 | expectation for children                                                |
|  31 | [ss]tudent [ee]xpectation                                               |
|  31 | aspiring to                                                             |
|  31 | aspire to                                                               |
|  31 | full of energy                                                          |
|  31 | see a pathway to success                                                |
|  31 | educational aspiration                                                  |
|-----+-------------------------------------------------------------------------|
|  32 | [dd]ropout                                                              |
|  32 | [dd]rop-out                                                             |
|  32 | [dd]rop out                                                             |
|  32 | [dd]ropping out                                                         |
|  32 | [hh]igh risk youth                                                      |
|  32 | [gg]raduate from high school                                            |
|  32 | [gg]raduating from high school                                          |
|  32 | [cc]omplete high school                                                 |
|  32 | [ff]inish high school                                                   |
|  32 | [ff]inishing high school                                                |
|  32 | [cc]omplete high school                                                 |
|  32 | [cc]ompleting high school                                               |
|  32 | [hh]igh school completion                                               |
|  32 | [gg]raduate on time                                                     |
|  32 | [gg]raduation [rr]ate                                                   |
|  32 | [hh]igh [ss]chool [gg]raduation                                         |
|  32 | high.school completion rate                                             |
|-----+-------------------------------------------------------------------------|
|  33 | [pp]overty                                                              |
|  33 | [hh]omeless                                                             |
|  33 | [mm]igrant                                                              |
|  33 | [ll]ow ses                                                              |
|  33 | [ll]ow socio.economic.status                                            |
|  33 | [ll]owest income                                                        |
|  33 | [ll]ow [ii]ncome                                                        |
|  33 | [ll]ow-[ii]ncome                                                        |
|  33 | low- income                                                             |
|  33 | [pp]oor child                                                           |
|  33 | [pp]oor kid                                                             |
|  33 | [pp]oor neighbourhood                                                   |
|  33 | [pp]oor neighborhood                                                    |
|  33 | [pp]oor school                                                          |
|  33 | [pp]oor district                                                        |
|  33 | [pp]oorest school                                                       |
|  33 | [tt]ough neighborhood                                                   |
|  33 | [ii]mpoverished background                                              |
|  33 | [ee]conomically disagdvantaged                                          |
|  33 | [dd]isadvantaged student                                                |
|  33 | [dd]isadvantaged child                                                  |
|  33 | [dd]isadvantaged youth                                                  |
|  33 | [pp]oor student                                                         |
|  33 | [pp]oor and minority student                                            |
|  33 | [pp]oor minority student                                                |
|  33 | [pp]oor and minority child                                              |
|  33 | [pp]oor parent                                                          |
|  33 | [pp]oorer students                                                      |
|  33 | neediest student                                                        |
|  33 | neediest district                                                       |
|  33 | neediest school                                                         |
|  33 | need school                                                             |
|  33 | high-need school                                                        |
|  33 | highest-need school                                                     |
|  33 | highest-needs school                                                    |
|  33 | highest needs school                                                    |
|  33 | high-needs school                                                       |
|  33 | need communit                                                           |
|  33 | needy students                                                          |
|  33 | [tt]itle i                                                              |
|  33 | [tt]itle 1                                                              |
|  33 | at risk student                                                         |
|  33 | at- risk student                                                        |
|  33 | at-risk student                                                         |
|  33 | low.wealth                                                              |
|-----+-------------------------------------------------------------------------|
|  34 | [tt]eacher salar                                                        |
|  34 | [tt]eacher recruit                                                      |
|  34 | [rr]ecruit teacher                                                      |
|  34 | [rr]eward teacher                                                       |
|  34 | [rr]etain teacher                                                       |
|  34 | [rr]ewarding teacher                                                    |
|  34 | [rr]ewarding effective teacher                                          |
|  34 | [ee]mpower teacher                                                      |
|  34 | [ee]mpowers teacher                                                     |
|  34 | [tt]eacher incentive                                                    |
|  34 | [ii]ncentive for teacher                                                |
|  34 | [ii]ncentive for the teacher                                            |
|  34 | [ii]ncentives for teacher                                               |
|  34 | [ii]ncentives for the teacher                                           |
|  34 | [ss]upport our teacher                                                  |
|  34 | dedicated educator                                                      |
|  34 | teacher bonus                                                           |
|  34 | teacher retention                                                       |
|  34 | [tt]eacher [ii]ncentive                                                 |
|-----+-------------------------------------------------------------------------|
|  35 | [tt]eacher evaluation                                                   |
|  35 | [ee]valuation of teacher                                                |
|  35 | [ee]valuating teacher                                                   |
|  35 | [ee]valuating their teacher                                             |
|  35 | [ee]valuate [tt]eacher                                                  |
|  35 | [tt]eacher [aa]ssessment                                                |
|  35 | [tt]eacher [tt]esting                                                   |
|  35 | [jj]udging a teacher                                                    |
|-----+-------------------------------------------------------------------------|
|  36 | [tt]eacher certification                                                |
|  36 | [tt]eacher credential                                                   |
|  36 | [tt]eacher. need to be certified                                        |
|  36 | [tt]eaching certificate                                                 |
|  36 | [tt]eacher qualification standard                                       |
|  36 | [cc]ertified teacher                                                    |
|  36 | [ss]tate [cc]ertifi                                                     |
|  36 | teacher licensure                                                       |
|  36 | teacher qualification                                                   |
|-----+-------------------------------------------------------------------------|
|  37 | [pp]rofessional [dd]evelopment                                          |
|  37 | [tt]eacher [tt]rain                                                     |
|  37 | [tt]eacher [ee]nhancement                                               |
|  37 | [tt]raining these teacher                                               |
|  37 | [tt]raining to teacher                                                  |
|  37 | [tt]eacher. train                                                       |
|  37 | [tt]rain teacher                                                        |
|  37 | [tt]raining system for teacher                                          |
|  37 | mentor                                                                  |
|  37 | teacher-train                                                           |
|-----+-------------------------------------------------------------------------|
|  38 | [qq]uality of the teacher                                               |
|  38 | [qq]uality of our teacher                                               |
|  38 | [qq]uality of teacher                                                   |
|  38 | [tt]eacher [qq]uality                                                   |
|  38 | [tt]eacher [pp]erformance                                               |
|  38 | [bb]est teacher                                                         |
|  38 | [ww]orst teacher                                                        |
|  38 | [bb]ad teacher                                                          |
|  38 | skilled teacher                                                         |
|  38 | top teacher                                                             |
|  38 | [qq]uality [tt]eacher                                                   |
|  38 | [qq]ualified [tt]eacher                                                 |
|  38 | [qq]ualified math [tt]eacher                                            |
|  38 | [qq]ualified math and science [tt]eacher                                |
|  38 | [gg]ood teacher                                                         |
|  38 | talented teacher                                                        |
|  38 | [cc]ompetent teacher                                                    |
|  38 | [oo]utstanding teacher                                                  |
|  38 | [ee]xcellent teacher                                                    |
|  38 | [ee]ffective teacher                                                    |
|  38 | [gg]reat teacher                                                        |
|  38 | [hh]ighly trained teacher                                               |
|  38 | [tt]eacher quality                                                      |
|  38 | [tt]eacher competency                                                   |
|  38 | [tt]eacher competencies                                                 |
|  38 | [bb]etter teacher                                                       |
|  38 | well.trained teacher                                                    |
|  38 | out of field                                                            |
|  38 | teachers are highly qualified                                           |
|-----+-------------------------------------------------------------------------|
|  39 | [Rr]eplace teacher                                                      |
|  39 | [Rr]eplacing teacher                                                    |
|  39 | [Rr]eplace staff                                                        |
|  39 | [Ee]liminating bad teacher                                              |
|  39 | [Ee]liminate bad teacher                                                |
|  39 | [Ff]iring bad teacher                                                   |
|  39 | [Ff]ire bad teacher                                                     |
|  39 | [Ff]ire incompetent teacher                                             |
|  39 | [Rr]eplacing much of the staff                                          |
|  39 | [Rr]eplacing bad teacher                                                |
|  39 | [Rr]eplace bad teacher                                                  |
|  39 | [Rr]eplace bad ones                                                     |
|  39 | [Rr]eplacing the staff                                                  |
|  39 | [Rr]eplacing bad ones                                                   |
|  39 | [Rr]eplacing the principal                                              |
|  39 | [Rr]eplacing the teacher.                                               |

*** DONE Congressional hearings						:sec:

# c-c c-v t to generate the r script
The /R/ script loads the code categories (themes) in the =congr-hearings= project database from the file =themes.tsv=, then loads the codes from =codes.tsv= and assigns them to their category. Finally it tentatively applies them to paragraphs or lines of the source documents using word patterns from =code-patterns.tsv=.  

To run the script type the following at the /R/ prompt

#+begin_src R
library("RQDA")
openProject("~/Dropbox/dissertation/data/congr-hearings.rqda", updateGUI = TRUE)
setwd("~/Dropbox/dissertation/data")
source("congr-hearings-codes.R")
#+end_src

#+begin_latex
\noindent
#+end_latex
This will run the following /R/ script that will read the =tsv= files with the code and theme information and populate the "freecode", "codecat", "treecode" and "coding" tables in the project database

#+srcname: congr-hearings-codes
#+begin_src R :tangle ~/Dropbox/dissertation/data/congr-hearings-codes.R
# remove old objects
#
rm(list = ls())

# define variables
#
project.name <- "congr-hearings"
home.dir     <- "~/Dropbox/dissertation/data/"
user         <- Sys.getenv("USER")
today        <- Sys.Date()
drv          <- dbDriver("sqlite")
con          <- dbConnect(drv, dbname = paste(project.name, ".rqda", sep = ""))

# move to working directory
#
setwd(home.dir)

# read in theme id, name, and memo from tsv file
#
temp <- read.delim("themes.tsv", header = TRUE,
                   colclasses = c("integer","character","character"),
                   fill = FALSE)

# build the "codecat" data frame
#
DF <- data.frame(name   = temp$name,
                 cid    = NA,
                 catid  = as.integer(temp$catid),
                 owner  = as.character(user),
                 date   = as.character(today),
                 datem  = NA,
                 memo   = temp$description,
                 status = as.integer(1))

# load codes into table "codecat"
#
dbWriteTable(con, "codecat", DF, row.names = FALSE, overwrite = TRUE)  

# read in code id, name, and memo from tsv file
#
temp <- read.delim("codes.tsv", header = TRUE,
                   colclasses = c("integer","character","character","integer"),
                   fill = FALSE)

# build the "freecode" data frame
#
DF <- data.frame(name   = temp$name,
                 memo   = temp$description,
                 owner  = as.character(user),
                 date   = as.character(today),
                 datem  = NA,
                 id     = as.integer(temp$id),
                 status = as.integer(1),
                 color  = NA)

# load codes into table "freecode"
#
dbWriteTable(con, "freecode", DF, row.names = FALSE, overwrite = TRUE)  

# build the "treecode" data frame
#
DF <- data.frame(cid    = as.integer(temp$id),
                 catid  = as.integer(temp$catid),
                 date   = as.character(today),
                 datem  = NA,
                 memo   = NA,
                 status = as.integer(1),
                 owner  = as.character(user))

# load codes into table "treecode"
#
dbWriteTable(con, "treecode", DF, row.names = FALSE, overwrite = TRUE)  

# clean the "coding" table, careful!
#
RQDAQuery("delete from coding")

# read code pattern table
DF <- read.delim("code-patterns.tsv", header = TRUE,
                 colclasses = c("integer","character"),
                 fill = FALSE)

# loop over cid
#
for (i in 1:length(DF[,1]))
{
  codingBySearch(DF[i,2], fid = getFileIds(), Df[i,1])
}
dbDisconnect(con)
#+end_src

*** DONE Presidential documents						:sec:

# c-c c-v t to generate the r script
# c-c c-c to run the bash script
The /R/ script loads the code categories (themes) in the =pres-docs= project database from the file =themes.tsv=, then loads the codes from =codes.tsv= and assigns them to their category. Finally it tentatively applies them to paragraphs or lines of the source documents using word patterns from =code-patterns.tsv=. When coding I ignored the questions in a question-and-answer setting.

To run the script type the following at the /R/ prompt

#+begin_src R
library("RQDA")
openProject("~/Dropbox/dissertation/data/pres-docs.rqda", updateGUI = TRUE)
setwd("~/Dropbox/dissertation/data")
source("pres-docs-codes.r")
#+end_src

#+begin_latex
\noindent
#+end_latex
This will run the following /R/ script that will read the =tsv= files with the code and theme information and populate the "freecode", "codecat", "treecode" and "coding" tables in the project database

#+srcname: pres-docs-codes
#+begin_src R :tangle ~/Dropbox/dissertation/data/pres-docs-codes.R
# remove old objects
#
rm(list = ls())

# define variables
#
project.name <- "pres-docs"
home.dir     <- "~/Dropbox/dissertation/data"
user         <- Sys.getenv("USER")
today        <- Sys.Date()
drv          <- dbDriver("sqlite")
con          <- dbConnect(drv, dbname = paste(project.name, ".rqda", sep = ""))

# move to working directory
#
setwd(home.dir)

# read in theme id, name, and memo from tsv file
#
temp <- read.delim("themes.tsv", header = TRUE,
                   colclasses = c("integer","character","character"),
                   fill = FALSE)

# build the "codecat" data frame
#
df <- data.frame(name   = temp$name,
                 cid    = NA,
                 catid  = as.integer(temp$catid),
                 owner  = as.character(user),
                 date   = as.character(today),
                 datem  = NA,
                 memo   = temp$description,
                 status = as.integer(1))

# load codes categories (themes) into table "codecat"
#
dbWriteTable(con, "codecat", df, row.names = FALSE, overwrite = TRUE)  

# read in code id, name, and memo from tsv file
#
temp <- read.delim("codes.tsv", header = TRUE,
                   colclasses = c("integer","character","character","integer"),
                   fill = FALSE)

# build the "freecode" data frame
#
df <- data.frame(name   = temp$name,
                 memo   = temp$description,
                 owner  = as.character(user),
                 date   = as.character(today),
                 datem  = NA,
                 id     = as.integer(temp$id),
                 status = as.integer(1),
                 color  = NA)

# load codes into table "freecode"
#
dbWriteTable(con, "freecode", df, row.names = FALSE, overwrite = TRUE)  

# build the "treecode" data frame
#
df <- data.frame(cid    = as.integer(temp$id),
                 catid  = as.integer(temp$catid),
                 date   = as.character(today),
                 datem  = NA,
                 memo   = NA,
                 status = as.integer(1),
                 owner  = as.character(user))

# load codes into table "treecode"
#
dbWriteTable(con, "treecode", df, row.names = FALSE, overwrite = TRUE)  

# clean the "coding" table, careful!
#
RQDAQuery("delete from coding")

# read code pattern table
df <- read.delim("code-patterns.tsv", header = TRUE,
                   colclasses = c("integer","character"),
                   fill = FALSE)

# loop over cid
#
for (i in 1:length(df[,1]))
{
  codingBySearch(df[i,2], fid = getFileIds(), df[i,1])
}
dbDisconnect(con)
#+end_src

*** Congressional bills					       :noexport:sec:

# c-c c-v t to generate the r script
The /R/ script loads the code categories (themes) in the =conrg-bills= project database from the file =themes.tsv=, then loads the codes from =codes.tsv= and assigns them to their category. Finally it tentatively applies them to paragraphs or lines of the source documents using word patterns from =code-patterns.tsv=.  Some source document have line breaks at the end of each line even though they belong to the same paragraph.  Unfortunately in this case the code will only mark the line and not the whole paragraph.

To run the script type the following at the /R/ prompt

#+begin_src r
library("RQDA")
openProject("~/Dropbox/dissertation/data/congr-bills.rqda", updateGUI = TRUE)
setwd("~/Dropbox/dissertation/data")
source("congr-bills-codes.r")
#+end_src

#+begin_latex
\noindent
#+end_latex
This will run the following /r/ script that will read the =tsv= files with the code and theme information and populate the "freecode", "codecat", "treecode" and "coding" tables in the project database

#+srcname: congr-bills-codes
#+begin_src r :tangle ~/Dropbox/dissertation/data/congr-bills-codes.r
# remove old objects
#
rm(list = ls())

# define variables
#
project.name <- "congr-bills"
home.dir     <- "~/Dropbox/dissertation/data/"
user         <- sys.getenv("Username")
today        <- sys.date()
drv          <- dbdriver("sqlite")
con          <- dbconnect(drv, dbname = paste(project.name, ".rqda", sep = ""))

# move to working directory
#
setwd(home.dir)

# read in theme id, name, and memo from tsv file
#
temp <- read.delim("themes.tsv", header = true,
                   colclasses = c("integer","character","character"),
                   fill = false)

# build the "codecat" data frame
#
df <- data.frame(name   = temp$name,
                 cid    = na,
                 catid  = as.integer(temp$catid),
                 owner  = as.character(user),
                 date   = as.character(today),
                 datem  = na,
                 memo   = temp$description,
                 status = as.integer(1))

# load codes into table "codecat"
#
dbwritetable(con, "codecat", df, row.names = false, overwrite = true)  

# read in code id, name, and memo from tsv file
#
temp <- read.delim("codes.tsv", header = true,
                   colclasses = c("integer","character","character","integer"),
                   fill = false)

# build the "freecode" data frame
#
df <- data.frame(name   = temp$name,
                 memo   = temp$description,
                 owner  = as.character(user),
                 date   = as.character(today),
                 datem  = na,
                 id     = as.integer(temp$id),
                 status = as.integer(1),
                 color  = na)

# load codes into table "freecode"
#
dbwritetable(con, "freecode", df, row.names = false, overwrite = true)  

# build the "treecode" data frame
#
df <- data.frame(cid    = as.integer(temp$id),
                 catid  = as.integer(temp$catid),
                 date   = as.character(today),
                 datem  = na,
                 memo   = na,
                 status = as.integer(1),
                 owner  = as.character(user))

# load codes into table "treecode"
#
dbwritetable(con, "treecode", df, row.names = false, overwrite = true)  

# clean the "coding" table, careful!
#
rqdaquery("delete from coding")

# read code pattern table
df <- read.delim("code-patterns.tsv", header = true,
                   colclasses = c("integer","character"),
                   fill = false)

# loop over cid
#
for (i in 1:length(df[,1]))
{
  codingbysearch(df[i,2], fid = getfileids(), df[i,1])
}
dbdisconnect(con)
#+end_src
** STRT Summary results of QDA codes in /RQDA/				:chp:
# c-c c-v t to generate scripts
# by /R/ functions /rqda/ can:
# 1. calculate the relation between two codings, given the coding indexes
# 2. give a summary of codings (frequencies) and inter-code relationship

*** DONE Introduction							:sec:

In this chapter are the scripts that created several descriptive statistics of the (a) codes and code categories, and (b) case attributes. the descriptive statistics for the codes are the (1) number of codings for each code, (2) average number of words, (3) files associated with each code, and (4) the relationship between the codes. The relation between two codes is defined as number of paragraphs that are marked with both codes.  The detection of code relations helped the development of the QDA narratives.

This chapter corresponds to the step "QDA statistics and plots".

To explore the relationships between the codes I assume that when the same paragraph is coded by more than one code, those codes are related. These relations can be diagrammatically represented by an *undirected weighted network graph* (see e.g. http://en.wikipedia.org/wiki/graph_(mathematics)).  It is *undirected* because logically there is no directionality between the various codes.  In addition the codes network graphs are *weighted* because some codes will be more highly correlated than others. Numerically this corresponds to the count of paragraphs where both codes are applied.

The software used to build the network graphs is the /igraph/ package of /R/.  For information on this package see http://igraph.sourceforge.net and http://cneurocvs.rmki.kfki.hu/igraphbook. All the other plots were created using the /ggplot2/ package (http://http://had.co.nz/ggplot2).

The relationships are obtained from /RQDA/ as an upper triangular matrix, which is used to create the project network graph.  In addition, because the diagonal of the matrix, the code will also create a table of frequency for each code. 

There are 39 codes belonging to 4 code categories.  The graphical representation of so many codes is confusing.  Thus it is best to refer to tabulations for descriptive statistics on the codes and to graphs only for the code categories and case attributes.

*** STRT Presidential documents						:sec:

To calculate the coding descriptive statistics for the =pres-docs= project type the following at the /R/ prompt

#+begin_src R
library("RQDA")
openProject("~/Dropbox/dissertation/data/pres-docs.rqda", updateGUI = TRUE)
setwd("~/Dropbox/dissertation/data")
source("pres-docs-code-stats.R")
#+end_src

#+begin_latex
\noindent
#+end_latex
This will run the following /R/ script

#+srcname: pres-docs-stats
#+begin_src R :tangle ~/Dropbox/dissertation/data/pres-docs-code-stats.R
########################
cat("Load libraries\n")
########################
library("gtools")
library("ggplot2")
library("reshape")

################################################
cat("Remove old objects, close plot windows\n")
################################################
rm(list = ls())
graphics.off()

###########################################
cat("Define variables and set home dir\n")
###########################################
project.name <- "pres-docs"
graph.dir    <- "~/Dropbox/dissertation/graphic/"
home.dir     <- "~/Dropbox/dissertation/data/"
setwd(home.dir)

sql <- "select coding.cid, freecode.name from coding, freecode 
        where coding.cid=freecode.id and coding.status=1
        group by coding.cid"

####################################
cat("Number of coded paragraphs\n")
####################################
coding.tbl <- getCodingTable()
total.codings <- length(coding.tbl$rowid)
print(paste("Total number of codings: ", total.codings, sep = ""))

temp.1 <- coding.tbl[,c(3,5,6,7.8)]
temp.2 <- temp.1[!duplicated(temp.1), ]
print(paste("Number of coded paragraphs: ", length(temp.2$fid), sep = ""))

########################
cat("Code summaries\n")
########################
# I am only interested in the first and last summary
#
temp <- summaryCodings(byFile = FALSE)

# (1) number of codings per code
#
DF.1 <- data.frame(table.1 = temp$NumOfCoding)
DF.2 <- data.frame(DF.1, Prop = round(DF.1$table.1.Freq/total.codings,
  digits = 4))
colnames(DF.2) <- c("Code","NumCodings","Proportion")

# need for Mann-Whitney test
#
pres.docs.code.freq <- DF.2

DF.2 <- DF.2[order(DF.2$NumCodings, decreasing = TRUE), ]
rownames(DF.2) <- NULL
DF.3 <- data.frame(Code = DF.2$Code, 
                   Rank = cumsum(!duplicated(DF.2$NumCodings)),
                   NumCodings = DF.2$NumCodings, 
                   Proportion = DF.2$Proportion)
print("Table of number of codings per code")
print(DF.3)

# (2) number of files associated with each code
#
total.files <- length(unique(coding.tbl$fid))
DF.4 <- data.frame(Code = rownames(temp$NumOfFile), NumFiles = temp$NumOfFile)
DF.5 <- data.frame(DF.4, Proportion = round(DF.4$NumFiles/total.files,
  digits = 4))
DF.5 <- DF.5[order(DF.5$NumFiles, decreasing = TRUE), ]
DF.6 <- data.frame(Code = DF.5$Code,
                   Rank = cumsum(!duplicated(DF.5$NumFiles)),
                   NumFiles = DF.5$NumFiles,
                   Proportion = DF.5$Proportion)
rownames(DF.6) <- NULL
print("Table of number of files per code")
print(DF.6)

#############################################
cat("Calculate code numbers per document\n")
#############################################
temp       <- summaryCodings(byFile = TRUE)
file.codes <- smartbind(temp$CodingOfFile[[1]], temp$CodingOfFile[[2]])

for (i in 3:length(temp$CodingOfFile))
{
  file.codes <- smartbind(file.codes, temp$CodingOfFile[[i]])
}

# fix rownames
#
rownames(file.codes) <- names(temp$CodingOfFile)

# fix NA
#
file.codes[is.na(file.codes)] <- 0
dim(file.codes)

# convert the data and join with doc.date, then sort and remove doc.date
#
file.attr <- getAttr(type = "file", attr = "DocDate")

DF.1 <- data.frame(doc.name = gsub("\\.scrb","", names(temp$CodingOfFile)),
  file.codes)
DF.2 <- data.frame(DF.1, doc.date = as.Date(file.attr$DocDate))
DF.3 <- DF.2[order(as.Date(DF.2$doc.date)), 1:ncol(DF.2)]
rownames(DF.3) <- NULL
dim(DF.3)

###################################################
# Out of this data frame we can build time graphs.
# Note that last column is the date.
###################################################
temp <- melt.data.frame(DF.3, id = c("doc.name","doc.date"))

#############################################
cat("create time plots for each QDA code\n")
#############################################
sql.code <- "select name, id from freecode;"
DF.codes <- RQDAQuery(sql)
colnames(DF.codes) <- c("CodeID","CodeName")
print(DF.codes)

for (i in 1:nrow(DF.codes))
{
  cat("Plotting code:", DF.codes$CodeName[i], ", cid:", DF.codes$CodeID[i],"\n")
  code.data <- subset(temp, variable %in% DF.codes$CodeName[i])
  plot.cid <- ggplot(data = code.data, aes(x = doc.date, y = value)) +
    geom_point(color = "blue", size = 4, alpha = 0.6) +
    ylab("Number of Codings") + xlab("") +
    geom_smooth(color = "red", se = FALSE)
  ggsave(plot.cid, 
        file = paste(graph.dir, project.name, "-cid-", DF.codes$CodeID[i], ".pdf",
        sep = ""), width = 16)
}

############################################
cat("Now plot combinations of QDA codes\n")
################################################################
cat("Time graph for EducGap, SchoolAccount, EducFunding\n")
cat("cid=6+7+19\n")
################################################################
data.6.7.19 <- subset(temp, variable %in% c("EducGap",
                                            "EducFunding",
                                            "SchoolAccount"))

plot.cid.6.7.19 <- ggplot(data = data.6.7.19, aes(x = doc.date, y = value)) +
 geom_point(aes(color = factor(variable)), size = 3) +
 xlab("") +
 ylab("Number of Codings") +
 scale_colour_discrete(name = "Code") +
 geom_smooth(aes(color = factor(variable)), se = FALSE, fullrange = TRUE) +
 theme(legend.position = c(0.1,0.85))

ggsave(plot.cid.6.7.19, file = paste(graph.dir, project.name,
       "-cid-6-7-19.pdf", sep = ""), width = 16)

##############################################################
cat("Time graph for EducGap, SchoolAccount, StudentAssess\n")
cat("cid=6+7+28\n")
##############################################################
data.6.19.28 <- subset(temp, variable %in% c("EducGap",
                                             "SchoolAccount",
                                             "StudentAssess"))

plot.cid.6.19.28 <- ggplot(data = data.6.19.28, aes(x = doc.date, y = value)) +
 geom_point(aes(color = factor(variable)), size = 3) +
 xlab("") +
 ylab("Number of Codings") +
 scale_colour_discrete(name = "Code") +
 geom_smooth(aes(color = factor(variable)), se = FALSE, fullrange = TRUE) +
 theme(legend.position = c(0.1,0.85))

ggsave(plot.cid.6.19.28, file = paste(graph.dir, project.name,
       "-cid-6-19-28.pdf", sep = ""), width = 16)

#########################################################
cat("Time graph for StudentPoverty, SchoolAccount\n")
cat("cid=26+33\n")
#########################################################
data.26.33 <- subset(temp, variable %in% c("SchoolReform",
                                           "StudentPoverty"))

plot.cid.26.33 <- ggplot(data = data.26.33, aes(x = doc.date, y = value)) +
 geom_point(aes(color = factor(variable)), size = 3) +
 xlab("") +
 ylab("Number of Codings") +
 scale_colour_discrete(name = "Code") +
 geom_smooth(aes(color = factor(variable)), se = FALSE, fullrange = TRUE) +
 theme(legend.position = c(0.1,0.85))

ggsave(plot.cid.26.33, file = paste(graph.dir, project.name,
       "-cid-26-33.pdf", sep = ""), width = 16)

##################################################################
cat("Time graph for NationEcon, NationInternComp, NationTech\n")
cat("cid=13+15+17\n")
##################################################################
data.13.15.17 <- subset(temp, variable %in% c("NationEcon",
                                              "NationInternComp",
                                              "NationTech"))

plot.cid.13.15.17 <- ggplot(data = data.13.15.17, aes(x = doc.date, y = value)) +
 geom_point(aes(color = factor(variable)), size = 3) +
 xlab("") +
 ylab("Number of Codings") +
 scale_colour_discrete(name = "Code") +
 geom_smooth(aes(color = factor(variable)), se = FALSE, fullrange = TRUE) +
 theme(legend.position = c(0.1,0.85))

ggsave(plot.cid.13.15.17, file = paste(graph.dir, project.name, 
       "-cid-13-15-17.pdf", sep = ""), width = 16)

######################################################################
cat("Time graph for NationDuty, NationInterest, SchoolAccount\n")
cat("cid=12+14+19\n")
######################################################################
data.12.14.19 <- subset(temp, variable %in% c("NationEcon",
                                              "NationInternComp",
                                              "NationTech"))

plot.cid.12.14.19 <- ggplot(data = data.12.14.19, aes(x = doc.date, y = value)) +
 geom_point(aes(color = factor(variable)), size = 3) +
 xlab("") +
 ylab("Number of Codings") +
 scale_colour_discrete(name = "Code") +
 geom_smooth(aes(color = factor(variable)), se = FALSE, fullrange = TRUE) +
 theme(legend.position = c(0.1,0.85))

ggsave(plot.cid.12.14.19, file = paste(graph.dir, project.name, 
       "-cid-12-14-19.pdf", sep = ""), width = 16)

#######################################################
cat("Time graph for EducGap, EducResearch, cid=6+9\n")
#######################################################
data.6.9 <- subset(temp, variable %in% c("EducGap",
                                         "EducResearch"))

plot.cid.6.9 <- ggplot(data = data.6.9, aes(x = doc.date, y = value)) +
 geom_point(aes(color = factor(variable)), size = 3) +
 xlab("") +
 ylab("Number of Codings") +
 scale_colour_discrete(name = "Code") +
 geom_smooth(aes(color = factor(variable)), se = FALSE, fullrange = TRUE) +
 theme(legend.position = c(0.1,0.85))

ggsave(plot.cid.6.9, file = paste(graph.dir, project.name, 
       "-cid-6-9.pdf", sep = ""), width = 16)

############################################################
cat("Time graph for EducGap, NationProsperity, cid=6+16\n")
############################################################
data.6.16 <- subset(temp, variable %in% c("EducGap",
                                          "NationProsperity"))

plot.cid.6.16 <- ggplot(data = data.6.16, aes(x = doc.date, y = value)) +
 geom_point(aes(color = factor(variable)), size = 3) +
 xlab("") +
 ylab("Number of Codings") +
 scale_colour_discrete(name = "Code") +
 geom_smooth(aes(color = factor(variable)), se = FALSE, fullrange = TRUE) +
 theme(legend.position = c(0.1,0.85))

ggsave(plot.cid.6.16, file = paste(graph.dir, project.name, 
       "-cid-6-16.pdf", sep = ""), width = 16)

######################################################
cat("Time graph for EducGap, EducMathSci, cid=6+8\n")
######################################################
data.6.8 <- subset(temp, variable %in% c("EducGap",
                                         "EducMathSci"))

plot.cid.6.8 <- ggplot(data = data.6.8, aes(x = doc.date, y = value)) +
 geom_point(aes(color = factor(variable)), size = 3) +
 xlab("") +
 ylab("Number of Codings") +
 scale_colour_discrete(name = "Code") +
 geom_smooth(aes(color = factor(variable)), se = FALSE, fullrange = TRUE) +
 theme(legend.position = c(0.1,0.85))

ggsave(plot.cid.6.8, file = paste(graph.dir, project.name, 
       "-cid-6-8.pdf", sep = ""), width = 16)

#################################################################
cat("Time graph for TeacherAssess, TeacherProfDev, cid=35+37\n")
#################################################################
data.35.37 <- subset(temp, variable %in% c("TeacherAssess",
                                           "TeacherProfDev"))

plot.cid.35.37 <- ggplot(data = data.35.37, aes(x = doc.date, y = value)) +
 geom_point(aes(color = factor(variable)), size = 3) +
 xlab("") +
 ylab("Number of Codings") +
 scale_colour_discrete(name = "Code") +
 geom_smooth(aes(color = factor(variable)), se = FALSE, fullrange = TRUE) +
 theme(legend.position = c(0.1,0.85))

ggsave(plot.cid.35.37, file = paste(graph.dir, project.name, 
       "-cid-35-37.pdf", sep = ""), width = 16)

###########################################################################
cat("Time graph for StudentCareer, StudentCollege, StudentGraduation\n")
cat("cid=29+30+32\n")
###########################################################################
data.29.30.32 <- subset(temp, variable %in% c("StudentCareer",
                                              "StudentCollege",
                                              "StudentGraduation"))

plot.cid.29.30.32 <- ggplot(data = data.29.30.32, aes(x = doc.date, y = value)) +
 geom_point(aes(color = factor(variable)), size = 3) +
 xlab("") +
 ylab("Number of Codings") +
 scale_colour_discrete(name = "Code") +
 geom_smooth(aes(color = factor(variable)), se = FALSE, fullrange = TRUE) +
 theme(legend.position = c(0.1,0.85))

ggsave(plot.cid.29.30.32, file = paste(graph.dir, project.name, 
       "-cid-29-30-32.pdf", sep = ""), width = 16)

######################################################################
cat("Time graph for TeacherCert, TeacherQuality, TeacherReplace\n")
cat("cid=36+38+39\n")
######################################################################
data.36.38.39 <- subset(temp, variable %in% c("TeacherCert",
                                              "TeacherQuality",
                                              "TeacherReplace"))

plot.cid.36.38.39 <- ggplot(data = data.36.38.39, aes(x = doc.date, y = value)) +
 geom_point(aes(color = factor(variable)), size = 3) +
 xlab("") +
 ylab("Number of Codings") +
 scale_colour_discrete(name = "Code") +
 geom_smooth(aes(color = factor(variable)), se = FALSE, fullrange = TRUE) +
 theme(legend.position = c(0.1,0.85))

ggsave(plot.cid.36.38.39, file = paste(graph.dir, project.name, 
       "-cid-36-38-39.pdf", sep = ""), width = 16)

####################################################################
cat("Time graph for ControlFederal, ControlLocal, ControlState\n")
cat("cid=1+2+3\n")
####################################################################
data.1.2.3 <- subset(temp, variable %in% c("ControlFed",
                                           "ControlLocal",
                                           "ControlState"))

plot.cid.1.2.3 <- ggplot(data = data.1.2.3, aes(x = doc.date, y = value)) +
 geom_point(aes(color = factor(variable)), size = 3) +
 xlab("") +
 ylab("Number of Codings") +
 scale_colour_discrete(name = "Code") +
 geom_smooth(aes(color = factor(variable)), se = FALSE, fullrange = TRUE) +
 theme(legend.position = c(0.1,0.85))

ggsave(plot.cid.1.2.3, file = paste(graph.dir, project.name, 
       "-cid-1-2-3.pdf", sep = ""), width = 16)

########################################################################
cat("Time graph for SchoolCharter, SchoolFixClose, TeacherReplace\n")
cat("cid=21+24+39\n")
########################################################################
data.21.24.39 <- subset(temp, variable %in% c("SchoolCharter",
                                              "SchoolFixClose",
                                              "TeacherReplace"))

plot.cid.21.24.39 <- ggplot(data = data.21.24.39, aes(x = doc.date, y = value)) +
 geom_point(aes(color = factor(variable)), size = 3) +
 xlab("") +
 ylab("Number of Codings") +
 scale_colour_discrete(name = "Code") +
 geom_smooth(aes(color = factor(variable)), se = FALSE, fullrange = TRUE) +
 theme(legend.position = c(0.1,0.85))

ggsave(plot.cid.21.24.39, file = paste(graph.dir, project.name, 
       "-cid-21-24-39.pdf", sep = ""), width = 16)

###############
# close graphs
###############
graphics.off()
#+end_src


#+srcname: pres-docs-stats-long
#+begin_src R :tangle ~/Dropbox/dissertation/data/pres-docs-code-stats-long.R
######################################
cat("Find code relationships\n")
cat("This takes a long time .....\n")
######################################
codes.df   <- RQDAQuery(sql)
my.codes   <- codes.df$name
coding.tbl <- getCodingTable()
pres.docs.codes.mx <- crossCodes(relation = "exact", 
                                 codeList = my.codes,
                                 data = coding.tbl)
print("The code relationships\n")
print(pres.docs.codes.mx)
#+end_src

*** STRT Congressional hearings 					:sec:

To calculate the coding descriptive statistics for the =congr-hearings= project type the following at the /R/ prompt

#+begin_src R
library("RQDA")
openProject("~/Dropbox/dissertation/data/congr-hearings.rqda", updateGUI = TRUE)
setwd("~/Dropbox/dissertation/data")
source("congr-hearings-code-stats.R")
#+end_src

#+begin_latex
\noindent
#+end_latex
This will run the following /R/ script.  Note that this script is computationally intensive.

#+srcname: congr-hearings-code-stats
#+begin_src R :tangle ~/Dropbox/dissertation/data/congr-hearings-code-stats.R
########################
cat("load libraries\n")
########################
library("gtools")
library("ggplot2")
library("igraph")
library("RColorBrewer")

################################################
cat("remove old objects, close plot windows\n")
################################################
rm(list = ls())
graphics.off()

###########################################
cat("define variables and set home dir\n")
###########################################
project.name <- "congr-hearings"
graph.dir    <- "~/Dropbox/dissertation/graphic/"
home.dir     <- "~/Dropbox/dissertation/data/"

sql          <- "select coding.cid, freecode.name from coding, freecode 
                 where coding.cid=freecode.id and coding.status=1
                 group by coding.cid"

setwd(home.dir)

###########################################
cat("code relationships, network graph\n")
###########################################
codes.df   <- RQDAQuery(sql)
my.codes   <- codes.df$name
coding.tbl <- getCodingTable()
codes.mx   <- crossCodes(relation = "exact", codeList = my.codes, data = coding.tbl)
codes.gr   <- graph.adjacency(codes.mx, mode = "upper", weighted = TRUE, 
                              diag = FALSE, add.rownames = "codename")

summary(codes.gr)

###############################################
cat("save the processed crossCodes to file\n")
###############################################
save(codes.mx, file = paste(project.name, "-cross-codes.Rdata", sep = ""))

# calculate frequency of each code, equal to first part of value of summaryCodings()
#
codes.fr <- data.frame(name = rownames(codes.mx), frequency = diag(codes.mx))
codes.fr <- codes.fr[order(codes.fr$name),]

#########################
cat("print tabulates\n")
#########################
print("table of the code overlaps")
print(codes.mx)
print("table of frequency of each code")
print(codes.fr)

# vertex colour based on theme
# catid 1 = orange, catid 2 = red, catid 3 = green, catid 4 = brown
#
V(codes.gr)[name ==  "1"]$color <- "red"
V(codes.gr)[name ==  "2"]$color <- "red"
V(codes.gr)[name ==  "3"]$color <- "red"
V(codes.gr)[name ==  "4"]$color <- "red"
V(codes.gr)[name ==  "5"]$color <- "red"
V(codes.gr)[name ==  "6"]$color <- "red"
V(codes.gr)[name ==  "7"]$color <- "green"
V(codes.gr)[name ==  "8"]$color <- "red"
V(codes.gr)[name ==  "9"]$color <- "red"
V(codes.gr)[name == "10"]$color <- "red"
V(codes.gr)[name == "11"]$color <- "red"
V(codes.gr)[name == "12"]$color <- "red"
V(codes.gr)[name == "13"]$color <- "red"
V(codes.gr)[name == "14"]$color <- "red"
V(codes.gr)[name == "15"]$color <- "red"
V(codes.gr)[name == "16"]$color <- "orange"
V(codes.gr)[name == "17"]$color <- "red"
V(codes.gr)[name == "18"]$color <- "brown"
V(codes.gr)[name == "19"]$color <- "green"
V(codes.gr)[name == "20"]$color <- "red"
V(codes.gr)[name == "21"]$color <- "green"
V(codes.gr)[name == "22"]$color <- "green"
V(codes.gr)[name == "23"]$color <- "red"
V(codes.gr)[name == "24"]$color <- "green"
V(codes.gr)[name == "25"]$color <- "red"
V(codes.gr)[name == "26"]$color <- "green"
V(codes.gr)[name == "27"]$color <- "red"
V(codes.gr)[name == "28"]$color <- "green"
V(codes.gr)[name == "29"]$color <- "brown"
V(codes.gr)[name == "30"]$color <- "brown"
V(codes.gr)[name == "31"]$color <- "brown"
V(codes.gr)[name == "32"]$color <- "brown"
V(codes.gr)[name == "33"]$color <- "orange"
V(codes.gr)[name == "34"]$color <- "red"
V(codes.gr)[name == "35"]$color <- "green"
V(codes.gr)[name == "36"]$color <- "green"
V(codes.gr)[name == "37"]$color <- "green"
V(codes.gr)[name == "38"]$color <- "red"
V(codes.gr)[name == "39"]$color <- "green"

##################################
cat("save network plot as pdf\n")
##################################
pdf(paste(graph.dir, project.name, "-network-graph.pdf", sep = ""), width = 12, height = 12)
plot.igraph(codes.gr, layout = layout.fruchterman.reingold, 
            vertex.label = V(codes.gr)$name,
            edge.color = "grey", 
            edge.width = E(codes.gr)$weight/100, 
            main = "Congressional hearings - Code relationships")
axis(1, labels = FALSE, tick = TRUE)
axis(2, labels = FALSE, tick = TRUE)
dev.off()

#################################
cat("degrees of the vertexes\n")
#################################
vertex.degree <- data.frame(code    = my.codes,
                            degrees = as.integer(degree(codes.gr, loops = FALSE)))

print(vertex.degree)

####################################################################
cat("list of connected nodes\n")
cat("be aware that the id of the nodes is used, not the code id\n")
####################################################################
compare <- 0:38
for (i in compare)
{ 
  temp <- neighborhood(codes.gr, order=1, node=V(codes.gr)[i] )
  print(temp)
  no.connect <- setdiff(compare, unlist(temp))
  print(no.connect)
}

###########################################################
# use V(codes.gr)[<number>]$name to find the code id
# use V(codes.gr)[<number>]$codename to find the code name
###########################################################

###################################################
# bar graph is too confusing -> use previous table
###################################################
# congr.hearings.code.degrees <- qplot(code.no, degrees, 
#                                data = vertex.degree, 
#                                geom = "bar", 
#                                fill = code.no,
#                                main = "Congressional hearings - Degrees of vertexes",
#                                xlab = "code number")
# ggsave(pres.docs.code.degrees, 
#        file = paste(graph.dir, project.name, "-code-degrees.pdf", sep = ""))

# print to file????
#
print("table of the degrees of the vertexes")
print(vertex.degree)

########################
cat("code summaries\n")
########################
temp <- summaryCodings(byfile = TRUE)

# (1) number of codings for each code
# already tabulated
#
# df.1 <- data.frame(temp$numofcoding)
# df.2 <- data.frame(code.no   = as.character(colnames(codes.mx)),
#                    code.name = df.1$var1, 
#                    code.freq = df.1$freq)
#
# pres.docs.num.codings <- qplot(code.no, code.freq,
#                                data = df.2, geom = "bar", 
#                                main = "presidential documents - number of codings per code", 
#                                xlab = "code number", 
#                                ylab = "number of codings", fill=code.no)
# ggsave(pres.docs.num.codings, 
#        file = paste(graph.dir, project.name, "-num-codings.pdf", sep = ""))

# (2) average number of words
#
df <- data.frame(avg.words = round(temp$avglength))
print("table of average number of words per code")
print(df)

# pres.docs.avg.codings <- qplot(code.no, avg.length, 
#                                geom = "bar", data = df,
#                                main = "presidential documents - average words per coding",
#                                xlab = "code number", ylab = "average words per coding",
#                                fill = code.no)
# ggsave(pres.docs.avg.codings, 
#        file = paste(graph.dir, project.name, "-avg-codings.pdf", sep = ""))

# (3) number of files associated with each code
#
DF.1 <- data.frame(num.files = temp$numoffile)
print("table of number of files for each code")
print(DF.1)

# df.2 <- data.frame(code.no    = as.character(colnames(codes.mx)),
#                    code.files = df.1$temp.numoffile)
#
# pres.docs.code.files <- qplot(code.no, code.files, 
#                               data = df.2, geom = "bar", 
#                               main = "presidential documents - files per code",
#                               xlab = "code number", ylab="number of files", 
#                               fill = code.no)
#
# ggsave(pres.docs.code.files, 
#        file = paste(graph.dir, project.name, "-code-files.pdf", sep = ""))

#############################################
cat("calculate code numbers per document\n")
#############################################
# temp       <- summaryCodings(byfile = true)
# file.codes <- smartbind(temp$codingoffile[[1]], temp$codingoffile[[2]])

# for (i in 3:length(temp$codingoffile))
# {
#    file.codes <- smartbind(file.codes, temp$codingoffile[[i]])
# }

# fix rownames
#
# rownames(file.codes) <- names(temp$codingoffile)

# fix na
#
# file.codes[is.na(file.codes)] <- 0
# dim(file.codes)

# convert the data and join with doc.date, then sort and remove doc.date
#
# file.attr <- getattr(type = "file", "docdate")

# df.1 <- data.frame(doc.name = gsub("\\.scrb","", names(temp$codingoffile)), file.codes)

# df.2 <- data.frame(df.1, doc.date = file.attr$docdate)

# df.3 <- df.2[order(as.date(df.2$doc.date)), 1:ncol(df.2)]

# df.4 <- subset(df.3, select=-doc.date)

# df.5 <- melt.data.frame(df.4, measure.vars = colnames(file.codes),variable_name = "codes")

# plot and save to pdf
#
# p.1 <- ggplot(df.5, aes(x = doc.name, y = value, 
#        fill = codes)) + geom_histogram(position = "stack", stat = "identity") + 
#        labs(x = "documents", y = "number of codings") + 
#        opts(title = "presidential documents - codings over time")

# ggsave(p.1, file = paste(graph.dir, project.name, "-codings-stack.pdf", sep = ""), width = 12)

# p.2 <- ggplot(df.5, aes(x = doc.name, y = value, 
#        fill = codes)) + geom_histogram(position = "fill", stat = "identity") +
#        labs(x = "documents", y = "number of codings") + 
#        opts(title = "presidential documents - codings over time, fill")

# ggsave(p.2, file = paste(graph.dir, project.name, "-codings-fill.pdf", sep = ""), width = 12)
#+end_src

** STRT Create /tm/ corpora from /RQDA/ codings				:chp:
# "C-c C-v t" to generate the R scripts

*** DONE Introduction							:sec:

# these /r/ scripts perform the operations corresponding to the step "import into *tm*" of the work flow diagram (\ref{fig:workflow-detail}). the /rqda/ texts are exported and then converted into a *tm* /corpus/. in addition the /rqda/ file attributes are converted into the meta-data of the corpus. at the end the corpus is saved into a /r/ data file.  the meta-data is corpus, not document specific and is of type indexed.

During this step the text of the coded paragraphs (codings) were converted into a text "corpus".  The corpus was then subjected to text processing and analysis (text mining) as described in the following two chapters.  I decided to create a corpus of only the coded paragraphs to avoid the inclusion of non-relevant material into the analysis.  This way the statistical results and representations better reflected the political discourse specific to the subject at hand.

The software used to perform text mining is the /tm/ package of /R/.  See http://tm.r-forge.r-project.org/.  The /R/ code for this step was adapted from code of the /RQDAtm/ package.

To obtain the description of the corpus and its metadata for a specific document collection type the following at the /R/ prompt

#+begin_src R
library("tm")
setwd("~/Dropbox/dissertation/data")
summary(corpus)
meta(corpus)
meta(corpus[1])
# meta(corpus[1], tag = "author", type = "indexed")
# meta(corpus,    tag = "title",  type = "indexed")
#+end_src

#+begin_latex
\noindent
#+end_latex

#the above commands will display the metadata corresponding to (1) the "author" attribute (tag) for the first file in the corpus, and (2) the "title" attribute (tag) for all documents in the project.

The first meta command will display the metadata for all files in the corpus.  The second command will do so for only file 1.  At the moment the /R/ code will only create two categories of metadata: (1) the /RQDA/ file id, and (2) the file name.  Other possible metadata categories are the document author and document date.  However, at this point of the text mining analysis these pieces of information are not necessary.

*** DONE Presidential documents						:sec:

To create the =pres-docs= corpus start /R/ and type the following at the prompt

#+begin_src r
library("RQDA")
library("tm")
setwd("~/Dropbox/dissertation/data")
openProject("~/Dropbox/dissertation/data/pres-docs.rqda", updateGUI = TRUE)
source("pres-docs-rqda2tm.R")
#+end_src

#+begin_latex
\noindent
#+end_latex
This will run the following /R/ script that will convert the /RQDA/ codings into a /tm/ corpus.

#+srcname: pres-docs-rqda2tm
#+begin_src R :tangle ~/Dropbox/dissertation/data/pres-docs-rqda2tm.R
############################
cat("remove old objects\n")
############################
rm(list = ls())

##################################################################
cat("define variables, working directory, database connection\n")
##################################################################
project.name <- "pres-docs"
home.dir     <- "~/Dropbox/dissertation/data/"
language     <- "en-US"
retrieval    <- NULL

#######################
cat("create corpus\n")
#######################
sql <- "select coding.fid, source.name, coding.seltext, coding.cid
        from coding, source where coding.status=1 and coding.fid=source.id 
        order by coding.fid, coding.cid"

retrieval <- RQDAQuery(sql)

Encoding(retrieval$seltext) <- "UTF-8"
Encoding(retrieval$name)    <- "UTF-8"

pres.docs.crp <- tm:::Corpus(tm::VectorSource(retrieval$seltext), readerControl = list(language = language))
meta(pres.docs.crp, tag = c("file.id","file.name","code.id")) <- retrieval[,c("fid","name","cid")]

##################################
cat("summary and save to file\n")
##################################
summary(pres.docs.crp)
save(pres.docs.crp, file = paste(project.name, ".Rdata", sep = ""))
#+end_src

*** DONE Congressional hearings						:sec:

To create the =congr-hearings= corpus start /R/ and type the following at the prompt

#+begin_src R
library("RQDA")
library("tm")
setwd("~/Dropbox/dissertation/data")
openProject("~/Dropbox/dissertation/data/congr-hearings.rqda", updateGUI = TRUE)
source("congr-hearings-rqda2tm.R")
#+end_src

#+begin_latex
\noindent
#+end_latex
This will run the following /R/ script that converts the /RQDA/ codings by files into a /tm/ corpus.

#+srcname: congr-hearings-rqda2tm
#+begin_src R :tangle ~/Dropbox/dissertation/data/congr-hearings-rqda2tm.R
############################
cat("remove old objects\n")
############################
rm(list = ls())

##################################################################
cat("define variables, working directory, database connection\n")
##################################################################
project.name <- "congr-hearings"
home.dir     <- "~/Dropbox/dissertation/data/"
language     <- "en-US"
retrieval    <- NULL

#######################
cat("create corpus\n")
#######################
sql <- "select coding.fid, source.name, coding.seltext, coding.cid
        from coding, source where coding.status=1 and coding.fid=source.id 
        order by coding.fid, coding.cid"

retrieval <- RQDAQuery(sql)

Encoding(retrieval$seltext) <- "UTF-8"
Encoding(retrieval$name)    <- "UTF-8"

congr.hearings.crp <- tm:::Corpus(tm::VectorSource(retrieval$seltext), readerControl = list(language = language))
meta(congr.hearings.crp, tag = c("file.id","file.name","code.id")) <- retrieval[,c("fid","name","cid")]

summary(congr.hearings.crp)

#############################
cat("save corpus to file\n")
#############################
save(congr.hearings.crp, file = paste(project.name, ".Rdata", sep = ""))
#+end_src

** STRT Process /tm/ corpora						:chp:
*** STRT Introduction							:sec:
After the creation of the /corpus/ the text needs to be processed before the actual text mining can take place. The processing consists of the conversion to lower case, the removal of low-content words, the conversion of relevant synonyms onto a single term, the removal of numbers and the removal of white space and punctuation.  An additional step in the processing is the stemming of the words, which is the removal of suffixes from related words to convert all of them into a single term.  

This stage of the research sequence corresponds to the step "process text".  The processing will create two /corpi/, the first one without stemming (unstemmed) and the second one with stemming.  We need the unstemmed corpus for some descriptive statistics such as a "word cloud", and we will use the stemmed corpus with strict text mining statistics.

All these text modifications are performed by /R/ scripts that utilize the /tm/ package.

The list of "stopwords" was obtained recursively from the next step.  I checked which words were most frequent in the document-term matrix and, if not relevant, I added them to the stop word list and reran the text processing step.  The stop words consist of names of persons ("mary", "yvonne", "diane", etc.) and of months, but not names of universities such as "john hopkins", scholarships such as "noyce" or juridic terms such as "brown", or names of members of congress, or names of cities.  Notice that the are specified in lowercase because they will eliminated after the conversion to lowercase as can be seen in the script. In addition I removed numbers expressed in Roman numerals, because the function used to remove numbers only removes Arabic numbers.

The processing of texts is a computationally very intensive.  Thus, I have performed it on =brazos.tamu.edu= the Texas A&M computer cluster.  For details see http://brazos.tamu.edu.

*** STRT Congressional hearings						:sec:

# C-c C-v t to generate R script

To process the text of the =congr-hearings= corpus start /R/ and type the following at the prompt

#+begin_src R
setwd("~/Dropbox/dissertation/data")
source("congr-hearings-process.R")
#+end_src

#+begin_latex
\noindent
#+end_latex
This will run the following /R/ script that processes the =congr-hearings= corpus and creates the two processed /corpi/.

#+begin_src R :tangle ~/Dropbox/dissertation/data/congr-hearings-process.R
#######################################################
cat("clean, load library, set directory, stopwords\n")
#######################################################
rm(list = ls())
library("tm")
project.name <- "congr-hearings"
home.dir     <- "~/Dropbox/dissertation/data"
setwd(home.dir)

mystopwords <- c("vii","diane","august","margarita","calderon","pauline","gibbons","jennifer","myriam","mary","capellini","davies","sandra","yvonne","lori","katharine","krueger","whitmore","sub","february","sue","gendron","whichever","hamilton","duncan","sweller","powersource","chi","bjork","mayer","ontology","holyoak","kid-friendly","edgewood","san antonio","bryan","darv","winnick")

#####################################
cat("load corpus from Rdata file\n")
#####################################

load(paste(project.name, ".Rdata", sep = ""))
summary(congr.hearings.crp)

######################
cat("process text\n")
######################

# to lower case
#
congr.hearings.crp <- tm_map(congr.hearings.crp, tolower)

# important acronyms
#
for (j in 1:length(congr.hearings.crp))
{
  congr.hearings.crp[[j]] <- gsub("third international math and science study", "timss", congr.hearings.crp[[j]])
  congr.hearings.crp[[j]] <- gsub("programme for international student assessment", "pisa", congr.hearings.crp[[j]])
  congr.hearings.crp[[j]] <- gsub("program for international student assessment", "pisa", congr.hearings.crp[[j]])
  congr.hearings.crp[[j]] <- gsub("programme of international student assessment", "pisa", congr.hearings.crp[[j]])
  congr.hearings.crp[[j]] <- gsub("program of international student assessment", "pisa", congr.hearings.crp[[j]])
  congr.hearings.crp[[j]] <- gsub("national assessment of educational progress", "naep", congr.hearings.crp[[j]])
  congr.hearings.crp[[j]] <- gsub("national assessment of education progress", "naep", congr.hearings.crp[[j]])
}
##########################
cat("remove stopwords\n")
##########################
congr.hearings.crp <- tm_map(congr.hearings.crp, removeWords, stopwords("english"))
congr.hearings.crp <- tm_map(congr.hearings.crp, removeWords, mystopwords)

for (j in 1:length(congr.hearings.crp))
{
  # remove "--"
  #
  congr.hearings.crp[[j]] <- gsub("--", " ", congr.hearings.crp[[j]])

  # reduce mathematics to math
  #
  congr.hearings.crp[[j]] <- gsub("mathematics", "math", congr.hearings.crp[[j]])

  # "fix" 21st
  #
  congr.hearings.crp[[j]] <- gsub("21st", "twentyfirst", congr.hearings.crp[[j]])

  # from plural to singular
  #
  congr.hearings.crp[[j]] <- gsub("gaps", "gap", congr.hearings.crp[[j]])
  congr.hearings.crp[[j]] <- gsub("children", "child", congr.hearings.crp[[j]])
  congr.hearings.crp[[j]] <- gsub("students", "student", congr.hearings.crp[[j]])
  congr.hearings.crp[[j]] <- gsub("schools", "school", congr.hearings.crp[[j]])
  congr.hearings.crp[[j]] <- gsub("teachers", "teacher", congr.hearings.crp[[j]])
  congr.hearings.crp[[j]] <- gsub("programs", "program", congr.hearings.crp[[j]])

  # "subgrant" and "grant" are very frequent in bills
  #
  congr.hearings.crp[[j]] <- gsub("subgrant", "fund", congr.hearings.crp[[j]])
  congr.hearings.crp[[j]] <- gsub("grant", "fund", congr.hearings.crp[[j]])

  # "are met" to "meet"
  #
  congr.hearings.crp[[j]] <- gsub("are met", "meet", congr.hearings.crp[[j]])

  # "labour" -> "labor"
  #
  congr.hearings.crp[[j]] <- gsub("labour", "labor", congr.hearings.crp[[j]])

  # combine into 1 word when connected by hyphen
  #
  congr.hearings.crp[[j]] <- gsub("-", "", congr.hearings.crp[[j]])
}

######################################################
cat("remove numbers, punctuation, and white space\n")
######################################################
congr.hearings.crp <- tm_map(congr.hearings.crp, removeNumbers)
congr.hearings.crp <- tm_map(congr.hearings.crp, removePunctuation)
congr.hearings.crp <- tm_map(congr.hearings.crp, stripWhitespace)

################################################################
cat("copy to unstemmed corpus and save to file\n")
cat("use load('congr-hearings-us.Rdata') to retrieve into R\n")
################################################################
congr.hearings.us.crp <- congr.hearings.crp
summary(congr.hearings.us.crp)
save(congr.hearings.us.crp, file = paste(project.name, "-us.Rdata", sep = ""))

################################################################
cat("stem the corpus and save to file\n")
cat("use load('congr-hearings-st.Rdata') to retrieve into R\n")
################################################################
congr.hearings.st.crp <- tm_map(congr.hearings.crp, stemDocument)
summary(congr.hearings.st.crp)
save(congr.hearings.st.crp, file = paste(project.name, "-st.Rdata", sep = ""))
#+end_src

*** STRT Presidential documents						:sec:

The following /R/ script pre-process the =pres-docs= corpus. To run it type the following at the /R/ prompt

#+begin_src R
setwd("~/Dropbox/dissertation/data")
source("pres-docs-process.R")
#+end_src

#+begin_latex
\noindent
#+end_latex
This will run the following script

#+srcname: pres-docs-process
#+begin_src R :tangle ~/Dropbox/dissertation/data/pres-docs-process.R
##############################################################
cat("clean, load library, set directory, define stopwords\n")
##############################################################
rm(list = ls())
library("tm")
project.name <- "pres-docs"
home.dir     <- "~/Dropbox/dissertation/data/"
setwd(home.dir)
mystopwords <- c("fourth","five","guy","half","hand","maybe","please","okay","mom","pennsylvania","put","pretty","ran","run","round","secretary","say","rod","see","seen","speak","tell","york","third","second","unless","california","arkansas","come","dad","elsewhere","wisconsin","washington","say","texas","virginia","margaret","maryland","month","minute","laura","eighth","daughter","carolina","day","indiana","hattiesburg","columbia","twice","montgomery","midland","linda","tucker","january","geoffrey","harlem","gregg","oklahoma","sister","thank","newt","gingrich","sharpton","losangeles","puertorico","memphis","vii","bloomberg","thurgood","mike","marshall","tucker")

#########################################
cat("load raw corpus from rdata file\n")
#########################################
load(paste(project.name, ".Rdata", sep = ""))
summary(pres.docs.crp)

#######################
cat("to lower case\n")
#######################
pres.docs.crp <- tm_map(pres.docs.crp, tolower)

######################################
cat("substitute significant terms\n")
######################################
# important acronyms
#
for (j in 1:length(pres.docs.crp))
{
  pres.docs.crp[[j]] <- gsub("third international math and science study", "timss", pres.docs.crp[[j]])
  pres.docs.crp[[j]] <- gsub("programme for international student assessment", "pisa", pres.docs.crp[[j]])
  pres.docs.crp[[j]] <- gsub("program for international student assessment", "pisa", pres.docs.crp[[j]])
  pres.docs.crp[[j]] <- gsub("programme of international student assessment", "pisa", pres.docs.crp[[j]])
  pres.docs.crp[[j]] <- gsub("program of international student assessment", "pisa", pres.docs.crp[[j]])
  pres.docs.crp[[j]] <- gsub("national assessment of educational progress", "naep", pres.docs.crp[[j]])
  pres.docs.crp[[j]] <- gsub("national assessment of education progress", "naep", pres.docs.crp[[j]])
#
# Geographical names
#
  pres.docs.crp[[j]] <- gsub("puerto rico", "puertorico", pres.docs.crp[[j]])
  pres.docs.crp[[j]] <- gsub("los angeles", "losangeles", pres.docs.crp[[j]])
}

####################
# remove stopwords
####################
pres.docs.crp <- tm_map(pres.docs.crp, removeWords, stopwords("english"))
pres.docs.crp <- tm_map(pres.docs.crp, removeWords, mystopwords)

for (j in 1:length(pres.docs.crp))
{
  # remove "--"
  #
  pres.docs.crp[[j]] <- gsub("--", " ", pres.docs.crp[[j]])

  # reduce mathematics to math
  #
  pres.docs.crp[[j]] <- gsub("mathematics", "math", pres.docs.crp[[j]])

  # "fix" 21st
  #
  pres.docs.crp[[j]] <- gsub("21st", "twentyfirst", pres.docs.crp[[j]])

  # from plural to singular
  #
  pres.docs.crp[[j]] <- gsub("gaps", "gap", pres.docs.crp[[j]])
  pres.docs.crp[[j]] <- gsub("children", "child", pres.docs.crp[[j]])
  pres.docs.crp[[j]] <- gsub("students", "student", pres.docs.crp[[j]])
  pres.docs.crp[[j]] <- gsub("schools", "school", pres.docs.crp[[j]])
  pres.docs.crp[[j]] <- gsub("teachers", "teacher", pres.docs.crp[[j]])
  pres.docs.crp[[j]] <- gsub("programs", "program", pres.docs.crp[[j]])

  # "subgrant" and "grant" are very frequent in bills
  #
  pres.docs.crp[[j]] <- gsub("subgrant", "fund", pres.docs.crp[[j]])
  pres.docs.crp[[j]] <- gsub("grant", "fund", pres.docs.crp[[j]])

  # "are met" to "meet"
  #
  pres.docs.crp[[j]] <- gsub("are met", "meet", pres.docs.crp[[j]])

  # "labour" -> "labor"
  #
  pres.docs.crp[[j]] <- gsub("labour", "labor", pres.docs.crp[[j]])

  # combine into 1 word when connected by hyphen
  #
  pres.docs.crp[[j]] <- gsub("-", "", pres.docs.crp[[j]])
}

#############################
cat("continue processing\n")
#############################
pres.docs.crp <- tm_map(pres.docs.crp, removePunctuation)
pres.docs.crp <- tm_map(pres.docs.crp, removeNumbers)
pres.docs.crp <- tm_map(pres.docs.crp, stripWhitespace)

##########################################################
cat("save processed unstemmed corpus\n")
cat("use load('pres-docs-us.Rdata') to retrieve into R\n")
##########################################################
pres.docs.us.crp <- pres.docs.crp
summary(pres.docs.us.crp)
save(pres.docs.us.crp, file = paste(project.name, "-us.Rdata", sep = ""))

##################
cat("stemming\n")
##################
pres.docs.st.crp <- tm_map(pres.docs.crp, stemDocument)
summary(pres.docs.st.crp)

###########################################################
cat("save the processed stemmed corpus\n")
cat("use load('pres-docs-st.Rdata') to retrieve into R\n")
###########################################################
save(pres.docs.st.crp, file = paste(project.name, "-st.Rdata", sep = ""))
#+end_src

** STRT Text mine /tm/ corpora						:chp:
*** STRT Introduction							:sec:
  :PROPERTIES:
  :table_export_file: ~/Dropbox/dissertation/data/dictionary.tsv
  :table_export_format: orgtbl-to-tsv
  :END:
# org-table-export
# C-c C-v t to create the R scripts

During this stage the actual text mining occurred.  The /corpus/ that was created and 
processed during the previous stages was converted into a =document-term= matrix. This is a matrix where the rows correspond to the documents and the columns to the terms. Statistical analyses are performed on these matrices.  These processes correspond to the step "text mining".

I created a text mining 'dictionary' for each document collection that contains terms of interest based on the research question and context (see table \ref{tbl:dictionary}). Notice that the words in the table are "stemmed" as well as "unstemmed" to match the outcome of the stemming that was done during the processing of the documents. the columns "congr-hearings corr-limit" and "pres-docs-corr-limit" are used as a cut-off value when association between terms are calculated.  The lower the correlation limit, the higher the number of associated terms that are returned.  The correlation limit was set at a level that returned about a dozen correlated terms.

# org-table-export
#+tblname: dictionary
#+label: tbl:dictionary
#+caption: Dictionary for congr-hearings and pres-docs
#+attr_latex: placement=[htb]
| stemmed     | unstemmed     | congr-hearings-corr-limit | pres-docs-corr-limit |
|-------------+---------------+---------------------------+----------------------|
| achiev      | achievement   |                     0.150 |                0.190 |
| assess      | assessment    |                     0.175 |                0.205 |
| compet      | competition   |                     0.150 |                0.170 |
| disadvantag | disadvantaged |                     0.150 |                0.255 |
| economi     | economy       |                     0.170 |                0.170 |
| educat      | education     |                     0.150 |                0.150 |
| equal       | equal         |                     0.180 |                0.207 |
| equiti      | equity        |                     0.140 |                0.140 |
| gap         | gap           |                     0.120 |                0.220 |
| global      | global        |                     0.160 |                0.180 |
| inequ       | inequity      |                     0.180 |                0.185 |
| math        | math          |                     0.160 |                0.200 |
| naep        | naep          |                     0.175 |                0.200 |
| pisa        | pisa          |                     0.320 |                0.320 |
| poverti     | poverty       |                     0.120 |                0.255 |
| reform      | reform        |                     0.140 |                0.190 |
| school      | school        |                     0.160 |                0.160 |
| standard    | standard      |                     0.150 |                0.150 |
| teacher     | teacher       |                     0.170 |                0.180 |
| timss       | timss         |                     0.170 |                0.170 |

The table of terms of interest derives from the interests of the investigation.

# Not implemented yet

#+tblname: dictionary-ngram
#+label: tbl:dictionary-ngram
#+caption: dictionary - ngrams
#+attr_latex: placement=[htb]
#| no child left behind |
#| race to the top      |

*** STRT Presidential documents						:sec:

To perform the text mining on the =pres-docs= project type the following commands at the /R/ prompt

#+begin_src R
setwd("~/Dropbox/dissertation/data")
source("pres-docs-tm.R")
#+end_src

#+begin_latex
\noindent
#+end_latex
This will run the following script

#+srcname: pres-docs-tm
#+begin_src R :tangle ~/Dropbox/dissertation/data/pres-docs-tm.R
#####################################
cat("load libraries and clean up\n")
#####################################
library("tm")
library("wordcloud")
library("ggplot2")

rm(list = ls())
graphics.off()

############################################
cat("define variables and set directory\n")
############################################
project.name    <- "pres-docs"
home.dir        <- "~/Dropbox/dissertation/data/"
graph.dir       <- "~/Dropbox/dissertation/graphic/"
setwd(home.dir)

#############################################################
cat("load processed unstemmed & stemmed corpi from files\n")
#############################################################
load(paste(project.name, "-us.Rdata", sep = "")) # unstemmed
load(paste(project.name, "-st.Rdata", sep = "")) # stemmed
summary(pres.docs.us.crp)
summary(pres.docs.st.crp)

#########################################
cat("import dictionary from tsv file\n")
#########################################
# Note: the column of correlation limits relevant to this document
# collection is the 4th of 4.
#
dictionary <- read.delim("dictionary.tsv", header = TRUE,
                         colClasses = c("character","character","numeric","numeric"),
                         fill = FALSE)

colnames(dictionary) <- c("term","term.us","disregard","cor.limit")

##########################################################
cat("convert stemmed corpus into document-term matrix\n")
##########################################################
dtm <- DocumentTermMatrix(pres.docs.st.crp)
dim(dtm)
print(dtm)

#########################################
cat("find frequencies & associations\n")
#########################################
dtm.freq <- findFreqTerms(dtm, lowfreq = 300) 
length(dtm.freq)
print(dtm.freq)
print(length(dtm.freq))

# loop over terms in dictionary
# make sure that the terms are in the dtm
#
DF <- dictionary[dictionary$term%in%colnames(dtm),]

for (i in 1:nrow(DF))
{
  print(DF$term[i])
  x <- findAssocs(dtm, DF$term[i], DF$cor.limit[i])
  print(x)
}

#########################################
cat("create DTM of terms of interest\n")
#########################################
dic.dtm <- DocumentTermMatrix(pres.docs.st.crp, list(dictionary = DF$term))

# inspect(dic.dtm)

#################################################################
cat("hierarchical clustering (Ward's minimum variance method)\n") 
#################################################################
# transpose the matrix
#
dtm.t <- t(dtm)

# remove all other terms from the matrix
# by the subset operator [.

dtm.rel <- dtm.t[dtm.freq, ]
dtm.cl  <- hclust(dist(dtm.rel), method = "ward")

# save dendrogram of frequent terms
#
pdf(paste(graph.dir, project.name, "-freq.pdf", sep = ""), height = 10, width = 22)
plclust(dtm.cl, unit = TRUE, main = "Presidential documents - Dendrogram", 
        xlab = "most frequent terms")
dev.off()

# do the same for terms in dictionary
# make sure the cor.limits are correct or it will fail
#
for (i in 1:nrow(DF))
{
  x          <- findAssocs(dtm, DF$term[i], DF$cor.limit[i])
  dtm.dic    <- dtm.t[names(x), ]
  dtm.dic.cl <- hclust(dist(dtm.dic), method = "ward")
  pdf(paste(graph.dir, "pres-docs-dic-", names(x)[1], ".pdf", sep = ""), 
      height = 10, width = 12)
  plclust(dtm.dic.cl, unit = TRUE, main = "Presidential documents - Dendrogram", 
      xlab = names(x)[1])
  dev.off()
}

###########################
cat("correlation plots\n")
###########################
pdf(paste(graph.dir, project.name, "-corr-dic.pdf", sep = ""), height = 10, width = 12)
plot(dtm, terms = DF$term, corThreshold = 0.1, 
     main = "Presidential documents - Dictionary")
dev.off()

# interesting
few.terms <- findFreqTerms(dtm, lowfreq = 810)
pdf(paste(graph.dir, project.name, "-corr-freq.pdf", sep = ""), height = 10, width = 12)
plot(dtm, terms = few.terms, corThreshold = 0.2,
     main = "Presidential documents - Most frequent terms")
dev.off()

#######################################
cat("word cloud - unstemmed corpus\n")
######################################
TermDocumentMatrix(pres.docs.us.crp)           # display info
tdm.us <- TermDocumentMatrix(pres.docs.us.crp) # save to object
removeSparseTerms(tdm.us, 0.95)                # display info
tdm <- removeSparseTerms(tdm.us, 0.95)         # save to object

m    <- as.matrix(tdm)
v    <- sort(rowSums(m), decreasing = TRUE)
d    <- data.frame(word = names(v), freq = v)
pal2 <- brewer.pal(8,"Dark2")

png(paste(graph.dir, project.name, "-cloud.png", sep = ""), width = 800, height = 800)
wordcloud(d$word, d$freq, scale = c(8, 0.3), min.freq = 2, 
          max.words = 100, random.order = TRUE, 
          rot.per = 0.15, colors = pal2)
dev.off()

#######################################
# frequency barplot - unstemmed corpus
#######################################
DF.1 <- data.frame(word = as.character(names(v)),
                   freq = as.integer(v),
                   stringsAsFactors = FALSE)

DF.2 <- DF.1[1:20, ]

p <- qplot(word, freq, data = DF.2, geom = "bar", fill = word)
ggsave(p, filename = paste(graph.dir, project.name, "-word-freq.pdf", sep = ""), width = 12)
#+end_src

*** STRT Congressional hearings						:sec:

Note that (1) this step is computationally intensive and takes a long time, and (2) the unstemmed term may not be unique for each stemmed term.

The list of the most frequent terms that are not in the table are: "account", "child", "district", "educ", "fund", "improv", "learn", "level", "nation", "nclb", "percent", "perform", "program", "provid", "requir", "scienc", "student", "support", and "system".

To perform the text mining on the =congr-hearings= project type the following commands at the /R/ prompt

#+begin_src R
setwd("~/Dropbox/dissertation/data")
source("congr-hearings-tm.R")
#+end_src

#+begin_latex
\noindent
#+end_latex
This will run the following script

#+srcname: congr-hearings-tm
#+begin_src R :tangle ~/Dropbox/dissertation/data/congr-hearings-tm.R
########################
cat("load libraries\n")
########################
library("tm")
library("wordcloud")
library("ggplot2")

##################
cat("clean up\n")
##################
rm(list = ls())
graphics.off()

###################
cat("variables\n")
###################
project.name <- "congr-hearings"
home.dir     <- "~/Dropbox/dissertation/data/"
graph.dir    <- "~/Dropbox/dissertation/graphic/"

#############################################################
cat("load processed unstemmed & stemmed corpi from files\n")
#############################################################
setwd(home.dir)
load(paste(project.name, "-us.Rdata", sep = "")) # unstemmed
load(paste(project.name, "-st.Rdata", sep = "")) # stemmed
summary(congr.hearings.ns.crp)
summary(congr.hearings.st.crp)

#########################################
cat("import dictionary from tsv file\n")
#########################################
# Note: the column of correlation limits relevant to this document
# collection is the 3rd of 4.
#
dictionary <- read.delim("dictionary.tsv", header = TRUE,
                          colClasses = c("character","character","numeric","numeric"),
                          fill = FALSE)

colnames(dictionary) <- c("term","term.us","cor.limit","disregard")

##########################################################
cat("convert stemmed corpus into document-term matrix\n")
##########################################################
dtm <- DocumentTermMatrix(congr.hearings.st.crp)
dim(dtm)
print(dtm)

#########################################
cat("find frequencies & associations\n")
#########################################
dtm.freq <- findFreqTerms(dtm, lowfreq = 2300) 
length(dtm.freq)
print(dtm.freq)
print(length(dtm.freq))

# loop over terms in dictionary
# make sure that the terms are in the dtm
# otherwise the script will fail at this point
#
DF <- dictionary[dictionary$term%in%colnames(dtm),]

for (i in 1:nrow(DF))
{
  print(DF$term[i])
  x <- findAssocs(dtm, DF$term[i], DF$cor.limit[i])
  print(x)
  write.table(x, file=paste("congr-hearings-dic-", DF$term[i], ".txt", sep= ""), quote = FALSE)
}

#########################################
cat("create DTM of terms of interest\n")
#########################################
dic.dtm <- DocumentTermMatrix(congr.hearings.st.crp, list(dictionary = DF$term))

# inspect(dic.dtm)

#################################################################
cat("hierarchical clustering (Ward's minimum variance method\n")
#################################################################
# transpose the matrix
#
dtm.t <- t(dtm)

# remove all other terms from the matrix
# by the subset operator [.

dtm.rel <- dtm.t[dtm.freq, ]
dtm.cl  <- hclust(dist(dtm.rel), method = "ward")

###########################################
cat("save dendrogram of frequent terms\n")
###########################################
pdf(paste(graph.dir, project.name, "-freq.pdf", sep = ""), height = 10, width = 22)
plclust(dtm.cl, unit = TRUE, main = "Congressional hearings - Dendrogram", 
        xlab = "most frequent terms")
dev.off()

#############################################
cat("do the same for terms in dictionary\n")
#############################################
# make sure the cor.limits are correct or it will fail
#
for (i in 1:nrow(DF))
{
  x          <- findAssocs(dtm, DF$term[i], DF$cor.limit[i])
  dtm.dic    <- dtm.t[names(x), ]
  dtm.dic.cl <- hclust(dist(dtm.dic), method = "ward")
  pdf(paste(graph.dir, "congr-hearings-dic-", names(x)[1], ".pdf", sep = ""), 
      height = 10, width = 12)
  plclust(dtm.dic.cl, unit = TRUE, main = "Congressional hearings - Dendrogram", 
      xlab = names(x)[1])
  dev.off()
}

###########################
cat("correlation plots\n")
###########################
pdf(paste(graph.dir, project.name, "-corr-dic.pdf", sep = ""), height = 10, width = 12)
plot(dtm, terms = DF$term, corThreshold = 0.05, 
     main = "Congressional hearings - Dictionary")
dev.off()

# interesting
freq.terms <- findFreqTerms(dtm, lowfreq = 3500)
pdf(paste(graph.dir, project.name, "-corr-freq.pdf", sep = ""), height = 10, width = 12)
plot(dtm, terms = freq.terms, corThreshold = 0.15,
     main = "Congressional hearings - Most frequent terms")
dev.off()

#######################################
cat("word cloud - unstemmed corpus\n")
#######################################
#
# without removing sparse terms the as.matrix function
# fails because of lack of memory
#
tdm.us <- TermDocumentMatrix(congr.hearings.us.crp)
print(tdm.us)
tdm <- removeSparseTerms(tdm.us, 0.95)
print(tdm)

m    <- as.matrix(tdm)
v    <- sort(rowSums(m), decreasing = TRUE)
d    <- data.frame(word = names(v), freq = v)
pal2 <- brewer.pal(8,"Dark2")

png(paste(graph.dir, project.name, "-cloud.png", sep = ""), width = 800, height = 800)
wordcloud(d$word, d$freq, scale = c(8, 0.3), min.freq = 2, 
          max.words = 100, random.order = TRUE, 
          rot.per = 0.15, colors = pal2)
dev.off()

##############################################
cat("frequency barplot - unstemmed corpus\n")
##############################################
DF.1 <- data.frame(word = as.character(names(v)),
                   freq = as.integer(v),
                   stringsAsFactors = FALSE)

DF.2 <- DF.1[1:20, ]

p <- qplot(word, freq, data = DF.2, geom = "bar", fill = word)
ggsave(p, filename = paste(graph.dir, project.name, "-word-freq.pdf", sep = ""), width = 12)
dev.off()
#####################
cat("end of script")
#####################
#+end_src

** STRT Brazos computer cluster						:chp:
Computationally intensive /R/ scripts were run on /brazos.tamu.edu/ a computer cluster managed by the Brazos Computational Resource at Texas A&M University. For details see http://brazos.tamu.edu.

1. The /R/ data files are kept in the directory ~/Data
2. The batch program is run by entering the following command:

#+begin_src sh
qsub -q general -m abe congr-hearings-1.pbs
qsub -q general -m abe pres-docs-1.pbs
#+end_src

Which runs the following bath files:

#+begin_src sh
#PBS -l nodes=1:ppn=8
#PBS -l walltime=05:00:00   ### for pres-docs, but 10:00:00 for congr-hearings
#PBS -l mem=16GB
#PBS -q general

# run: qsub -q general -m abe congr-hearings-1.pbs

. /etc/profile.d/modules.sh
module load r/2.14.1/gcc/64

cd $HOME
R CMD BATCH /home/hindiogine/congr-hearings-process.R

wait
exit 0
#+end_src

3. The files =congr-hearings-process.R= and =pres-docs-process.R= are transferred by =scp= to the computer cluster and then slightly edited to conform to the local directories.

4. Once the job has started one can check its status by entering =qstat -u hindiogine=.

More info is obtained by entering: =qstat -f JobID=

5. To terminate a job enter  =qsig -s 15 JobID=.  Use =qstat= to find out the job id.

6. To install a /R/ library:

#+begin_src sh
module avail
module load java/sun-jre-1.6
module load r/2.14.1/gcc/64
R
install.packages("tm")
install.packages("Snowball")
#+end_src

** STRT QDA database searches						:chp:

In this appendix I present the results of the /RQDA/ database searches for specific QDA codes and code combinations.  These results were used in Chapter 5.  See Table \ref{tbl:qda-codes} and Table \ref{tbl:code-descr} for specifics about the QDA codes.  Please note that not all results are mentioned in that chapter.

To search for a specific word or word combination in the QDA database use the following /R/ code

#+begin_src R
library("RQDA")
openProject("~/Dropbox/dissertation/data/<my project>.rqda", updateGUI = TRUE)
RQDAQuery("select fid, seltext from coding where seltext like '%my words%';")
#+end_src

In this appendix section are the /R/ codes for the Presidential documents QDA database searches and their results.  The code will first query the Presidential documents and then the Congressional hearings.

*** Procedures

To find the source document paragraphs pertaining to the discussion in Subsection 5.2.2 I performed the following set operation: /EducFunding/ AND /SchoolAccount/ AND (/SchoolFixClose/ OR /StudentAssess/ OR /TeacherAssess/).

I coded this set operation into the following /R/ code

#+begin_src R
library("RQDA")
openProject("~/Dropbox/dissertation/data/pres-docs.rqda", updateGUI = TRUE)
my.list <- getCodingsByOne(7) %and% getCodingsByOne(19) %and% (getCodingsByOne(24)
 %or% getCodingsByOne(28) %or% getCodingsByOne(35))
str(my.list)
unique(my.list$filename)
my.list
closeProject()
openProject("~/Dropbox/dissertation/data/congr-hearings.rqda", updateGUI = TRUE)
my.list <- getCodingsByOne(7) %and% getCodingsByOne(19) %and% (getCodingsByOne(24)
 %or% getCodingsByOne(28) %or% getCodingsByOne(35))
str(my.list)
unique(my.list$filename)
my.list
#+end_src

Presidential documents results: 35 codings from 30 files.  Congressional hearings results: 61 codings from 28 files.

*** Analyses & Reflections

To find the source document paragraphs pertaining to the discussion in Subsection 5.2.3, I performed the following set operation: (/EducEquity/ OR /EducGap/) AND (/NationBestFirst/ OR /NationDuty/ OR /NationInterest/) OR
(/EducEquity/ OR /EducGap/) AND (/SchoolAccount/ OR /SchoolQuality/ OR /StudentExpectation/ OR /TeacherQuality/)

I coded this set operation into the following \emph{R} code

#+begin_src R
library("RQDA")
openProject("~/Dropbox/dissertation/data/pres-docs.rqda", updateGUI = TRUE)
my.list <- ((getCodingsByOne(5) %or% getCodingsByOne(6)) %and% 
(getCodingsByOne(11) %or% getCodingsByOne(12) %or% getCodingsByOne(14)))
%or% ((getCodingsByOne(5) %or% getCodingsByOne(6)) %and% getCodingsByOne(19)
%and% (getCodingsByOne(25) %or% getCodingsByOne(31) %or% getCodingsByOne(38)))
str(my.list)
unique(my.list$filename)
my.list
closeProject()
openProject("~/Dropbox/dissertation/data/congr-hearings.rqda", updateGUI = TRUE)
my.list <- ((getCodingsByOne(5) %or% getCodingsByOne(6)) %and% 
(getCodingsByOne(11) %or% getCodingsByOne(12) %or% getCodingsByOne(14)))
%or% ((getCodingsByOne(5) %or% getCodingsByOne(6)) %and% getCodingsByOne(19)
%and% (getCodingsByOne(25) %or% getCodingsByOne(31) %or% getCodingsByOne(38)))
str(my.list)
unique(my.list$filename)
my.list
#+end_src

Presidential document results: 72 codings from 56 files.  Congressional hearings results: 288 codings from 71 files.

I performed a search for education research, /EducResearch/, however not in combination with any other codes and a combination of /EducMathSci/ with /NationInterest/ as is shown in the following piece of /R/ code

#+begin_src R
getCodingsByOne(9)
getCodingsByOne(8) %and% getCodingsByOne(14)
#+end_src

*** Calculations & Tactics

To find the source document paragraphs pertaining to the discussion in Subsection 5.2.4, I performed the following set operations:

/EducGap/ AND /EducStandard/

The corresponding /R/ code is

#+begin_src R
getCodingByOne(6) %and% getCodingByOne(10)
#+end_src

/EducGap/ AND /TeacherApprec/

The corresponding /R/ code is

#+begin_src R
getCodingByOne(6) %and% getCodingByOne(34)
#+end_src

/TeacherApprec/ AND /TeacherAssess/


The corresponding /R/ code is

#+begin_src R
getCodingByOne(35) %and% getCodingByOne(34)
#+end_src

*** Population

To find the source document paragraphs pertaining to the discussion in Subsection 5.2.5, I performed the following set operations:

/EducEquity/ OR /EducGap/ AND /StudentPoverty/


The corresponding /R/ code is 

#+begin_src R
(getCodingsByOne(5) %or% getCodingsByOne(6)) %and% getCodingsByOne(33)
#+end_src

(/EducEquity/ OR /EducGap/) AND /StudentPoverty/ AND /NationProsperity/


The corresponding /R/ code is

#+begin_src R
(getCodingsByOne(5) %or% getCodingsByOne(6)) %and% getCodingsByOne(33) %and% getCodingsByOne(16)
#+end_src


(/EducEquity/ OR /EducGap/) AND /ParentInvolve/

The corresponding /R/ code is

#+begin_src R
(getCodingsByOne(5) %or% getCodingsByOne(6)) %and% getCodingsByOne(18)
#+end_src

*** Form of knowledge

To find the source document paragraphs pertaining to the discussion in Subsection 5.2.6, I performed the following set operation:

(/EducGap/ OR /EducEquity/) AND (/NationEcon/ OR /NationInternComp/)


The corresponding /R/ code is

#+begin_src R
(getCodingsByOne(5) %or% getCodingsByOne(6)) %and% (getCodingsByOne(13) %or% getCodingsByOne(15) %or% getCodingsByOne(17))
#+end_src

*** Apparatuses of security

To find the source document paragraphs pertaining to the discussion in Subsection 5.2.7, I performed the following set operation:

(/EducGap/ OR /EducEquity/) AND /StudentAll/

The corresponding /R/ code is

#+begin_src R
(getCodingsByOne(5) %or% getCodingsByOne(6)) %and% getCodingsByOne(27)
#+end_src


(/EducGap/ OR /EducEquity/) AND /StudentAll/ AND /EducAchiev/


#+begin_src R
(getCodingsByOne(5) %or% getCodingsByOne(6)) %and% getCodingsByOne(27) %and% getCodingsByOne(4)
#+end_src


(/EducGap/ OR /EducEquity/) AND (/TeacherCert/ OR /TearcherQuality/ OR /TeacherReplace/)

The corresponding /R/ code is

#+begin_src R
(getCodingsByOne(5) %or% getCodingsByOne(6)) %and% (getCodingsByOne(36) %or% getCodingsByOne(38) %or% getCodingsByOne(39))
#+end_src

*** Local or State Control versus Federal Control

To find the source document paragraphs pertaining to the discussion in Subsection 5.4.2, I performed the following set operation: (/ControlLocal/ OR /ControlState/) AND /ControlFederal/.

I coded this set operation into the following /R/ code

#+begin_src R
library("RQDA")
openProject("~/Dropbox/dissertation/data/pres-docs.rqda", updateGUI = TRUE)
my.list <- getCodingsByOne(1) %and% (getCodingsByOne(2) %or% getCodingsByOne(3)) 
str(my.list)
unique(my.list$filename)
my.list
closeProject()
openProject("~/Dropbox/dissertation/data/congr-hearings.rqda", updateGUI = TRUE)
my.list <- getCodingsByOne(1) %and% (getCodingsByOne(2) %or% getCodingsByOne(3)) 
str(my.list)
unique(my.list$filename)
my.list
#+end_src

Presidential documents result: 11 codings from 9 files.  Congressional hearings result: 34 codings from 15 files.

*** Control of Education and the Economy

To find the source document paragraphs pertaining to the discussion in Subsection 5.4.3, I performed the following set operation: (/ControlLocal/ OR /ControlState/ OR /ControlFederal/) AND /NationInternComp/.

I coded this set operation into the following /R/ code

#+begin_src R
library("RQDA")
openProject("~/Dropbox/dissertation/data/pres-docs.rqda", updateGUI = TRUE)
my.list <- (getCodingsByOne(1) %or% getCodingsByOne(2) %or% getCodingsByOne(3)) 
%and% getCodingsByOne(15)
str(my.list)
unique(my.list$filename)
my.list
closeProject()
openProject("~/Dropbox/dissertation/data/congr-hearings.rqda", updateGUI = TRUE)
my.list <- (getCodingsByOne(1) %or% getCodingsByOne(2) %or% getCodingsByOne(3)) 
%and% getCodingsByOne(15)
str(my.list)
unique(my.list$filename)
my.list
#+end_src

Presidential documents result: 10 codings from 10 files.  Congressional hearings result: 10 codings from 5 files.

*** Control of Education and Funding

To find the source document paragraphs pertaining to the discussion in Subsection 5.4.4, I performed the following set operation:

To find the codings that contained both references to governmental control and funding I performed the following set operation: (/ControlLocal/ OR /ControlFederal/ OR /ControlState/) AND /EducFunding/.

I coded this set operation into the following /R/ code

#+begin_src R
library("RQDA")
openProject("~/Dropbox/dissertation/data/pres-docs.rqda", updateGUI = TRUE)
my.list <- (getCodingsByOne(1) %or% getCodingsByOne(2) %or% getCodingsByOne(3)) 
%and% getCodingsByOne(7)
str(my.list)
unique(my.list$filename)
my.list
closeProject()
openProject("~/Dropbox/dissertation/data/congr-hearings.rqda", updateGUI = TRUE)
my.list <- (getCodingsByOne(1) %or% getCodingsByOne(2) %or% getCodingsByOne(3)) 
%and% getCodingsByOne(7)
str(my.list)
unique(my.list$filename)
my.list
#+end_src

Presidential documents result: 62 codings from 41 files.  Congressional hearings result: 141 codings from 42 files.

*** Control of Education and Testing

To find the codings that contained both references to governmental control and testing or assessment, I performed the following set operation: (/ControlLocal/ OR /ControlFederal/ OR /ControlState/) AND (/StudentAssess/ OR /TeacherAssess/).

I coded this set operation into the following /R/ code

#+begin_src R
library("RQDA")
openProject("~/Dropbox/dissertation/data/pres-docs.rqda", updateGUI = TRUE)
my.list <- (getCodingsByOne(1) %or% getCodingsByOne(2) %or% getCodingsByOne(3)) 
%and% (getCodingsByOne(28) %or% getCodingsByOne(35))
str(my.list)
unique(my.list$filename)
my.list
closeProject()
openProject("~/Dropbox/dissertation/data/congr-hearings.rqda", updateGUI = TRUE)
my.list <- (getCodingsByOne(1) %or% getCodingsByOne(2) %or% getCodingsByOne(3)) 
%and% (getCodingsByOne(28) %or% getCodingsByOne(35))
str(my.list)
unique(my.list$filename)
my.list
#+end_src

Presidential documents results: 44 codings from 30 files.  Congressional hearing results: 80 codings from 29 files.

*** Control of Education and Accountability

To find the codings that contained both references to governmental control and testing or assessment, I performed the following set operation: (/ControlLocal/ OR /ControlFederal/ OR /ControlState/) AND /SchoolAccount/.

I coded this set operation into the following /R/ code

#+begin_src R
library("RQDA")
openProject("~/Dropbox/dissertation/data/pres-docs.rqda", updateGUI = TRUE)
my.list <- (getCodingsByOne(1) %or% getCodingsByOne(2) %or% getCodingsByOne(3)) 
%and% getCodingsByOne(19)
str(my.list)
unique(my.list$filename)
my.list
closeProject()
openProject("~/Dropbox/dissertation/data/congr-hearings.rqda", updateGUI = TRUE)
my.list <- (getCodingsByOne(1) %or% getCodingsByOne(2) %or% getCodingsByOne(3)) 
%and% getCodingsByOne(19)
str(my.list)
unique(my.list$filename)
my.list
#+end_src

Results: 39 codings from 27 files.  Congressional hearings results: 65 codings from 29 files.

*** Control of Education and School intervention

To find the source document paragraphs pertaining to the discussion in Subsection 5.4.7, I performed the following set operation: (/ControlLocal/ OR /ControlFederal/ OR /ControlState/) AND /SchoolFixClose/.

I coded this set operation into the following /R/ code

#+begin_src R
library("RQDA")
openProject("~/Dropbox/dissertation/data/pres-docs.rqda", updateGUI = TRUE)
my.list <- (getCodingsByOne(1) %or% getCodingsByOne(2) %or% getCodingsByOne(3)) 
%and% getCodingsByOne(24)
str(my.list)
unique(my.list$filename)
my.list
closeProject()
openProject("~/Dropbox/dissertation/data/congr-hearings.rqda", updateGUI = TRUE)
my.list <- (getCodingsByOne(1) %or% getCodingsByOne(2) %or% getCodingsByOne(3)) 
%and% getCodingsByOne(24)
str(my.list)
unique(my.list$filename)
my.list
#+end_src

Presidential documents results: 15 codings from 13 files.  Congressional hearing results: 17 codings from 10 files.

*** Gaps and government

To find the source document paragraphs pertaining to the discussion in Section 5.3

(/EducGap/ OR /EducEquity/) AND (/ControlFed/ OR /ControlState/ OR /ControlLocal/)

I coded this set operation into the following /R/ code

#+begin_src R
(getCodingsByOne(6) %or% getCodingsByOne(5))  %and% (getCodingsByOne(1) %or% getCodingsByOne(2) %or% getCodingsByOne(3))
#+end_src

** DONE Software installation						:chp:

I use the following computer programs for my work: GNU Emacs (http://www.gnu.org/software/emacs/), LaTeX (http://www.latex-project.org/), and R-project (http://www.r-project.org). These programs can be installed using the normal guidelines for the operating system. However, there are /R/ libraries that need to be added to the default installation. They are =tm= for text mining and =RQDA= for qualitative data analysis.

First install the /R/ packages from the normal repository using apt-get

#+begin_src sh
sudo apt-get install r-base \
                     r-recommended \
                     r-base-dev \
                     r-doc-pdf \
                     r-mathlib \
                     r-cran-odbc \
                     r-cran-rsprng \
                     r-cran-rpvm \
                     r-cran-rmpi \
                     gdal-bin \
                     mpi-default-dev \
                     libgeos-dev \
                     xml2 \
                     libxml2-dev \
                     libglu1-mesa-dev \
                     libgtk2.0-dev \
                     graphviz-dev \
                     gtk2-engines-pixbuf \
                     sqlite3 \
                     sqlite3-doc
#+end_src

Make sure that you have a Java JDK also installed. Then, start /R/ as administrator and type at the prompt

#+begin_src R
install.packages("tm", dependencies=TRUE)
install.packages("igraph", dependencies=TRUE)
install.packages("RGtk2", dependencies=TRUE)
install.packages("gWidgetsRGtk2", dependencies=TRUE)
install.packages("ecodist", dependencies=TRUE)
# install.packages(c("RQDA","RQDAtm"), repos="http://R-Forge.R-project.org")
source("http://bioconductor.org/biocLite.R")
biocLite("Rgraphviz")
#+end_src

If we want the bleeding edge version of /RQDA/ (latest is revision 422) we need to download it from the developer repository

#+begin_src sh
cd ~/Downloads
rm -rf rqda
svn checkout svn://svn.r-forge.r-project.org/svnroot/rqda
#+end_src

Then log into R as root and enter

#+begin_src R
install.packages("/home/henk/Downloads/rqda/pkg/RQDA", repos = NULL, type = "source")
install.packages("/home/henk/Downloads/rqda/pkg/RQDAtm", repos = NULL, type = "source")
#+end_src

#+begin_src R
install.packages("tm.corpus.Congress", repos = "http://datacube.wu.ac.at")
data(Congress)
summary(Congress)
#+end_src

Installation from the UNIX command line of a downloaded package:

#+begin_src sh
sudo R CMD INSTALL  <package-name>.tar.gz
#+end_src

* misc bash & R code 					      :noexport:temp:
** Text mine "term frequency scatter plot"

#+begin_src R
rm(list = ls())
doInstall <- TRUE  # Change to FALSE if you don't want packages installed.
toInstall <- c("zoo", "tm", "ggplot2", "Snowball")
if(doInstall){install.packages(toInstall, repos = "http://cran.r-project.org")}
lapply(toInstall, library, character.only = TRUE)
 
# From: http://www.cnn.com/2012/10/03/politics/debate-transcript/index.html
Transcript <- readLines("https://raw.github.com/dsparks/Test_image/master/Denver_Debate_Transcript.txt")
head(Transcript, 20)
 
Transcript <- data.frame(Words = Transcript, Speaker = NA, stringsAsFactors = FALSE)
Transcript$Speaker[regexpr("LEHRER: ", Transcript$Words) != -1] <- 1
Transcript$Speaker[regexpr("OBAMA: ", Transcript$Words) != -1] <- 2
Transcript$Speaker[regexpr("ROMNEY: ", Transcript$Words) != -1] <- 3
table(Transcript$Speaker)
Transcript$Speaker <- na.locf(Transcript$Speaker)
 
# Remove moderator:
Transcript <- Transcript[Transcript$Speaker != 1, ]
 
myCorpus <- Corpus(DataframeSource(Transcript))
inspect(myCorpus)
 
myCorpus <- tm_map(myCorpus, tolower)  # Make lowercase
myCorpus <- tm_map(myCorpus, removePunctuation, preserve_intra_word_dashes = FALSE)
myCorpus <- tm_map(myCorpus, removeWords, stopwords("english"))  # Remove stopwords
myCorpus <- tm_map(myCorpus, removeWords, c("lehrer", "obama", "romney"))
myCorpus <- tm_map(myCorpus, stemDocument)  # Stem words
 
inspect(myCorpus)
docTermMatrix <- DocumentTermMatrix(myCorpus)
 
docTermMatrix <- inspect(docTermMatrix)
sort(colSums(docTermMatrix))
table(colSums(docTermMatrix))
 
termCountFrame <- data.frame(Term = colnames(docTermMatrix))
termCountFrame$Obama <- colSums(docTermMatrix[Transcript$Speaker == 2, ])
termCountFrame$Romney <- colSums(docTermMatrix[Transcript$Speaker == 3, ])
 
head(termCountFrame)
 
# Plot
zp1 <- ggplot(termCountFrame)
zp1 <- zp1 + geom_text(aes(x = Obama, y = Romney, label = Term))
print(zp1)
#+end_src

** Remove newlines (\n):

#+begin_src sh
tr '\n' ' ' < input
tr -d '\n' < input
cat test.txt |  while read line; do echo -n "$line "; done
sed -i "s/\\\n//g" filename
#+end_src

** apply codes

The following code will create a vector of integers, where each element is the number of the first line of each paragraph. Note that title lines will be counted as paragraphs also.

Concerning *selfirst* and *selend* I too thought it should be integers - maybe Wincent can help on this point - and that they state the first and the last character of a coding where the number refers to the overall characters in a file - most likely without something like newline things.

To find the first and last characters of a file I wrote something like this:
####################
The following /R/ script assumes that paragraphs are separated by blank lines.  It finds the line number of blank lines and it also finds the position of the first and last character of each line, including blank lines.

#+srcname: pres-docs-auto-codes
#+begin_src R
# load library, DBI is loaded automatically
library("RSQLite")

# define variables
#
home.dir <- "/home/henk/Dropbox/dissertation/data"
data.dir <- paste(home.dir, "/pres-docs", sep="")
user     <- Sys.getenv("USERNAME")
today    <- Sys.Date()
drv      <- dbDriver("SQLite")

# connect to project database
setwd(home.dir)
con <- dbConnect(drv, dbname = "pres-docs.rqda")

# vector of lines number where a certain term appears
# need to take into account that the word may be at beginning
# or end of line
my.code <- grep("* gap *", txt, ignore.case = TRUE, value = FALSE)
print(my.code)

# read file names from database into a data frame
res <- dbSendQuery(con, "select id, name from source where status==1")
data <- fetch(res)
dbClearResult(res)
str(data)

####################################
# build a list of character vectors
####################################

setwd(data.dir)

# find no of lines (nol) of each text file
nol <- data.frame(matrix(nrow = length(data$id), ncol =2))
total <- 0
for (i in 1:length(data$id))
{
  txt <- readLines(data$name[i])
  nol[i,] <- length(txt)
  # running total
}


tmp <- data.frame(matrix(nrow = length(data$id), ncol = 2)) 

for (i in 1:length(data$id))
{
  tmp[i,] <- readLines(data$name[i])
}


# vector of numbers of blank lines
paragraph <- grep("^$", txt)
paragraph
length(paragraph)

# vector of number of characters of each line
txt.l <- nchar(txt)
txt.l
length(txt.l)

# column names "from" and "to" -> data frame
txt.df <- cbind(txt.l,NA,NA)
colnames(txt.df) <- c("length","from","to")
str(txt.df)

x <- 0
for (i in 1:length(txt))
{
  x <- txt.df[i,1]+x
  txt.df[i,3]  <- x
  if (i==1) txt.df[i,2] <- 0 
       else txt.df[i,2] <- txt.df[i-1,3]
}
str(txt.df)

# find blank line above and below this line
above      <- vector()
line.above <- vector()
line.below <- vector()

for (i in 1:length(my.code))
{
  above[i] <- findInterval(my.code[i], paragraph)
}
# actually "above[i]" contains the position in the index of the paragraph vector
for (i in 1:length(above))
{
  line.above[i] <- paragraph[above[i]]
  line.below[i] <- paragraph[above[i]+1]
}
selfirst <- vector()  # integer
selend   <- vector()  # integer
seltext  <- vector()  # character

# find positions and adjust for "\n"
for (i in 1:length(above)) 
{ 
  seltext[i]  <- paste(txt[(line.above[i]+1):(line.below[i]-1)], collapse=" ")
  selfirst[i] <- as.integer(txt.df[line.above[i],2]+line.above[i])
  selend[i]   <- as.integer(txt.df[line.below[i],3]+line.below[i]-2)
}
# create data frame
DF <- data.frame(cid      = rep(as.integer(7), times=length(above)),
                 fid      = rep(as.integer(1), times=length(above)),
                 seltext  = as.character(seltext),
                 selfirst = as.integer(selfirst),
                 selend   = as.integer(selend),
                 status   = rep(as.integer(1), times=length(above)),
                 owner    = rep(as.character(user), times=length(above)),
                 date     = rep(as.character(today), times=length(above)),
                 memo     = rep("computer generated", times=length(above)),
              stringsAsFactors=FALSE)
str(DF)

# Connect and write to database
setwd(home.dir)

# dbWriteTable(con, "coding", DF, row.names = FALSE, overwrite = TRUE)
# dbReadTable(con, "coding")
dbDisconnect(con)
#+end_src

** DONE create code lists

Start /RQDA/ and open the =congr-hearings= project. Then run the following /R/ script. The script will generate the file =congr-hearings-codes.csv= in the =data= directory.

#+srcname: congr-hearings-codes-find
#+begin_src R
# define variables, set working directory
#
out.file     <- "congr-hearings-codes.csv"
sql          <- "select id, name, file from source where status==1"
home.dir     <- "~/Dropbox/dissertation/data"
setwd(home.dir)

# remove old csv file
#
if (file.exists(out.file)) file.remove(out.file)

# load text file contents into data frame
#
txt.df        <- RQDAQuery(sql)
names(txt.df) <- c("FileID","FileName","FileContent")
str(txt.df)

# character vector of all documents in collection
# Warning: can be very large
#
txt.vc <- vector()

# generate list for code (1) Accountability
code.pattern <- "[Aa]ccountability"
code.id      <- "1"
code.name    <- "Accountability"
#
for (j in 1:nrow(txt.df))
{
  # split content into a vector of lines and grep
  #
  txt.vc       <- unlist(strsplit(txt.df[[3]][j], "\n", fixed=TRUE))
  code.match   <- grep(pattern=code.pattern, txt.vc, ignore.case=FALSE, value=FALSE)
  
  if (length(code.match>0))
  { # data frames of grep matches
    #
    DF <- data.frame(CodeID   = code.id,
                     CodeName = code.name,
                     FileID   = as.integer(txt.df[[1]][j]),
                     FileName = as.character(txt.df[[2]][j]),
                     RowNum   = code.match)

    # write matches to csv file
    write.table(DF, file=out.file, append=TRUE, 
                col.names=TRUE, row.names=FALSE, sep=",")
  }
}

# generate list for code (2) Achievement
code.pattern <- "[Aa]chievement"
code.id      <- "2"
code.name    <- "Achievement"
#
for (j in 1:nrow(txt.df))
{
  # split content into a vector of lines and grep
  #
  txt.vc       <- unlist(strsplit(txt.df[[3]][j], "\n", fixed=TRUE))
  code.match   <- grep(pattern=code.pattern, txt.vc, ignore.case=FALSE, value=FALSE)

  if (length(code.match>0))
  { # data frames of grep matches
    #
    DF <- data.frame(CodeID   = code.id,
                     CodeName = code.name,
                     FileID   = as.integer(txt.df[[1]][j]),
                     FileName = as.character(txt.df[[2]][j]),
                     RowNum   = code.match)

    # write matches to csv file
    write.table(DF, file=out.file, append=TRUE, 
                col.names=TRUE, row.names=FALSE, sep=",")
  }
}

# generate list for code (3) AllStudents
code.pattern <- "[Aa]ll students"
code.id      <- "3"
#
for (j in 1:nrow(txt.df))
{
  # split content into a vector of lines and grep
  #
  txt.vc       <- unlist(strsplit(txt.df[[3]][j], "\n", fixed=TRUE))
  code.match   <- grep(pattern=code.pattern, txt.vc, ignore.case=FALSE, value=FALSE)

  if (length(code.match>0))
  { # data frames of grep matches
    #
    DF <- data.frame(CodeID   = code.id,
                     CodeName = code.name,
                     FileID   = as.integer(txt.df[[1]][j]),
                     FileName = as.character(txt.df[[2]][j]),
                     RowNum   = code.match)

    # write matches to csv file
    write.table(DF, file=out.file, append=TRUE, 
                col.names=TRUE, row.names=FALSE, sep=",")
  }  
}

# generate list for code (4) AssessStudent and (5) AssessSchool
code.pattern <- "[Aa]ssess"
code.id      <- "4 or 5"
code.name    <- "Assess +"
#
for (j in 1:nrow(txt.df))
{
  # split content into a vector of lines and grep
  #
  txt.vc       <- unlist(strsplit(txt.df[[3]][j], "\n", fixed=TRUE))
  code.match   <- grep(pattern=code.pattern, txt.vc, ignore.case=FALSE, value=FALSE)
  
  if (length(code.match>0))
  { # data frames of grep matches
    #
    DF <- data.frame(CodeID   = code.id,
                     CodeName = code.name,
                     FileID   = as.integer(txt.df[[1]][j]),
                     FileName = as.character(txt.df[[2]][j]),
                     RowNum   = code.match)

    # write matches to csv file
    write.table(DF, file=out.file, append=TRUE, 
                col.names=TRUE, row.names=FALSE, sep=",")
  }
}

# generate list for code (6) BestFirst
code.pattern <- "first in the world"
code.id      <- "6"
code.name    <- "BestFirst"
#
for (j in 1:nrow(txt.df))
{
  # split content into a vector of lines and grep
  #
  txt.vc       <- unlist(strsplit(txt.df[[3]][j], "\n", fixed=TRUE))
  code.match   <- grep(pattern=code.pattern, txt.vc, ignore.case=FALSE, value=FALSE)
  
  if (length(code.match>0))
  { # data frames of grep matches
    #
    DF <- data.frame(CodeID   = code.id,
                     CodeName = code.name,
                     FileID   = as.integer(txt.df[[1]][j]),
                     FileName = as.character(txt.df[[2]][j]),
                     RowNum   = code.match)

    # write matches to csv file
    write.table(DF, file=out.file, append=TRUE, 
                col.names=TRUE, row.names=FALSE, sep=",")
  }
}

# generate list for code (7) Diversity
code.pattern <- "diversity"
code.id      <- "7"
code.name    <- "Diversity"
#
for (j in 1:nrow(txt.df))
{
  # split content into a vector of lines and grep
  #
  txt.vc       <- unlist(strsplit(txt.df[[3]][j], "\n", fixed=TRUE))
  code.match   <- grep(pattern=code.pattern, txt.vc, ignore.case=FALSE, value=FALSE)
  
  if (length(code.match>0))
  { # data frames of grep matches
    #
    DF <- data.frame(CodeID   = code.id,
                     CodeName = code.name,
                     FileID   = as.integer(txt.df[[1]][j]),
                     FileName = as.character(txt.df[[2]][j]),
                     RowNum   = code.match)

    # write matches to csv file
    write.table(DF, file=out.file, append=TRUE, 
                col.names=TRUE, row.names=FALSE, sep=",")
  }
}

# generate list for code (8) DropOut
code.pattern <- "[Dd]ropout"
code.id      <- "8"
code.name    <- "DropOut"
#
for (j in 1:nrow(txt.df))
{
  # split content into a vector of lines and grep
  #
  txt.vc       <- unlist(strsplit(txt.df[[3]][j], "\n", fixed=TRUE))
  code.match   <- grep(pattern=code.pattern, txt.vc, ignore.case=FALSE, value=FALSE)
  
  if (length(code.match>0))
  { # data frames of grep matches
    #
    DF <- data.frame(CodeID   = code.id,
                     CodeName = code.name,
                     FileID   = as.integer(txt.df[[1]][j]),
                     FileName = as.character(txt.df[[2]][j]),
                     RowNum   = code.match)

    # write matches to csv file
    write.table(DF, file=out.file, append=TRUE, 
                col.names=TRUE, row.names=FALSE, sep=",")
  }
}

# generate list for code (9) Economy
code.pattern <- "[Ee]conomy"
code.id      <- "9"
code.name    <- "Economy"
#
for (j in 1:nrow(txt.df))
{
  # split content into a vector of lines and grep
  #
  txt.vc       <- unlist(strsplit(txt.df[[3]][j], "\n", fixed=TRUE))
  code.match   <- grep(pattern=code.pattern, txt.vc, ignore.case=FALSE, value=FALSE)
  
  if (length(code.match>0))
  { # data frames of grep matches
    #
    DF <- data.frame(CodeID   = code.id,
                     CodeName = code.name,
                     FileID   = as.integer(txt.df[[1]][j]),
                     FileName = as.character(txt.df[[2]][j]),
                     RowNum   = code.match)

    # write matches to csv file
    write.table(DF, file=out.file, append=TRUE, 
                col.names=TRUE, row.names=FALSE, sep=",")
  }
}

# generate list for code (10) EducEquity
code.pattern <- "[Ee]quity"
code.id      <- "10"
code.name    <- "EducEquity"
#
for (j in 1:nrow(txt.df))
{
  # split content into a vector of lines and grep
  #
  txt.vc       <- unlist(strsplit(txt.df[[3]][j], "\n", fixed=TRUE))
  code.match   <- grep(pattern=code.pattern, txt.vc, ignore.case=FALSE, value=FALSE)
  
  if (length(code.match>0))
  { # data frames of grep matches
    #
    DF <- data.frame(CodeID   = code.id,
                     CodeName = code.name,
                     FileID   = as.integer(txt.df[[1]][j]),
                     FileName = as.character(txt.df[[2]][j]),
                     RowNum   = code.match)

    # write matches to csv file
    write.table(DF, file=out.file, append=TRUE, 
                col.names=TRUE, row.names=FALSE, sep=",")
  }
}

# generate list for code (11) Funding
code.pattern <- "[Ff]unding"
code.id      <- "11"
code.name    <- "Funding"
#
for (j in 1:nrow(txt.df))
{
  # split content into a vector of lines and grep
  #
  txt.vc       <- unlist(strsplit(txt.df[[3]][j], "\n", fixed=TRUE))
  code.match   <- grep(pattern=code.pattern, txt.vc, ignore.case=FALSE, value=FALSE)
  
  if (length(code.match>0))
  { # data frames of grep matches
    #
    DF <- data.frame(CodeID   = code.id,
                     CodeName = code.name,
                     FileID   = as.integer(txt.df[[1]][j]),
                     FileName = as.character(txt.df[[2]][j]),
                     RowNum   = code.match)

    # write matches to csv file
    write.table(DF, file=out.file, append=TRUE, 
                col.names=TRUE, row.names=FALSE, sep=",")
  }
}

# generate list for code (12) Gap
code.pattern <- "gap"
code.id      <- "12"
code.name    <- "Gap"
#
for (j in 1:nrow(txt.df))
{
  # split content into a vector of lines and grep
  #
  txt.vc       <- unlist(strsplit(txt.df[[3]][j], "\n", fixed=TRUE))
  code.match   <- grep(pattern=code.pattern, txt.vc, ignore.case=FALSE, value=FALSE)
  
  if (length(code.match>0))
  { # data frames of grep matches
    #
    DF <- data.frame(CodeID   = code.id,
                     CodeName = code.name,
                     FileID   = as.integer(txt.df[[1]][j]),
                     FileName = as.character(txt.df[[2]][j]),
                     RowNum   = code.match)

    # write matches to csv file
    write.table(DF, file=out.file, append=TRUE, 
                col.names=TRUE, row.names=FALSE, sep=",")
  }
}

# generate list for code (13) Globalization
code.pattern <- "[Gg]lobalization"
code.id      <- "13"
code.name    <- "Globalization"
#
for (j in 1:nrow(txt.df))
{
  # split content into a vector of lines and grep
  #
  txt.vc       <- unlist(strsplit(txt.df[[3]][j], "\n", fixed=TRUE))
  code.match   <- grep(pattern=code.pattern, txt.vc, ignore.case=FALSE, value=FALSE)
  
  if (length(code.match>0))
  { # data frames of grep matches
    #
    DF <- data.frame(CodeID   = code.id,
                     CodeName = code.name,
                     FileID   = as.integer(txt.df[[1]][j]),
                     FileName = as.character(txt.df[[2]][j]),
                     RowNum   = code.match)

    # write matches to csv file
    write.table(DF, file=out.file, append=TRUE, 
                col.names=TRUE, row.names=FALSE, sep=",")
  }
}

# generate list for code (14) HumanBenefit
code.pattern <- "[Pp]rosperity"
code.id      <- "14"
code.name    <- "HumanBenefit"
#
for (j in 1:nrow(txt.df))
{
  # split content into a vector of lines and grep
  #
  txt.vc       <- unlist(strsplit(txt.df[[3]][j], "\n", fixed=TRUE))
  code.match   <- grep(pattern=code.pattern, txt.vc, ignore.case=FALSE, value=FALSE)

  if (length(code.match>0))
  { # data frames of grep matches
    #
    DF <- data.frame(CodeID   = code.id,
                     CodeName = code.name,
                     FileID   = as.integer(txt.df[[1]][j]),
                     FileName = as.character(txt.df[[2]][j]),
                     RowNum   = code.match)

    # write matches to csv file
    write.table(DF, file=out.file, append=TRUE, 
                col.names=TRUE, row.names=FALSE, sep=",")
  }
}

# generate list for code (15) InternatComp
code.pattern <- "[Cc]ompetition"
code.id      <- "15"
code.name    <- "InternatComp"
#
for (j in 1:nrow(txt.df))
{
  # split content into a vector of lines and grep
  #
  txt.vc       <- unlist(strsplit(txt.df[[3]][j], "\n", fixed=TRUE))
  code.match   <- grep(pattern=code.pattern, txt.vc, ignore.case=FALSE, value=FALSE)
  
  if (length(code.match>0))
  { # data frames of grep matches
    #
    DF <- data.frame(CodeID   = code.id,
                     CodeName = code.name,
                     FileID   = as.integer(txt.df[[1]][j]),
                     FileName = as.character(txt.df[[2]][j]),
                     RowNum   = code.match)

    # write matches to csv file
    write.table(DF, file=out.file, append=TRUE, 
                col.names=TRUE, row.names=FALSE, sep=",")
  }
}

# generate list for code (16) MathScienceEng
code.pattern <- "[Mm]ath|[Ss]cience|[Ee]ngineering|STEM"
code.id      <- "16"
code.name    <- "MathSciEng"
#
for (j in 1:nrow(txt.df))
{
  # split content into a vector of lines and grep
  #
  txt.vc       <- unlist(strsplit(txt.df[[3]][j], "\n", fixed=TRUE))
  code.match   <- grep(pattern=code.pattern, txt.vc, ignore.case=FALSE, value=FALSE)
  
  if (length(code.match>0))
  { # data frames of grep matches
    #
    DF <- data.frame(CodeID   = code.id,
                     CodeName = code.name,
                     FileID   = as.integer(txt.df[[1]][j]),
                     FileName = as.character(txt.df[[2]][j]),
                     RowNum   = code.match)

    # write matches to csv file
    write.table(DF, file=out.file, append=TRUE, 
                col.names=TRUE, row.names=FALSE, sep=",")
  }
}

# generate list for code (17) NationDefense
code.pattern <- "[Nn]ational [Dd]efense"
code.id      <- "17"
code.name    <- "NationDefense"
#
for (j in 1:nrow(txt.df))
{
  # split content into a vector of lines and grep
  #
  txt.vc       <- unlist(strsplit(txt.df[[3]][j], "\n", fixed=TRUE))
  code.match   <- grep(pattern=code.pattern, txt.vc, ignore.case=FALSE, value=FALSE)
  
  if (length(code.match>0))
  { # data frames of grep matches
    #
    DF <- data.frame(CodeID   = code.id,
                     CodeName = code.name,
                     FileID   = as.integer(txt.df[[1]][j]),
                     FileName = as.character(txt.df[[2]][j]),
                     RowNum   = code.match)

    # write matches to csv file
    write.table(DF, file=out.file, append=TRUE, 
                col.names=TRUE, row.names=FALSE, sep=",")
  }
}

# generate list for code (18) NationInterest
code.pattern <- "[Nn]ational [Ii]nterest"
code.id      <- "18"
code.name    <- "NationInterest"
#
for (j in 1:nrow(txt.df))
{
  # split content into a vector of lines and grep
  #
  txt.vc       <- unlist(strsplit(txt.df[[3]][j], "\n", fixed=TRUE))
  code.match   <- grep(pattern=code.pattern, txt.vc, ignore.case=FALSE, value=FALSE)
  
  if (length(code.match>0))
  { # data frames of grep matches
    #
    DF <- data.frame(CodeID   = code.id,
                     CodeName = code.name,
                     FileID   = as.integer(txt.df[[1]][j]),
                     FileName = as.character(txt.df[[2]][j]),
                     RowNum   = code.match)

    # write matches to csv file
    write.table(DF, file=out.file, append=TRUE, 
                col.names=TRUE, row.names=FALSE, sep=",")
  }
}

# generate list for code (19) Poverty
code.pattern <- "[Pp]overty"
code.id      <- "19"
code.name    <- "NationInterest"
#
for (j in 1:nrow(txt.df))
{
  # split content into a vector of lines and grep
  #
  txt.vc       <- unlist(strsplit(txt.df[[3]][j], "\n", fixed=TRUE))
  code.match   <- grep(pattern=code.pattern, txt.vc, ignore.case=FALSE, value=FALSE)
  
  if (length(code.match>0))
  { # data frames of grep matches
    #
    DF <- data.frame(CodeID   = code.id,
                     CodeName = code.name,
                     FileID   = as.integer(txt.df[[1]][j]),
                     FileName = as.character(txt.df[[2]][j]),
                     RowNum   = code.match)

    # write matches to csv file
    write.table(DF, file=out.file, append=TRUE, 
                col.names=TRUE, row.names=FALSE, sep=",")
  }
}

# generate list for code (20) Reform
code.pattern <- "[Rr]eform"
code.id      <- "20"
code.name    <- "Reform"
#
for (j in 1:nrow(txt.df))
{
  # split content into a vector of lines and grep
  #
  txt.vc       <- unlist(strsplit(txt.df[[3]][j], "\n", fixed=TRUE))
  code.match   <- grep(pattern=code.pattern, txt.vc, ignore.case=FALSE, value=FALSE)
  
  if (length(code.match>0))
  { # data frames of grep matches
    #
    DF <- data.frame(CodeID   = code.id,
                     CodeName = code.name,
                     FileID   = as.integer(txt.df[[1]][j]),
                     FileName = as.character(txt.df[[2]][j]),
                     RowNum   = code.match)

    # write matches to csv file
    write.table(DF, file=out.file, append=TRUE, 
                col.names=TRUE, row.names=FALSE, sep=",")
  }
}

# generate list for code (21) Standards
code.pattern <- "[Ss]tandards"
code.id      <- "21"
code.name    <- "Standards"
#
for (j in 1:nrow(txt.df))
{
  # split content into a vector of lines and grep
  #
  txt.vc       <- unlist(strsplit(txt.df[[3]][j], "\n", fixed=TRUE))
  code.match   <- grep(pattern=code.pattern, txt.vc, ignore.case=FALSE, value=FALSE)
  
  if (length(code.match>0))
  { # data frames of grep matches
    #
    DF <- data.frame(CodeID   = code.id,
                     CodeName = code.name,
                     FileID   = as.integer(txt.df[[1]][j]),
                     FileName = as.character(txt.df[[2]][j]),
                     RowNum   = code.match)

    # write matches to csv file
    write.table(DF, file=out.file, append=TRUE, 
                col.names=TRUE, row.names=FALSE, sep=",")
  }
}

# generate list for code (22) TechnoSociety
code.pattern <- "[Tt]echnological"
code.id      <- "22"
code.name    <- "TechnoSociety"
#
for (j in 1:nrow(txt.df))
{
  # split content into a vector of lines and grep
  #
  txt.vc       <- unlist(strsplit(txt.df[[3]][j], "\n", fixed=TRUE))
  code.match   <- grep(pattern=code.pattern, txt.vc, ignore.case=FALSE, value=FALSE)
  
  if (length(code.match>0))
  { # data frames of grep matches
    #
    DF <- data.frame(CodeID   = code.id,
                     CodeName = code.name,
                     FileID   = as.integer(txt.df[[1]][j]),
                     FileName = as.character(txt.df[[2]][j]),
                     RowNum   = code.match)

    # write matches to csv file
    write.table(DF, file=out.file, append=TRUE, 
                col.names=TRUE, row.names=FALSE, sep=",")
  }
}

# generate list for code (23) WorkCareer
code.pattern <- "[Ww]ork|[Cc]areer|[Ee]mployment"
code.id      <- "23"
code.name    <- "WorkCareer"
#
for (j in 1:nrow(txt.df))
{
  # split content into a vector of lines and grep
  #
  txt.vc       <- unlist(strsplit(txt.df[[3]][j], "\n", fixed=TRUE))
  code.match   <- grep(pattern=code.pattern, txt.vc, ignore.case=FALSE, value=FALSE)
  
  if (length(code.match>0))
  { # data frames of grep matches
    #
    DF <- data.frame(CodeID   = code.id,
                     CodeName = code.name,
                     FileID   = as.integer(txt.df[[1]][j]),
                     FileName = as.character(txt.df[[2]][j]),
                     RowNum   = code.match)

    # write matches to csv file
    write.table(DF, file=out.file, append=TRUE, 
                col.names=TRUE, row.names=FALSE, sep=",")
  }
}
#+end_src

** STRT apply codes to lines

Start RQDA, open the project and run the script

#+srcname: test-pattern
#+begin_src R
# define variables, set working directory
#
home.dir     <- "~/Dropbox/dissertation/data"
setwd(home.dir)

database     <- "congr-hearings.rqda"
sql.select   <- "select id, name, file from source where status==1"
sql.clean    <- "delete from coding"
user         <- Sys.getenv("USERNAME")
today        <- Sys.Date()
drv          <- dbDriver("SQLite")
con          <- dbConnect(drv, dbname=database)

# clean the "coding" table, then
# load text file contents into data frame
#
RQDAQuery(sql.clean)
txt.df        <- RQDAQuery(sql.select)
names(txt.df) <- c("FileID","FileName","FileContent")
str(txt.df)

# load the content of _all_ files into a character vector
#
txt.vc <- vector()

# loop through all files and apply code (1)
#
code.pattern <- "[Aa]ccountability"
code.id      <- as.integer(1)
code.memo    <- "computer generated code 1"

for (j in 1:nrow(txt.df))
{
  # split content into a vector of lines and grep
  #
  txt.vc       <- unlist(strsplit(txt.df[[3]][j], "\n", fixed=TRUE))
  lines.l      <- nchar(txt.vc)
  code.match   <- grep(pattern=code.pattern, txt.vc, ignore.case=FALSE, value=FALSE)

  if (length(code.match)>0)
  {
    # data frames of grep matches
    #
    DF.1 <- data.frame(cid     = code.id,
                       fid     = as.integer(txt.df[[1]][j]),
                       rownum  = code.match)     # matching rows

    # prepare data frame of line lengths and positions
    #
    DF.2 <- cbind(seq(1, length(txt.vc)), lines.l, NA, NA)
    colnames(DF.2) <- c("rownum","length","from","to")

    # populate data frame, adjusted for "\n"
    #
    DF.2[1,3] <- 0
    DF.2[1,4] <- DF.2[1,2]
    for (i in 2:length(txt.vc))
    {
      DF.2[i,3] <- DF.2[i-1,4] + 1
      DF.2[i,4] <- DF.2[i,3] + DF.2[i,2]
    }
    str(DF.1)
    str(DF.2)

    # merge the 2 data frames
    #
    DF.3 <- merge(DF.1, DF.2, by.x="rownum", by.y="rownum")
    str(DF.3)

    # find entries of "coding" table
    #
    selfirst <- vector()  # integer
    selend   <- vector()  # integer
    seltext  <- vector()  # character

    for (i in 1:length(code.match))
    {
       seltext[i]  <- txt.vc[code.match[i]]
       selfirst[i] <- as.integer(DF.3[i,5])
       selend[i]   <- as.integer(DF.3[i,6])
    }

    # create coding data frame
    DF.4 <- data.frame(cid      = as.integer(DF.3$cid),
                       fid      = as.integer(DF.3$fid),
                       seltext  = as.character(seltext),
                       selfirst = as.integer(selfirst),
                       selend   = as.integer(selend),
                       status   = as.integer(1),
                       owner    = as.character(user),
                       date     = as.character(today),
                       memo     = code.memo)
    str(DF.4)
    dbWriteTable(con, "coding", DF.4, row.names=FALSE, overwrite=FALSE, append=TRUE)
  }
}

# loop through all files and apply code (2)
#
code.pattern <- "[Aa]chievement"
code.id      <- as.integer(2)
code.memo    <- "computer generated code 2"

for (j in 1:nrow(txt.df))
{
  # split content into a vector of lines and grep
  #
  txt.vc       <- unlist(strsplit(txt.df[[3]][j], "\n", fixed=TRUE))
  lines.l      <- nchar(txt.vc)
  code.match   <- grep(pattern=code.pattern, txt.vc, ignore.case=FALSE, value=FALSE)

  if (length(code.match)>0)
  {
    # data frames of grep matches
    #
    DF.1 <- data.frame(cid     = code.id,
                       fid     = as.integer(txt.df[[1]][j]),
                       rownum  = code.match)     # matching rows

    # prepare data frame of line lengths and positions
    #
    DF.2 <- cbind(seq(1, length(txt.vc)), lines.l, NA, NA)
    colnames(DF.2) <- c("rownum","length","from","to")

    # populate data frame, adjusted for "\n"
    #
    DF.2[1,3] <- 0
    DF.2[1,4] <- DF.2[1,2]
    for (i in 2:length(txt.vc))
    {
      DF.2[i,3] <- DF.2[i-1,4] + 1
      DF.2[i,4] <- DF.2[i,3] + DF.2[i,2]
    }
    str(DF.1)
    str(DF.2)

    # merge the 2 data frames
    #
    DF.3 <- merge(DF.1, DF.2, by.x="rownum", by.y="rownum")
    str(DF.3)

    # find entries of "coding" table
    #
    selfirst <- vector()  # integer
    selend   <- vector()  # integer
    seltext  <- vector()  # character

    for (i in 1:length(code.match))
    {
       seltext[i]  <- txt.vc[code.match[i]]
       selfirst[i] <- as.integer(DF.3[i,5])
       selend[i]   <- as.integer(DF.3[i,6])
    }

    # create coding data frame
    DF.4 <- data.frame(cid      = as.integer(DF.3$cid),
                       fid      = as.integer(DF.3$fid),
                       seltext  = as.character(seltext),
                       selfirst = as.integer(selfirst),
                       selend   = as.integer(selend),
                       status   = as.integer(1),
                       owner    = as.character(user),
                       date     = as.character(today),
                       memo     = code.memo)
    str(DF.4)
    dbWriteTable(con, "coding", DF.4, row.names=FALSE, overwrite=FALSE, append=TRUE)
  }
}

# loop through all files and apply code (3)
#
code.pattern <- "[Aa]ll [Ss]tudents"
code.id      <- as.integer(3)
code.memo    <- "computer generated code 3"

for (j in 1:nrow(txt.df))
{
  # split content into a vector of lines and grep
  #
  txt.vc       <- unlist(strsplit(txt.df[[3]][j], "\n", fixed=TRUE))
  lines.l      <- nchar(txt.vc)
  code.match   <- grep(pattern=code.pattern, txt.vc, ignore.case=FALSE, value=FALSE)

  if (length(code.match)>0)
  {
    # data frames of grep matches
    #
    DF.1 <- data.frame(cid     = code.id,
                       fid     = as.integer(txt.df[[1]][j]),
                       rownum  = code.match)     # matching rows

    # prepare data frame of line lengths and positions
    #
    DF.2 <- cbind(seq(1, length(txt.vc)), lines.l, NA, NA)
    colnames(DF.2) <- c("rownum","length","from","to")

    # populate data frame, adjusted for "\n"
    #
    DF.2[1,3] <- 0
    DF.2[1,4] <- DF.2[1,2]
    for (i in 2:length(txt.vc))
    {
      DF.2[i,3] <- DF.2[i-1,4] + 1
      DF.2[i,4] <- DF.2[i,3] + DF.2[i,2]
    }
    str(DF.1)
    str(DF.2)

    # merge the 2 data frames
    #
    DF.3 <- merge(DF.1, DF.2, by.x="rownum", by.y="rownum")
    str(DF.3)

    # find entries of "coding" table
    #
    selfirst <- vector()  # integer
    selend   <- vector()  # integer
    seltext  <- vector()  # character

    for (i in 1:length(code.match))
    {
       seltext[i]  <- txt.vc[code.match[i]]
       selfirst[i] <- as.integer(DF.3[i,5])
       selend[i]   <- as.integer(DF.3[i,6])
    }

    # create coding data frame
    DF.4 <- data.frame(cid      = as.integer(DF.3$cid),
                       fid      = as.integer(DF.3$fid),
                       seltext  = as.character(seltext),
                       selfirst = as.integer(selfirst),
                       selend   = as.integer(selend),
                       status   = as.integer(1),
                       owner    = as.character(user),
                       date     = as.character(today),
                       memo     = code.memo)
    str(DF.4)
    dbWriteTable(con, "coding", DF.4, row.names=FALSE, overwrite=FALSE, append=TRUE)
  }
}

# loop through all files and apply codes (4) and (5)
#
code.pattern <- "[Aa]ssess"
code.id      <- as.integer(4)
code.memo    <- "computer generated codes 4 and 5"

for (j in 1:nrow(txt.df))
{
  # split content into a vector of lines and grep
  #
  txt.vc       <- unlist(strsplit(txt.df[[3]][j], "\n", fixed=TRUE))
  lines.l      <- nchar(txt.vc)
  code.match   <- grep(pattern=code.pattern, txt.vc, ignore.case=FALSE, value=FALSE)

  if (length(code.match)>0)
  {
    # data frames of grep matches
    #
    DF.1 <- data.frame(cid     = code.id,
                       fid     = as.integer(txt.df[[1]][j]),
                       rownum  = code.match)     # matching rows

    # prepare data frame of line lengths and positions
    #
    DF.2 <- cbind(seq(1, length(txt.vc)), lines.l, NA, NA)
    colnames(DF.2) <- c("rownum","length","from","to")

    # populate data frame, adjusted for "\n"
    #
    DF.2[1,3] <- 0
    DF.2[1,4] <- DF.2[1,2]
    for (i in 2:length(txt.vc))
    {
      DF.2[i,3] <- DF.2[i-1,4] + 1
      DF.2[i,4] <- DF.2[i,3] + DF.2[i,2]
    }
    str(DF.1)
    str(DF.2)

    # merge the 2 data frames
    #
    DF.3 <- merge(DF.1, DF.2, by.x="rownum", by.y="rownum")
    str(DF.3)

    # find entries of "coding" table
    #
    selfirst <- vector()  # integer
    selend   <- vector()  # integer
    seltext  <- vector()  # character

    for (i in 1:length(code.match))
    {
       seltext[i]  <- txt.vc[code.match[i]]
       selfirst[i] <- as.integer(DF.3[i,5])
       selend[i]   <- as.integer(DF.3[i,6])
    }

    # create coding data frame
    DF.4 <- data.frame(cid      = as.integer(DF.3$cid),
                       fid      = as.integer(DF.3$fid),
                       seltext  = as.character(seltext),
                       selfirst = as.integer(selfirst),
                       selend   = as.integer(selend),
                       status   = as.integer(1),
                       owner    = as.character(user),
                       date     = as.character(today),
                       memo     = code.memo)
    str(DF.4)
    dbWriteTable(con, "coding", DF.4, row.names=FALSE, overwrite=FALSE, append=TRUE)
  }
}

# loop through files to apply code (6) BestFirst
#
code.pattern <- "first in the world"
code.id      <- as.integer(6)
code.memo    <- "computer generated code 6"

for (j in 1:nrow(txt.df))
{
  # split content into a vector of lines and grep
  #
  txt.vc       <- unlist(strsplit(txt.df[[3]][j], "\n", fixed=TRUE))
  lines.l      <- nchar(txt.vc)
  code.match   <- grep(pattern=code.pattern, txt.vc, ignore.case=FALSE, value=FALSE)

  if (length(code.match)>0)
  {
    # data frames of grep matches
    #
    DF.1 <- data.frame(cid     = code.id,
                       fid     = as.integer(txt.df[[1]][j]),
                       rownum  = code.match)     # matching rows

    # prepare data frame of line lengths and positions
    #
    DF.2 <- cbind(seq(1, length(txt.vc)), lines.l, NA, NA)
    colnames(DF.2) <- c("rownum","length","from","to")

    # populate data frame, adjusted for "\n"
    #
    DF.2[1,3] <- 0
    DF.2[1,4] <- DF.2[1,2]
    for (i in 2:length(txt.vc))
    {
      DF.2[i,3] <- DF.2[i-1,4] + 1
      DF.2[i,4] <- DF.2[i,3] + DF.2[i,2]
    }
    str(DF.1)
    str(DF.2)

    # merge the 2 data frames
    #
    DF.3 <- merge(DF.1, DF.2, by.x="rownum", by.y="rownum")
    str(DF.3)

    # find entries of "coding" table
    #
    selfirst <- vector()  # integer
    selend   <- vector()  # integer
    seltext  <- vector()  # character

    for (i in 1:length(code.match))
    {
       seltext[i]  <- txt.vc[code.match[i]]
       selfirst[i] <- as.integer(DF.3[i,5])
       selend[i]   <- as.integer(DF.3[i,6])
    }

    # create coding data frame
    DF.4 <- data.frame(cid      = as.integer(DF.3$cid),
                       fid      = as.integer(DF.3$fid),
                       seltext  = as.character(seltext),
                       selfirst = as.integer(selfirst),
                       selend   = as.integer(selend),
                       status   = as.integer(1),
                       owner    = as.character(user),
                       date     = as.character(today),
                       memo     = code.memo)
    str(DF.4)
    dbWriteTable(con, "coding", DF.4, row.names=FALSE, overwrite=FALSE, append=TRUE)
  }
}

# loop through files to apply code (7) Diversity
#
code.pattern <- "[Dd]iversity"
code.id      <- as.integer(7)
code.memo    <- "computer generated code 7"

for (j in 1:nrow(txt.df))
{
  # split content into a vector of lines and grep
  #
  txt.vc       <- unlist(strsplit(txt.df[[3]][j], "\n", fixed=TRUE))
  lines.l      <- nchar(txt.vc)
  code.match   <- grep(pattern=code.pattern, txt.vc, ignore.case=FALSE, value=FALSE)

  if (length(code.match)>0)
  {
    # data frames of grep matches
    #
    DF.1 <- data.frame(cid     = code.id,
                       fid     = as.integer(txt.df[[1]][j]),
                       rownum  = code.match)     # matching rows

    # prepare data frame of line lengths and positions
    #
    DF.2 <- cbind(seq(1, length(txt.vc)), lines.l, NA, NA)
    colnames(DF.2) <- c("rownum","length","from","to")

    # populate data frame, adjusted for "\n"
    #
    DF.2[1,3] <- 0
    DF.2[1,4] <- DF.2[1,2]
    for (i in 2:length(txt.vc))
    {
      DF.2[i,3] <- DF.2[i-1,4] + 1
      DF.2[i,4] <- DF.2[i,3] + DF.2[i,2]
    }

    # merge the 2 data frames
    #
    DF.3 <- merge(DF.1, DF.2, by.x="rownum", by.y="rownum")

    # find entries of "coding" table
    #
    selfirst <- vector()  # integer
    selend   <- vector()  # integer
    seltext  <- vector()  # character

    for (i in 1:length(code.match))
    {
       seltext[i]  <- txt.vc[code.match[i]]
       selfirst[i] <- as.integer(DF.3[i,5])
       selend[i]   <- as.integer(DF.3[i,6])
    }

    # create coding data frame
    DF.4 <- data.frame(cid      = as.integer(DF.3$cid),
                       fid      = as.integer(DF.3$fid),
                       seltext  = as.character(seltext),
                       selfirst = as.integer(selfirst),
                       selend   = as.integer(selend),
                       status   = as.integer(1),
                       owner    = as.character(user),
                       date     = as.character(today),
                       memo     = code.memo)

    dbWriteTable(con, "coding", DF.4, row.names=FALSE, overwrite=FALSE, append=TRUE)
  }
}

# loop through files to apply code (8) Diversity
#
code.pattern <- "[Dd]ropout"
code.id      <- as.integer(8)
code.memo    <- "computer generated code 8"

for (j in 1:nrow(txt.df))
{
  # split content into a vector of lines and grep
  #
  txt.vc       <- unlist(strsplit(txt.df[[3]][j], "\n", fixed=TRUE))
  lines.l      <- nchar(txt.vc)
  code.match   <- grep(pattern=code.pattern, txt.vc, ignore.case=FALSE, value=FALSE)

  if (length(code.match)>0)
  {
    # data frames of grep matches
    #
    DF.1 <- data.frame(cid     = code.id,
                       fid     = as.integer(txt.df[[1]][j]),
                       rownum  = code.match)     # matching rows

    # prepare data frame of line lengths and positions
    #
    DF.2 <- cbind(seq(1, length(txt.vc)), lines.l, NA, NA)
    colnames(DF.2) <- c("rownum","length","from","to")

    # populate data frame, adjusted for "\n"
    #
    DF.2[1,3] <- 0
    DF.2[1,4] <- DF.2[1,2]
    for (i in 2:length(txt.vc))
    {
      DF.2[i,3] <- DF.2[i-1,4] + 1
      DF.2[i,4] <- DF.2[i,3] + DF.2[i,2]
    }

    # merge the 2 data frames
    #
    DF.3 <- merge(DF.1, DF.2, by.x="rownum", by.y="rownum")

    # find entries of "coding" table
    #
    selfirst <- vector()  # integer
    selend   <- vector()  # integer
    seltext  <- vector()  # character

    for (i in 1:length(code.match))
    {
       seltext[i]  <- txt.vc[code.match[i]]
       selfirst[i] <- as.integer(DF.3[i,5])
       selend[i]   <- as.integer(DF.3[i,6])
    }

    # create coding data frame
    DF.4 <- data.frame(cid      = as.integer(DF.3$cid),
                       fid      = as.integer(DF.3$fid),
                       seltext  = as.character(seltext),
                       selfirst = as.integer(selfirst),
                       selend   = as.integer(selend),
                       status   = as.integer(1),
                       owner    = as.character(user),
                       date     = as.character(today),
                       memo     = code.memo)

    dbWriteTable(con, "coding", DF.4, row.names=FALSE, overwrite=FALSE, append=TRUE)
  }
}

# loop through files to apply code (9) Economy
#
code.pattern <- "[Ee]conomy"
code.id      <- as.integer(9)
code.memo    <- "computer generated code 9"

for (j in 1:nrow(txt.df))
{
  # split content into a vector of lines and grep
  #
  txt.vc       <- unlist(strsplit(txt.df[[3]][j], "\n", fixed=TRUE))
  lines.l      <- nchar(txt.vc)
  code.match   <- grep(pattern=code.pattern, txt.vc, ignore.case=FALSE, value=FALSE)

  if (length(code.match)>0)
  {
    # data frames of grep matches
    #
    DF.1 <- data.frame(cid     = code.id,
                       fid     = as.integer(txt.df[[1]][j]),
                       rownum  = code.match)     # matching rows

    # prepare data frame of line lengths and positions
    #
    DF.2 <- cbind(seq(1, length(txt.vc)), lines.l, NA, NA)
    colnames(DF.2) <- c("rownum","length","from","to")

    # populate data frame, adjusted for "\n"
    #
    DF.2[1,3] <- 0
    DF.2[1,4] <- DF.2[1,2]
    for (i in 2:length(txt.vc))
    {
      DF.2[i,3] <- DF.2[i-1,4] + 1
      DF.2[i,4] <- DF.2[i,3] + DF.2[i,2]
    }

    # merge the 2 data frames
    #
    DF.3 <- merge(DF.1, DF.2, by.x="rownum", by.y="rownum")

    # find entries of "coding" table
    #
    selfirst <- vector()  # integer
    selend   <- vector()  # integer
    seltext  <- vector()  # character

    for (i in 1:length(code.match))
    {
       seltext[i]  <- txt.vc[code.match[i]]
       selfirst[i] <- as.integer(DF.3[i,5])
       selend[i]   <- as.integer(DF.3[i,6])
    }

    # create coding data frame
    DF.4 <- data.frame(cid      = as.integer(DF.3$cid),
                       fid      = as.integer(DF.3$fid),
                       seltext  = as.character(seltext),
                       selfirst = as.integer(selfirst),
                       selend   = as.integer(selend),
                       status   = as.integer(1),
                       owner    = as.character(user),
                       date     = as.character(today),
                       memo     = code.memo)

    dbWriteTable(con, "coding", DF.4, row.names=FALSE, overwrite=FALSE, append=TRUE)
  }
}


# loop through files to apply code (10) EducEquity
#
code.pattern <- "[Ee]quity"
code.id      <- as.integer(10)
code.memo    <- "computer generated code 10"

for (j in 1:nrow(txt.df))
{
  # split content into a vector of lines and grep
  #
  txt.vc       <- unlist(strsplit(txt.df[[3]][j], "\n", fixed=TRUE))
  lines.l      <- nchar(txt.vc)
  code.match   <- grep(pattern=code.pattern, txt.vc, ignore.case=FALSE, value=FALSE)

  if (length(code.match)>0)
  {
    # data frames of grep matches
    #
    DF.1 <- data.frame(cid     = code.id,
                       fid     = as.integer(txt.df[[1]][j]),
                       rownum  = code.match)     # matching rows

    # prepare data frame of line lengths and positions
    #
    DF.2 <- cbind(seq(1, length(txt.vc)), lines.l, NA, NA)
    colnames(DF.2) <- c("rownum","length","from","to")

    # populate data frame, adjusted for "\n"
    #
    DF.2[1,3] <- 0
    DF.2[1,4] <- DF.2[1,2]
    for (i in 2:length(txt.vc))
    {
      DF.2[i,3] <- DF.2[i-1,4] + 1
      DF.2[i,4] <- DF.2[i,3] + DF.2[i,2]
    }

    # merge the 2 data frames
    #
    DF.3 <- merge(DF.1, DF.2, by.x="rownum", by.y="rownum")

    # find entries of "coding" table
    #
    selfirst <- vector()  # integer
    selend   <- vector()  # integer
    seltext  <- vector()  # character

    for (i in 1:length(code.match))
    {
       seltext[i]  <- txt.vc[code.match[i]]
       selfirst[i] <- as.integer(DF.3[i,5])
       selend[i]   <- as.integer(DF.3[i,6])
    }

    # create coding data frame
    DF.4 <- data.frame(cid      = as.integer(DF.3$cid),
                       fid      = as.integer(DF.3$fid),
                       seltext  = as.character(seltext),
                       selfirst = as.integer(selfirst),
                       selend   = as.integer(selend),
                       status   = as.integer(1),
                       owner    = as.character(user),
                       date     = as.character(today),
                       memo     = code.memo)

    dbWriteTable(con, "coding", DF.4, row.names=FALSE, overwrite=FALSE, append=TRUE)
  }
}

# loop through files to apply code (11) Funding
#
code.pattern <- "[Ff]unding"
code.id      <- as.integer(11)
code.memo    <- "computer generated code 11"

for (j in 1:nrow(txt.df))
{
  # split content into a vector of lines and grep
  #
  txt.vc       <- unlist(strsplit(txt.df[[3]][j], "\n", fixed=TRUE))
  lines.l      <- nchar(txt.vc)
  code.match   <- grep(pattern=code.pattern, txt.vc, ignore.case=FALSE, value=FALSE)

  if (length(code.match)>0)
  {
    # data frames of grep matches
    #
    DF.1 <- data.frame(cid     = code.id,
                       fid     = as.integer(txt.df[[1]][j]),
                       rownum  = code.match)     # matching rows

    # prepare data frame of line lengths and positions
    #
    DF.2 <- cbind(seq(1, length(txt.vc)), lines.l, NA, NA)
    colnames(DF.2) <- c("rownum","length","from","to")

    # populate data frame, adjusted for "\n"
    #
    DF.2[1,3] <- 0
    DF.2[1,4] <- DF.2[1,2]
    for (i in 2:length(txt.vc))
    {
      DF.2[i,3] <- DF.2[i-1,4] + 1
      DF.2[i,4] <- DF.2[i,3] + DF.2[i,2]
    }

    # merge the 2 data frames
    #
    DF.3 <- merge(DF.1, DF.2, by.x="rownum", by.y="rownum")

    # find entries of "coding" table
    #
    selfirst <- vector()  # integer
    selend   <- vector()  # integer
    seltext  <- vector()  # character

    for (i in 1:length(code.match))
    {
       seltext[i]  <- txt.vc[code.match[i]]
       selfirst[i] <- as.integer(DF.3[i,5])
       selend[i]   <- as.integer(DF.3[i,6])
    }

    # create coding data frame
    DF.4 <- data.frame(cid      = as.integer(DF.3$cid),
                       fid      = as.integer(DF.3$fid),
                       seltext  = as.character(seltext),
                       selfirst = as.integer(selfirst),
                       selend   = as.integer(selend),
                       status   = as.integer(1),
                       owner    = as.character(user),
                       date     = as.character(today),
                       memo     = code.memo)

    dbWriteTable(con, "coding", DF.4, row.names=FALSE, overwrite=FALSE, append=TRUE)
  }
}

# loop through files to apply code (12) Gap
#
code.pattern <- "gap"
code.id      <- as.integer(12)
code.memo    <- "computer generated code 12"

for (j in 1:nrow(txt.df))
{
  # split content into a vector of lines and grep
  #
  txt.vc       <- unlist(strsplit(txt.df[[3]][j], "\n", fixed=TRUE))
  lines.l      <- nchar(txt.vc)
  code.match   <- grep(pattern=code.pattern, txt.vc, ignore.case=FALSE, value=FALSE)

  if (length(code.match)>0)
  {
    # data frames of grep matches
    #
    DF.1 <- data.frame(cid     = code.id,
                       fid     = as.integer(txt.df[[1]][j]),
                       rownum  = code.match)     # matching rows

    # prepare data frame of line lengths and positions
    #
    DF.2 <- cbind(seq(1, length(txt.vc)), lines.l, NA, NA)
    colnames(DF.2) <- c("rownum","length","from","to")

    # populate data frame, adjusted for "\n"
    #
    DF.2[1,3] <- 0
    DF.2[1,4] <- DF.2[1,2]
    for (i in 2:length(txt.vc))
    {
      DF.2[i,3] <- DF.2[i-1,4] + 1
      DF.2[i,4] <- DF.2[i,3] + DF.2[i,2]
    }

    # merge the 2 data frames
    #
    DF.3 <- merge(DF.1, DF.2, by.x="rownum", by.y="rownum")

    # find entries of "coding" table
    #
    selfirst <- vector()  # integer
    selend   <- vector()  # integer
    seltext  <- vector()  # character

    for (i in 1:length(code.match))
    {
       seltext[i]  <- txt.vc[code.match[i]]
       selfirst[i] <- as.integer(DF.3[i,5])
       selend[i]   <- as.integer(DF.3[i,6])
    }

    # create coding data frame
    DF.4 <- data.frame(cid      = as.integer(DF.3$cid),
                       fid      = as.integer(DF.3$fid),
                       seltext  = as.character(seltext),
                       selfirst = as.integer(selfirst),
                       selend   = as.integer(selend),
                       status   = as.integer(1),
                       owner    = as.character(user),
                       date     = as.character(today),
                       memo     = code.memo)

    dbWriteTable(con, "coding", DF.4, row.names=FALSE, overwrite=FALSE, append=TRUE)
  }
}

# loop through files to apply code (13) Globalization
#
code.pattern <- "[Gg]lobalization"
code.id      <- as.integer(13)
code.memo    <- "computer generated code 13"

for (j in 1:nrow(txt.df))
{
  # split content into a vector of lines and grep
  #
  txt.vc       <- unlist(strsplit(txt.df[[3]][j], "\n", fixed=TRUE))
  lines.l      <- nchar(txt.vc)
  code.match   <- grep(pattern=code.pattern, txt.vc, ignore.case=FALSE, value=FALSE)

  if (length(code.match)>0)
  {
    # data frames of grep matches
    #
    DF.1 <- data.frame(cid     = code.id,
                       fid     = as.integer(txt.df[[1]][j]),
                       rownum  = code.match)     # matching rows

    # prepare data frame of line lengths and positions
    #
    DF.2 <- cbind(seq(1, length(txt.vc)), lines.l, NA, NA)
    colnames(DF.2) <- c("rownum","length","from","to")

    # populate data frame, adjusted for "\n"
    #
    DF.2[1,3] <- 0
    DF.2[1,4] <- DF.2[1,2]
    for (i in 2:length(txt.vc))
    {
      DF.2[i,3] <- DF.2[i-1,4] + 1
      DF.2[i,4] <- DF.2[i,3] + DF.2[i,2]
    }

    # merge the 2 data frames
    #
    DF.3 <- merge(DF.1, DF.2, by.x="rownum", by.y="rownum")

    # find entries of "coding" table
    #
    selfirst <- vector()  # integer
    selend   <- vector()  # integer
    seltext  <- vector()  # character

    for (i in 1:length(code.match))
    {
       seltext[i]  <- txt.vc[code.match[i]]
       selfirst[i] <- as.integer(DF.3[i,5])
       selend[i]   <- as.integer(DF.3[i,6])
    }

    # create coding data frame
    DF.4 <- data.frame(cid      = as.integer(DF.3$cid),
                       fid      = as.integer(DF.3$fid),
                       seltext  = as.character(seltext),
                       selfirst = as.integer(selfirst),
                       selend   = as.integer(selend),
                       status   = as.integer(1),
                       owner    = as.character(user),
                       date     = as.character(today),
                       memo     = code.memo)

    dbWriteTable(con, "coding", DF.4, row.names=FALSE, overwrite=FALSE, append=TRUE)
  }
}

# loop through files to apply code (14) HumanBenefit
#
code.pattern <- "[Pp]rosperity"
code.id      <- as.integer(14)
code.memo    <- "computer generated code 14"

for (j in 1:nrow(txt.df))
{
  # split content into a vector of lines and grep
  #
  txt.vc       <- unlist(strsplit(txt.df[[3]][j], "\n", fixed=TRUE))
  lines.l      <- nchar(txt.vc)
  code.match   <- grep(pattern=code.pattern, txt.vc, ignore.case=FALSE, value=FALSE)

  if (length(code.match)>0)
  {
    # data frames of grep matches
    #
    DF.1 <- data.frame(cid     = code.id,
                       fid     = as.integer(txt.df[[1]][j]),
                       rownum  = code.match)     # matching rows

    # prepare data frame of line lengths and positions
    #
    DF.2 <- cbind(seq(1, length(txt.vc)), lines.l, NA, NA)
    colnames(DF.2) <- c("rownum","length","from","to")

    # populate data frame, adjusted for "\n"
    #
    DF.2[1,3] <- 0
    DF.2[1,4] <- DF.2[1,2]
    for (i in 2:length(txt.vc))
    {
      DF.2[i,3] <- DF.2[i-1,4] + 1
      DF.2[i,4] <- DF.2[i,3] + DF.2[i,2]
    }

    # merge the 2 data frames
    #
    DF.3 <- merge(DF.1, DF.2, by.x="rownum", by.y="rownum")

    # find entries of "coding" table
    #
    selfirst <- vector()  # integer
    selend   <- vector()  # integer
    seltext  <- vector()  # character

    for (i in 1:length(code.match))
    {
       seltext[i]  <- txt.vc[code.match[i]]
       selfirst[i] <- as.integer(DF.3[i,5])
       selend[i]   <- as.integer(DF.3[i,6])
    }

    # create coding data frame
    DF.4 <- data.frame(cid      = as.integer(DF.3$cid),
                       fid      = as.integer(DF.3$fid),
                       seltext  = as.character(seltext),
                       selfirst = as.integer(selfirst),
                       selend   = as.integer(selend),
                       status   = as.integer(1),
                       owner    = as.character(user),
                       date     = as.character(today),
                       memo     = code.memo)

    dbWriteTable(con, "coding", DF.4, row.names=FALSE, overwrite=FALSE, append=TRUE)
  }
}

# loop through files to apply code (15) InternatComp
#
code.pattern <- "[Cc]ompetion"
code.id      <- as.integer(15)
code.memo    <- "computer generated code 15"

for (j in 1:nrow(txt.df))
{
  # split content into a vector of lines and grep
  #
  txt.vc       <- unlist(strsplit(txt.df[[3]][j], "\n", fixed=TRUE))
  lines.l      <- nchar(txt.vc)
  code.match   <- grep(pattern=code.pattern, txt.vc, ignore.case=FALSE, value=FALSE)

  if (length(code.match)>0)
  {
    # data frames of grep matches
    #
    DF.1 <- data.frame(cid     = code.id,
                       fid     = as.integer(txt.df[[1]][j]),
                       rownum  = code.match)     # matching rows

    # prepare data frame of line lengths and positions
    #
    DF.2 <- cbind(seq(1, length(txt.vc)), lines.l, NA, NA)
    colnames(DF.2) <- c("rownum","length","from","to")

    # populate data frame, adjusted for "\n"
    #
    DF.2[1,3] <- 0
    DF.2[1,4] <- DF.2[1,2]
    for (i in 2:length(txt.vc))
    {
      DF.2[i,3] <- DF.2[i-1,4] + 1
      DF.2[i,4] <- DF.2[i,3] + DF.2[i,2]
    }

    # merge the 2 data frames
    #
    DF.3 <- merge(DF.1, DF.2, by.x="rownum", by.y="rownum")

    # find entries of "coding" table
    #
    selfirst <- vector()  # integer
    selend   <- vector()  # integer
    seltext  <- vector()  # character

    for (i in 1:length(code.match))
    {
       seltext[i]  <- txt.vc[code.match[i]]
       selfirst[i] <- as.integer(DF.3[i,5])
       selend[i]   <- as.integer(DF.3[i,6])
    }

    # create coding data frame
    DF.4 <- data.frame(cid      = as.integer(DF.3$cid),
                       fid      = as.integer(DF.3$fid),
                       seltext  = as.character(seltext),
                       selfirst = as.integer(selfirst),
                       selend   = as.integer(selend),
                       status   = as.integer(1),
                       owner    = as.character(user),
                       date     = as.character(today),
                       memo     = code.memo)

    dbWriteTable(con, "coding", DF.4, row.names=FALSE, overwrite=FALSE, append=TRUE)
  }
}

# loop through files to apply code (16) MathSciEng
#
code.pattern <- "[Mm]ath|[Ss]cience |[Ee]ngineering |STEM"
code.id      <- as.integer(16)
code.memo    <- "computer generated code 16"

for (j in 1:nrow(txt.df))
{
  # split content into a vector of lines and grep
  #
  txt.vc       <- unlist(strsplit(txt.df[[3]][j], "\n", fixed=TRUE))
  lines.l      <- nchar(txt.vc)
  code.match   <- grep(pattern=code.pattern, txt.vc, ignore.case=FALSE, value=FALSE)

  if (length(code.match)>0)
  {
    # data frames of grep matches
    #
    DF.1 <- data.frame(cid     = code.id,
                       fid     = as.integer(txt.df[[1]][j]),
                       rownum  = code.match)     # matching rows

    # prepare data frame of line lengths and positions
    #
    DF.2 <- cbind(seq(1, length(txt.vc)), lines.l, NA, NA)
    colnames(DF.2) <- c("rownum","length","from","to")

    # populate data frame, adjusted for "\n"
    #
    DF.2[1,3] <- 0
    DF.2[1,4] <- DF.2[1,2]
    for (i in 2:length(txt.vc))
    {
      DF.2[i,3] <- DF.2[i-1,4] + 1
      DF.2[i,4] <- DF.2[i,3] + DF.2[i,2]
    }

    # merge the 2 data frames
    #
    DF.3 <- merge(DF.1, DF.2, by.x="rownum", by.y="rownum")

    # find entries of "coding" table
    #
    selfirst <- vector()  # integer
    selend   <- vector()  # integer
    seltext  <- vector()  # character

    for (i in 1:length(code.match))
    {
       seltext[i]  <- txt.vc[code.match[i]]
       selfirst[i] <- as.integer(DF.3[i,5])
       selend[i]   <- as.integer(DF.3[i,6])
    }

    # create coding data frame
    DF.4 <- data.frame(cid      = as.integer(DF.3$cid),
                       fid      = as.integer(DF.3$fid),
                       seltext  = as.character(seltext),
                       selfirst = as.integer(selfirst),
                       selend   = as.integer(selend),
                       status   = as.integer(1),
                       owner    = as.character(user),
                       date     = as.character(today),
                       memo     = code.memo)

    dbWriteTable(con, "coding", DF.4, row.names=FALSE, overwrite=FALSE, append=TRUE)
  }
}

# loop through files to apply code (17) NationDefense
#
code.pattern <- "[Nn]ational [Dd]efense"
code.id      <- as.integer(17)
code.memo    <- "computer generated code 17"

for (j in 1:nrow(txt.df))
{
  # split content into a vector of lines and grep
  #
  txt.vc       <- unlist(strsplit(txt.df[[3]][j], "\n", fixed=TRUE))
  lines.l      <- nchar(txt.vc)
  code.match   <- grep(pattern=code.pattern, txt.vc, ignore.case=FALSE, value=FALSE)

  if (length(code.match)>0)
  {
    # data frames of grep matches
    #
    DF.1 <- data.frame(cid     = code.id,
                       fid     = as.integer(txt.df[[1]][j]),
                       rownum  = code.match)     # matching rows

    # prepare data frame of line lengths and positions
    #
    DF.2 <- cbind(seq(1, length(txt.vc)), lines.l, NA, NA)
    colnames(DF.2) <- c("rownum","length","from","to")

    # populate data frame, adjusted for "\n"
    #
    DF.2[1,3] <- 0
    DF.2[1,4] <- DF.2[1,2]
    for (i in 2:length(txt.vc))
    {
      DF.2[i,3] <- DF.2[i-1,4] + 1
      DF.2[i,4] <- DF.2[i,3] + DF.2[i,2]
    }

    # merge the 2 data frames
    #
    DF.3 <- merge(DF.1, DF.2, by.x="rownum", by.y="rownum")

    # find entries of "coding" table
    #
    selfirst <- vector()  # integer
    selend   <- vector()  # integer
    seltext  <- vector()  # character

    for (i in 1:length(code.match))
    {
       seltext[i]  <- txt.vc[code.match[i]]
       selfirst[i] <- as.integer(DF.3[i,5])
       selend[i]   <- as.integer(DF.3[i,6])
    }

    # create coding data frame
    DF.4 <- data.frame(cid      = as.integer(DF.3$cid),
                       fid      = as.integer(DF.3$fid),
                       seltext  = as.character(seltext),
                       selfirst = as.integer(selfirst),
                       selend   = as.integer(selend),
                       status   = as.integer(1),
                       owner    = as.character(user),
                       date     = as.character(today),
                       memo     = code.memo)

    dbWriteTable(con, "coding", DF.4, row.names=FALSE, overwrite=FALSE, append=TRUE)
  }
}

# loop through files to apply code (18) NationInterest
#
code.pattern <- "[Nn]ational [Ii]nterest"
code.id      <- as.integer(18)
code.memo    <- "computer generated code 18"

for (j in 1:nrow(txt.df))
{
  # split content into a vector of lines and grep
  #
  txt.vc       <- unlist(strsplit(txt.df[[3]][j], "\n", fixed=TRUE))
  lines.l      <- nchar(txt.vc)
  code.match   <- grep(pattern=code.pattern, txt.vc, ignore.case=FALSE, value=FALSE)

  if (length(code.match)>0)
  {
    # data frames of grep matches
    #
    DF.1 <- data.frame(cid     = code.id,
                       fid     = as.integer(txt.df[[1]][j]),
                       rownum  = code.match)     # matching rows

    # prepare data frame of line lengths and positions
    #
    DF.2 <- cbind(seq(1, length(txt.vc)), lines.l, NA, NA)
    colnames(DF.2) <- c("rownum","length","from","to")

    # populate data frame, adjusted for "\n"
    #
    DF.2[1,3] <- 0
    DF.2[1,4] <- DF.2[1,2]
    for (i in 2:length(txt.vc))
    {
      DF.2[i,3] <- DF.2[i-1,4] + 1
      DF.2[i,4] <- DF.2[i,3] + DF.2[i,2]
    }

    # merge the 2 data frames
    #
    DF.3 <- merge(DF.1, DF.2, by.x="rownum", by.y="rownum")

    # find entries of "coding" table
    #
    selfirst <- vector()  # integer
    selend   <- vector()  # integer
    seltext  <- vector()  # character

    for (i in 1:length(code.match))
    {
       seltext[i]  <- txt.vc[code.match[i]]
       selfirst[i] <- as.integer(DF.3[i,5])
       selend[i]   <- as.integer(DF.3[i,6])
    }

    # create coding data frame
    DF.4 <- data.frame(cid      = as.integer(DF.3$cid),
                       fid      = as.integer(DF.3$fid),
                       seltext  = as.character(seltext),
                       selfirst = as.integer(selfirst),
                       selend   = as.integer(selend),
                       status   = as.integer(1),
                       owner    = as.character(user),
                       date     = as.character(today),
                       memo     = code.memo)

    dbWriteTable(con, "coding", DF.4, row.names=FALSE, overwrite=FALSE, append=TRUE)
  }
}

# loop through files to apply code (19) Poverty
#
code.pattern <- "[Pp]overty"
code.id      <- as.integer(19)
code.memo    <- "computer generated code 19"

for (j in 1:nrow(txt.df))
{
  # split content into a vector of lines and grep
  #
  txt.vc       <- unlist(strsplit(txt.df[[3]][j], "\n", fixed=TRUE))
  lines.l      <- nchar(txt.vc)
  code.match   <- grep(pattern=code.pattern, txt.vc, ignore.case=FALSE, value=FALSE)

  if (length(code.match)>0)
  {
    # data frames of grep matches
    #
    DF.1 <- data.frame(cid     = code.id,
                       fid     = as.integer(txt.df[[1]][j]),
                       rownum  = code.match)     # matching rows

    # prepare data frame of line lengths and positions
    #
    DF.2 <- cbind(seq(1, length(txt.vc)), lines.l, NA, NA)
    colnames(DF.2) <- c("rownum","length","from","to")

    # populate data frame, adjusted for "\n"
    #
    DF.2[1,3] <- 0
    DF.2[1,4] <- DF.2[1,2]
    for (i in 2:length(txt.vc))
    {
      DF.2[i,3] <- DF.2[i-1,4] + 1
      DF.2[i,4] <- DF.2[i,3] + DF.2[i,2]
    }

    # merge the 2 data frames
    #
    DF.3 <- merge(DF.1, DF.2, by.x="rownum", by.y="rownum")

    # find entries of "coding" table
    #
    selfirst <- vector()  # integer
    selend   <- vector()  # integer
    seltext  <- vector()  # character

    for (i in 1:length(code.match))
    {
       seltext[i]  <- txt.vc[code.match[i]]
       selfirst[i] <- as.integer(DF.3[i,5])
       selend[i]   <- as.integer(DF.3[i,6])
    }

    # create coding data frame
    DF.4 <- data.frame(cid      = as.integer(DF.3$cid),
                       fid      = as.integer(DF.3$fid),
                       seltext  = as.character(seltext),
                       selfirst = as.integer(selfirst),
                       selend   = as.integer(selend),
                       status   = as.integer(1),
                       owner    = as.character(user),
                       date     = as.character(today),
                       memo     = code.memo)

    dbWriteTable(con, "coding", DF.4, row.names=FALSE, overwrite=FALSE, append=TRUE)
  }
}

# loop through files to apply code (20) Reform
#
code.pattern <- "[Rr]eform"
code.id      <- as.integer(20)
code.memo    <- "computer generated code 20"

for (j in 1:nrow(txt.df))
{
  # split content into a vector of lines and grep
  #
  txt.vc       <- unlist(strsplit(txt.df[[3]][j], "\n", fixed=TRUE))
  lines.l      <- nchar(txt.vc)
  code.match   <- grep(pattern=code.pattern, txt.vc, ignore.case=FALSE, value=FALSE)

  if (length(code.match)>0)
  {
    # data frames of grep matches
    #
    DF.1 <- data.frame(cid     = code.id,
                       fid     = as.integer(txt.df[[1]][j]),
                       rownum  = code.match)     # matching rows

    # prepare data frame of line lengths and positions
    #
    DF.2 <- cbind(seq(1, length(txt.vc)), lines.l, NA, NA)
    colnames(DF.2) <- c("rownum","length","from","to")

    # populate data frame, adjusted for "\n"
    #
    DF.2[1,3] <- 0
    DF.2[1,4] <- DF.2[1,2]
    for (i in 2:length(txt.vc))
    {
      DF.2[i,3] <- DF.2[i-1,4] + 1
      DF.2[i,4] <- DF.2[i,3] + DF.2[i,2]
    }

    # merge the 2 data frames
    #
    DF.3 <- merge(DF.1, DF.2, by.x="rownum", by.y="rownum")

    # find entries of "coding" table
    #
    selfirst <- vector()  # integer
    selend   <- vector()  # integer
    seltext  <- vector()  # character

    for (i in 1:length(code.match))
    {
       seltext[i]  <- txt.vc[code.match[i]]
       selfirst[i] <- as.integer(DF.3[i,5])
       selend[i]   <- as.integer(DF.3[i,6])
    }

    # create coding data frame
    DF.4 <- data.frame(cid      = as.integer(DF.3$cid),
                       fid      = as.integer(DF.3$fid),
                       seltext  = as.character(seltext),
                       selfirst = as.integer(selfirst),
                       selend   = as.integer(selend),
                       status   = as.integer(1),
                       owner    = as.character(user),
                       date     = as.character(today),
                       memo     = code.memo)

    dbWriteTable(con, "coding", DF.4, row.names=FALSE, overwrite=FALSE, append=TRUE)
  }
}

# loop through files to apply code (21) Standards
#
code.pattern <- "[Ss]tandards"
code.id      <- as.integer(21)
code.memo    <- "computer generated code 21"

for (j in 1:nrow(txt.df))
{
  # split content into a vector of lines and grep
  #
  txt.vc       <- unlist(strsplit(txt.df[[3]][j], "\n", fixed=TRUE))
  lines.l      <- nchar(txt.vc)
  code.match   <- grep(pattern=code.pattern, txt.vc, ignore.case=FALSE, value=FALSE)

  if (length(code.match)>0)
  {
    # data frames of grep matches
    #
    DF.1 <- data.frame(cid     = code.id,
                       fid     = as.integer(txt.df[[1]][j]),
                       rownum  = code.match)     # matching rows

    # prepare data frame of line lengths and positions
    #
    DF.2 <- cbind(seq(1, length(txt.vc)), lines.l, NA, NA)
    colnames(DF.2) <- c("rownum","length","from","to")

    # populate data frame, adjusted for "\n"
    #
    DF.2[1,3] <- 0
    DF.2[1,4] <- DF.2[1,2]
    for (i in 2:length(txt.vc))
    {
      DF.2[i,3] <- DF.2[i-1,4] + 1
      DF.2[i,4] <- DF.2[i,3] + DF.2[i,2]
    }

    # merge the 2 data frames
    #
    DF.3 <- merge(DF.1, DF.2, by.x="rownum", by.y="rownum")

    # find entries of "coding" table
    #
    selfirst <- vector()  # integer
    selend   <- vector()  # integer
    seltext  <- vector()  # character

    for (i in 1:length(code.match))
    {
       seltext[i]  <- txt.vc[code.match[i]]
       selfirst[i] <- as.integer(DF.3[i,5])
       selend[i]   <- as.integer(DF.3[i,6])
    }

    # create coding data frame
    DF.4 <- data.frame(cid      = as.integer(DF.3$cid),
                       fid      = as.integer(DF.3$fid),
                       seltext  = as.character(seltext),
                       selfirst = as.integer(selfirst),
                       selend   = as.integer(selend),
                       status   = as.integer(1),
                       owner    = as.character(user),
                       date     = as.character(today),
                       memo     = code.memo)

    dbWriteTable(con, "coding", DF.4, row.names=FALSE, overwrite=FALSE, append=TRUE)
  }
}

# loop through files to apply code (22) TechnoSociety
#
code.pattern <- "[Tt]echnological"
code.id      <- as.integer(22)
code.memo    <- "computer generated code 22"

for (j in 1:nrow(txt.df))
{
  # split content into a vector of lines and grep
  #
  txt.vc       <- unlist(strsplit(txt.df[[3]][j], "\n", fixed=TRUE))
  lines.l      <- nchar(txt.vc)
  code.match   <- grep(pattern=code.pattern, txt.vc, ignore.case=FALSE, value=FALSE)

  if (length(code.match)>0)
  {
    # data frames of grep matches
    #
    DF.1 <- data.frame(cid     = code.id,
                       fid     = as.integer(txt.df[[1]][j]),
                       rownum  = code.match)     # matching rows

    # prepare data frame of line lengths and positions
    #
    DF.2 <- cbind(seq(1, length(txt.vc)), lines.l, NA, NA)
    colnames(DF.2) <- c("rownum","length","from","to")

    # populate data frame, adjusted for "\n"
    #
    DF.2[1,3] <- 0
    DF.2[1,4] <- DF.2[1,2]
    for (i in 2:length(txt.vc))
    {
      DF.2[i,3] <- DF.2[i-1,4] + 1
      DF.2[i,4] <- DF.2[i,3] + DF.2[i,2]
    }

    # merge the 2 data frames
    #
    DF.3 <- merge(DF.1, DF.2, by.x="rownum", by.y="rownum")

    # find entries of "coding" table
    #
    selfirst <- vector()  # integer
    selend   <- vector()  # integer
    seltext  <- vector()  # character

    for (i in 1:length(code.match))
    {
       seltext[i]  <- txt.vc[code.match[i]]
       selfirst[i] <- as.integer(DF.3[i,5])
       selend[i]   <- as.integer(DF.3[i,6])
    }

    # create coding data frame
    DF.4 <- data.frame(cid      = as.integer(DF.3$cid),
                       fid      = as.integer(DF.3$fid),
                       seltext  = as.character(seltext),
                       selfirst = as.integer(selfirst),
                       selend   = as.integer(selend),
                       status   = as.integer(1),
                       owner    = as.character(user),
                       date     = as.character(today),
                       memo     = code.memo)

    dbWriteTable(con, "coding", DF.4, row.names=FALSE, overwrite=FALSE, append=TRUE)
  }
}

# loop through files to apply code (23) WorkCareer
#
code.pattern <- "[Ww]ork|[Cc]areer|[Ee]mploy"
code.id      <- as.integer(23)
code.memo    <- "computer generated code 23"

for (j in 1:nrow(txt.df))
{
  # split content into a vector of lines and grep
  #
  txt.vc       <- unlist(strsplit(txt.df[[3]][j], "\n", fixed=TRUE))
  lines.l      <- nchar(txt.vc)
  code.match   <- grep(pattern=code.pattern, txt.vc, ignore.case=FALSE, value=FALSE)

  if (length(code.match)>0)
  {
    # data frames of grep matches
    #
    DF.1 <- data.frame(cid     = code.id,
                       fid     = as.integer(txt.df[[1]][j]),
                       rownum  = code.match)     # matching rows

    # prepare data frame of line lengths and positions
    #
    DF.2 <- cbind(seq(1, length(txt.vc)), lines.l, NA, NA)
    colnames(DF.2) <- c("rownum","length","from","to")

    # populate data frame, adjusted for "\n"
    #
    DF.2[1,3] <- 0
    DF.2[1,4] <- DF.2[1,2]
    for (i in 2:length(txt.vc))
    {
      DF.2[i,3] <- DF.2[i-1,4] + 1
      DF.2[i,4] <- DF.2[i,3] + DF.2[i,2]
    }

    # merge the 2 data frames
    #
    DF.3 <- merge(DF.1, DF.2, by.x="rownum", by.y="rownum")

    # find entries of "coding" table
    #
    selfirst <- vector()  # integer
    selend   <- vector()  # integer
    seltext  <- vector()  # character

    for (i in 1:length(code.match))
    {
       seltext[i]  <- txt.vc[code.match[i]]
       selfirst[i] <- as.integer(DF.3[i,5])
       selend[i]   <- as.integer(DF.3[i,6])
    }

    # create coding data frame
    DF.4 <- data.frame(cid      = as.integer(DF.3$cid),
                       fid      = as.integer(DF.3$fid),
                       seltext  = as.character(seltext),
                       selfirst = as.integer(selfirst),
                       selend   = as.integer(selend),
                       status   = as.integer(1),
                       owner    = as.character(user),
                       date     = as.character(today),
                       memo     = code.memo)

    dbWriteTable(con, "coding", DF.4, row.names=FALSE, overwrite=FALSE, append=TRUE)
  }
}

dbDisconnect(con)
#+end_src

** text mining

#+begin_src R
#
# File:/home/henk/Dropbox/dissertation/data/bill.R
#
# R code to analyze the text of the selected congressional bills
#
# Should be run on Brazos cluster
#
#########################################################
# common code, libraries, working directory, definitions
#########################################################
library('tm')
library('RWeka')
library('Rgraphviz')
library('kernlab')
library('RKEA')
# library('wordnet')
# setDict('/usr/share/wordnet')
# setwd('~/Dropbox/dissertation/data')

mystopwords <- c('vii','viii','thereafter','thereof','subparagraph','subparagraphs','subclause,','subchapter','seq','semicolon','section','sec','pursuant','otherwise','notwithstanding','june','july','iii','paragraph','paragraphs','whichever','unless','subsection','subsections','subsequent','subclause','chapter','due','five','fourth','third','twice','throughout','subpart','ii','xii','xxii','xiii','iv','xix','xxi','xvii','xviii','xvi','title','clause','xxvii','ix','clerk','hr','eh','xiiiii','xiv','th','st','xi','d','g','h','vi','bb','iibb','c','et','october','clause','strike','secretary','secretaries','secretarial','rfs','january','earthquake','hazardous','xv','contents','john','delete','deleted','albert','hagel','striking','inserting','landrieu','amended','chairman','rank','committee')

mydict <- Dictionary(c('achiev','gap','educ','math','fund','global','economi','competit','minor','disadvantag','timss','poverti'))
################################
# create Corpus from plain text
################################
bill.crp <- Corpus(DirSource('~/Dropbox/dissertation/data/Bills-txt'), readerControl = list(reader = readPlain(), language = 'en-US'))

summary(bill.crp)

#############################
# Search for certain strings
#############################

# number of matches
# for (i in 1:length(bill.crp)) {print(c(i,grep('state',bill.crp[[i]], value = F)))}

# matching lines, only first line?
# for (j in 1:length(bill.crp)) {print(c(j,grep('math', bill.crp[[j]], value = T)))}

#########################
# pre-processing of text
#########################
# remove html tag
for (j in 1:length(bill.crp)) bill.crp[[j]] <- gsub("<DELETED>", " ", bill.crp[[j]])
for (j in 1:length(bill.crp)) bill.crp[[j]] <- gsub("</DELETED>", " ", bill.crp[[j]])

# combine first+last names
for (j in 1:length(bill.crp)) bill.crp[[j]] <- gsub("James Madison", "jamesmadison", bill.crp[[j]])
for (j in 1:length(bill.crp)) bill.crp[[j]] <- gsub("Dylan Lee James", "dylanleejames", bill.crp[[j]])

bill.crp <- tm_map(bill.crp, tolower)

# do not combine "literacy-related" and some hyphenated synonyms
for (i in 1:length(bill.crp)) bill.crp[[j]] <- gsub("literacy-related", "literacy related", bill.crp[[j]])
for (i in 1:length(bill.crp)) bill.crp[[j]] <- gsub("high-poverty", "poverty", bill.crp[[j]])
for (i in 1:length(bill.crp)) bill.crp[[j]] <- gsub("low-achieving",  "lowachieving", bill.crp[[j]])
for (i in 1:length(bill.crp)) bill.crp[[j]] <- gsub("low-performing", "lowachieving", bill.crp[[j]])
for (i in 1:length(bill.crp)) bill.crp[[j]] <- gsub("per-pupil", "perpupil", bill.crp[[j]])
for (i in 1:length(bill.crp)) bill.crp[[j]] <- gsub("school-within-a-school", "schoolwithinschool", bill.crp[[j]])
for (i in 1:length(bill.crp)) bill.crp[[j]] <- gsub("schools-within-schools", "schoolwithinschool", bill.crp[[j]])
for (i in 1:length(bill.crp)) bill.crp[[j]] <- gsub("experiment-oriented", "experimentoriented", bill.crp[[j]])
for (i in 1:length(bill.crp)) bill.crp[[j]] <- gsub("object-centered", "objectcentered", bill.crp[[j]])
for (i in 1:length(bill.crp)) bill.crp[[j]] <- gsub("minority-group-", "minority group", bill.crp[[j]])
for (i in 1:length(bill.crp)) bill.crp[[j]] <- gsub("minority-group-segregated", "minoritygroupsegredated", bill.crp[[j]])
for (i in 1:length(bill.crp)) bill.crp[[j]] <- gsub("minority group segregated", "minoritygroupsegredated", bill.crp[[j]])

# some fractions
for (j in 1:length(bill.crp)) bill.crp[[j]] <- gsub("four-fifths", "fourfifth", bill.crp[[j]])
for (j in 1:length(bill.crp)) bill.crp[[j]] <- gsub("one-third", "onethird", bill.crp[[j]])
for (j in 1:length(bill.crp)) bill.crp[[j]] <- gsub("two-thirds", "twothird", bill.crp[[j]])
for (j in 1:length(bill.crp)) bill.crp[[j]] <- gsub("third-party", "thirdparty", bill.crp[[j]])
for (j in 1:length(bill.crp)) bill.crp[[j]] <- gsub("third party", "thirdparty", bill.crp[[j]])

# important acronyms
for (j in 1:length(bill.crp)) bill.crp[[j]] <- gsub("Third International Math and Science Study", "timss", bill.crp[[j]])

bill.crp <- tm_map(bill.crp, tolower)
bill.crp <- tm_map(bill.crp, removeWords, mystopwords)

# remove "--"
for (j in 1:length(bill.crp)) bill.crp[[j]] <- gsub("--", " ", bill.crp[[j]])

# reduce mathematics to math
for (j in 1:length(bill.crp)) bill.crp[[j]] <- gsub("mathematics", "math", bill.crp[[j]])

# "fix" 21st
for (i in 1:length(bill.crp)) bill.crp[[i]] <- gsub("21st", "twentyfirst", bill.crp[[i]])

for (i in 1:length(bill.crp)) bill.crp[[i]] <- gsub("speech writing", "speechwriting", bill.crp[[i]])

# singular "gap" and "child"
for (i in 1:length(bill.crp)) bill.crp[[i]] <- gsub("gaps", "gap", bill.crp[[i]])
for (i in 1:length(bill.crp)) bill.crp[[i]] <- gsub("children", "child", bill.crp[[i]])

# 'subgrant' and 'grant' are very frequent
for (i in 1:length(bill.crp)) bill.crp[[i]] <- gsub("subgrant", "fund", bill.crp[[i]])

for (i in 1:length(bill.crp)) bill.crp[[i]] <- gsub("grant", "fund", bill.crp[[i]])

# "are met" to "meet"
for (i in 1:length(bill.crp)) bill.crp[[i]] <- gsub("are met", "meet", bill.crp[[i]])

# combine into 1 word
for (i in 1:length(bill.crp)) bill.crp[[i]] <- gsub("-", "", bill.crp[[i]])

bill.crp <- tm_map(bill.crp, removeWords, stopwords(language = 'english'))
bill.crp <- tm_map(bill.crp, removeWords, mystopwords)
bill.crp <- tm_map(bill.crp, removeNumbers)
bill.crp <- tm_map(bill.crp, removePunctuation)
bill.crp <- tm_map(bill.crp, stripWhitespace)

# save to disk before stemming
save(bill.crp, file = '~/Dropbox/dissertation/data/bill.Rdata')

# change to singular and other stemmings, then save to disk
bill.stem.crp <- tm_map(bill.crp, stemDocument) # takes a long time
save(bill.stem.crp, file = '~/Dropbox/dissertation/data/bill.stem.Rdata')

# rinse & repeat
bill.crp <- tm_map(bill.crp, removeWords, mystopwords)
bill.crp <- tm_map(bill.crp, stripWhitespace)

# use load('<object-name>.Rdata') to retrieve from brazos.tamu.edu

#######################################
# convert text to document-term matrix
#######################################
bill.dtm <- DocumentTermMatrix(bill.stem.crp)
ncol(bill.dtm); nrow(bill.dtm)      # 6118 x 52
dim(bill.dtm)                       # 52 6118

# not sure need to do following:
#
# bill.dtm <- removeSparseTerms(bill.dtm, 0.4)
# ncol(bill.dtm); nrow(bill.dtm) # 837 x 52,
# dim(bill.dtm)
# increasing stringency does not reduce cols

save(bill.dtm, file = '~/Dropbox/dissertation/data/bill.dtm.Rdata')

##################################
# find frequencies & associations
##################################
#
# "global" appears 69 times in bill.stem.crp
# removed with "removeSparseTerms()"

bill.freq          <- findFreqTerms(bill.dtm, lowfreq=2000) 
length(bill.freq)  # 166 terms
bill.freq

bill.assoc.achiev  <- findAssocs(bill.dtm, 'achiev', 0.88)
term.achiev <- names(bill.assoc.achiev)
length(term.achiev)   # 67
bill.assoc.achiev

# some funny business here
#
bill.assoc.gap     <- findAssocs(bill.dtm, 'gap',    0.669)
term.gap <- names(bill.assoc.gap)
length(term.gap)
bill.assoc.gap

bill.assoc.fund    <- findAssocs(bill.dtm, 'fund',   0.97)
term.fund <- names(bill.assoc.fund)
length(term.fund)
bill.assoc.fund

bill.assoc.educ    <- findAssocs(bill.dtm, 'educ',   0.94)
term.educ <- names(bill.assoc.educ)
length(term.educ)
bill.assoc.educ

bill.assoc.math    <- findAssocs(bill.dtm, 'math',   0.55)
term.math <- names(bill.assoc.math)
length(term.math)
bill.assoc.math

##################################
# create DTM of terms of interest
##################################
bill.dic.dtm <- DocumentTermMatrix(bill.stem.crp, list(dictionary = mydict))
inspect(bill.dic.dtm)

###########################################################
# hierarchical clustering (Ward's minimum variance method) 
###########################################################
bill.t.dtm <- t(bill.dtm)    # need for plotting

# Store the relevant terms in a variable
relevant.terms <- findFreqTerms(bill.dtm, lowfreq=2500) # 136 terms
relevant.terms

# project out all other terms from the matrix
# by the subset operator [.
bill.rel <- bill.t.dtm[relevant.terms, ]
bill.cl <- hclust(dist(bill.rel), method = 'ward') # 'simple' crashes R

pdf('bill_cl.pdf', height=10, width=22)
plclust(bill.cl, unit=T, main="Congress Bills Dendrogram", xlab="")
dev.off()

# do the same for term.achiev, term.educ, term.math, term.gap, term.fund
# try with economy and global, national interest.

###################
# correlation plots
###################
plot(bill.dtm, terms = mydict, corThreshold = 0.3)

pdf('bill_myterms.pdf',height=12,width=12)
plot(bill.dtm, terms = mydict, corThreshold = 0.3)
dev.off()

# not very interesting
few.terms <- findFreqTerms(bill.dtm, lowfreq = 9000)
plot(bill.dtm, terms = few.terms, corThreshold = 0.97)

# try with other terms.

###############################
# Principal component analysis
###############################
# require(stats)
# prcomp(top2, scale = T)
# summary(prcomp(top2, scale = T))

# Then examine top components
# .for (i in 1:15) {
#   top4[[i]] <- sort(survey.prcomp$rotation[,i], 
#   decreasing = T)[1:4]}
# top4. 

#############################################
# From "tm" FAQ
############################################
# Can I use bigrams instead of single tokens in a term-document matrix?
# Yes. RWeka provides a tokenizer for arbitrary n-grams which can be directly
# passed on to the term-document matrix constructor. E.g.:

BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
tdm <- TermDocumentMatrix(bill.stem.crp, control = list(tokenize = BigramTokenizer))
inspect(tdm[340:345,1:10])

###########################################
# Ngrams of interest - use pre-stem corpus
###########################################

# libary kernlab?

summary(bill.crp)

# technologically oriented and highly competitive society
# (increasingly) global economy
# competitive economy
# technologically oriented society
# accountable (boost/improve) academic achievement
# increase achievement
# (improving) student achievement
# low-achieving
# (narrow|closing|close) (this|the) achievement gap(s) - pre-process to singular "gap"
# all students = all pupils = all children = all youth
# minority students
# national interest
# no child is left behind
# accountability
# resources of the federal government
# disadvantaged children
# twentyfirst century - originally "21st century"
# internationally competitive content standards
# meet challenging (state) content (standards)
#      challenging (state) performance standards
# national education goals
# educational achievement
# significantly underrepresented
# educational handicap(s)
# (improve) educational opportunity
# development supplemental educational programs
# supplement programs
# educational needs
# achievement gap (remains / persists)
# poverty + schools / students
# at-risk categories
# federal education assistance
# low-performing schools
# scientifically based research
# educational strategies
# high standards
# state standards
# failing to meet state standards
# fully qualified (staff)
# math and science
# partnership with business and industry
# marketable vocational and technical skills
#
#+end_src

** From RQDA to tm

#+srcname: pres-docs-rqda2tm-old
#+begin_src R
fcat <- RQDAQuery("select treefile.fid, filecat.name from treefile,
                  filecat on filecat.catid==treefile.catid 
                  and treefile.status==1
                  and filecat.status==1")
Encoding(fcat$name) <- "UTF-8"
names(fcat) <- c("id","filecat")
txt <- RQDAQuery("select name, id, file, owner, date from source where status==1")
txt <- merge(txt, fcat, by="id", all.x=TRUE, all.y=FALSE)
Encoding(txt$file) <- "UTF-8"
Encoding(txt$name) <- "UTF-8"
pres.docs.crp <- tm::Corpus(tm::VectorSource(txt$file), 
  readerControl = list(language = "eng"))
tm::meta(pres.docs.crp, tag = c("fname","id","owner","date","filecat")) <- 
  txt[,c("name","id","owner","date","filecat")]
summary(pres.docs.crp)
save(pres.docs.crp, file = "~/Dropbox/dissertation/data/pres.docs.Rdata")
#+end_src

** from RQDA to tm + file attributes

#+srcname: pres-docs-rqda2tm-test
#+begin_src R
library("RQDAtm")

rm(list = ls())

project   <- "pres-docs"
language  <- "en-US"
retrieval <- NULL

sql <- "select coding.fid, source.name, coding.seltext
        from coding, source where coding.status=1 and coding.fid=source.id 
        group by coding.fid"

retrieval <- RQDAQuery(sql)
retrieval <- retrieval[order(retrieval$fid),]
dim(retrieval)
str(retrieval)

Encoding(retrieval$seltext) <- "UTF-8"
Encoding(retrieval$name)    <- "UTF-8"

corpus <- tm:::Corpus(tm::VectorSource(retrieval$seltext), readerControl = list(language = language))
meta(corpus, tag = c("fid","name")) <- retrieval[,c("fid","name")]
#+end_src

#+srcname: pres-docs-cast
#+begin_src R :tangle ~/Dropbox/dissertation/data/pres-docs-cast.R

# load libraries
#
library("RSQLite")
library("reshape")
library("tm")

# define variables, working directory, database connection
#
home.dir <- "~/Dropbox/dissertation/data"
out.file <- paste(home.dir, "/pres-docs.Rdata", sep = "")
setwd(home.dir)
drv <- dbDriver("SQLite")
con <- dbConnect(drv, dbname="pres-docs.rqda")

# read file attributes from "fileAttr"
#
temp <- dbReadTable(con, "fileAttr")

# build attributes data frame
#
DF.1 <- data.frame(fileID   = as.integer(temp$fileID),
                   variable = temp$variable, 
                   value    = temp$value, 
                   stringsAsFactors = FALSE)

attr.df <- cast(DF.1, fileID ~ variable)
str(attr.df)
num.row <- length(attr.df$fileID)

# read file name and content from "source"
#
sql <- "select id, name, file from source where status==1"
txt <- RQDAQuery(sql)
str(txt)

Encoding(txt$name) <- "UTF-8"
Encoding(txt$file) <- "UTF-8"

DF.2 <- data.frame(matrix(nrow=num.row, ncol=10), stringsAsFactors=FALSE)
names(DF.2) <- c("id",
                 "name",
                 "content",
                 "doc_id",
                 "doc_date",
                 "author",
                 "title",
                 "doc_type",
                 "author_cat",
                 "institution")

DF.2$id          <- txt$id
DF.2$name        <- txt$name
DF.2$content     <- txt$file
DF.2$doc_id      <- attr.df$doc_id
DF.2$doc_date    <- attr.df$doc_date
DF.2$author      <- attr.df$author
DF.2$title       <- attr.df$title
DF.2$doc_type    <- attr.df$doc_type
DF.2$author_cat  <- attr.df$author_cat
DF.2$institution <- attr.df$institution

str(DF.2)

# create Corpus
#
pres.docs.crp <- Corpus(VectorSource(DF.2$content), readerControl = list(language = "eng"))

# add indexed meta data
#
meta(pres.docs.crp, tag = c("id","name","doc_id","doc_date",
                            "author","title","doc_type",
                            "author_cat","institution")) <- 
  DF.2[,c("id","name","doc_id","doc_date",
          "author","title","doc_type","author_cat","institution")]

summary(pres.docs.crp)
save(pres.docs.crp, file = out.file)

#####################################################
# retrieve indexed meta data from specific document,
# or from all documents in the corpus
#####################################################
meta(pres.docs.crp[1], tag = "author", type = "indexed")
meta(pres.docs.crp,    tag = "title",  type = "indexed")

dbDisconnect(con)
#+end_src

** code relations

The intersectDiagram function in the plotrix package displays the intersections of sets as rectangles with widths (and areas) proportional to the number of members of each set intersection. This may be a way for you to represent your codes. For your example, you could proceed like this. Create a file ("hp.csv")containing the following:

paragraph,attribute
p1,code1
p1,code3
p2,code1
p2,code3
p3,code1
p3,code4
p4,code2
p5,code2
p6,code2
p7,code2
p7,code3
p8,code3
p9,code3
p10,code4
p11,code4
p12,code4

then:

library(plotrix)
hp<-read.csv("hp.csv")
intersectDiagram(hp,main="Combinations of codes")

There are other ways to represent your original data that intersectDiagram will read in that you might like to try.

################################

Another approach would be to redefine the cross-codes table as distances.
For example, if the cross-codes table is a matrix called m ...

# convert to "distances"
d <- 1 - m/diag(m)

# fill in the complete matrix
d[lower.tri(d)] <- d[upper.tri(d)]

# use multidimensional scaling to represent the distances in two dimensions
twodim <- cmdscale(d)
plot(twodim, type="n")
text(twodim, rownames(twodim)) 

** scrub text

#+srcname: congr-docs-scrub
#+begin_src sh
#!/bin/bash
  
# Define variables
HOME_DIR=$HOME/Dropbox/dissertation/data
DATA_DIR=$HOME_DIR/congr-docs
LOG_FILE=$HOME_DIR/congr-docs-scrub.log
  
# (1) display initial number of lines
cd $DATA_DIR
for i in *.txt
do
  echo "initial number of lines:" `wc -l $i` | tee $LOG_FILE
done

# (2) make backup copies of text files
echo "making backup copies of text files ..... " | tee $LOG_FILE
for file in *.txt
do
  cp $file ${file%.txt}.orig
done
echo "finished backup" | tee $LOG_FILE

# (3+4) extract high-content portions and recombine into single file
echo "begin extractions" | tee -a $LOG_FILE
echo "begin extractions on file CDOC-106hdoc68.txt" | tee -a $LOG_FILE

sed -n '25,29p
        47,51p
        74,79p' > temp.txt CDOC-106hdoc68.txt


# sed -n '25,29p' >  temp.txt CDOC-106hdoc68.txt
# sed -n '47,51p' >> temp.txt CDOC-106hdoc68.txt
# sed -n '74,79p' >> temp.txt CDOC-106hdoc68.txt
mv temp.txt CDOC-106hdoc68.txt
echo "finished extraction of file CDOC-106hdoc68.txt" | tee -a $LOG_FILE
  
# remove lines with "TIFF"
# echo "deleting lines with TIFF" | tee -a $LOG_FILE
# for i in *.txt
# do
#  sed '/TIFF/d' $i > $i.tmp
#   mv $i.tmp $i
# done
    
# remove blank lines
# echo "deleting blank lines" | tee -a $LOG_FILE
# for i in *.txt
# do
#  sed '/^$/d' $i > $i.tmp
#   mv $i.tmp $i
# done
  
# delete redundant lines from files
# sed '7,9d' -e '12,13d' filename.txt > filename.tmp
# mv filename.tmp filename.txt

# remove blank lines
# echo "deleting blank lines" | tee -a $LOG_FILE
# for i in *.txt
# do
#   sed '/^$/d' $i > $i.tmp
#   mv $i.tmp $i
# done
  
# delete lines from "Names: " to end of file from DCPD docs
#for i in *.txt
#do
#  sed '/Names: /,$d' $i > $i.tmp
# mv $i.tmp $i
#done
  
# delete lines containing "Government Printing Office" from WCPD docs
#for i in *.txt
#do
#  sed '/Government Printing Office/d' $i > $i.tmp
#mv $i.tmp $i
#done
    
# delete lines containing "R04" from WCPD docs
#for i in *.txt
#do
#  sed '/R04/d' $i > $i.tmp
#mv $i.tmp $i
#done

 
# final number of lines
for i in *.txt
do
  echo "final number of lines:" `wc -l $i` | tee -a $LOG_FILE
done

#+end_src

#+results: congr-docs-scrub
#+begin_example
removing old scrubbed files
extracting text from bills and resolutions
removing leftover html tags
count number of lines in scrubbed text files
1850 BILLS-103hr6rfs.scrb
23 BILLS-106hr2300rfs.scrb
291 BILLS-106hr2390ih.scrb
432 BILLS-106hr2719ih.scrb
971 BILLS-106hr2rfs.scrb
1083 BILLS-106hr4346ih.scrb
2522 BILLS-106s1180is.scrb
1353 BILLS-106s2rs.scrb
102 BILLS-107hr117ih.scrb
1265 BILLS-107hr1990ih.scrb
2617 BILLS-107hr1eas.scrb
1627 BILLS-107hr1enr.scrb
1039 BILLS-107hr340ih.scrb
859 BILLS-107hr345ih.scrb
96 BILLS-107s7is.scrb
64 BILLS-109hr2835ih.scrb
64 BILLS-110hr2204ih.scrb
16 BILLS-110hr2577ih.scrb
229 BILLS-110hr3746ih.scrb
290 BILLS-110s1775is.scrb
88 BILLS-111hr3973ih.scrb
119 BILLS-111hr4122ih.scrb
13 BILLS-111hr4330ih.scrb
140 BILLS-111hr558ih.scrb
69 BILLS-111hr6435ih.scrb
157 BILLS-111hr702ih.scrb
29 BILLS-111hres1133ih.scrb
91 BILLS-112hr2637ih.scrb
#+end_example
** sqlite + bash

#!/bin/bash
 
# Defining my databse first table
STRUCTURE="CREATE TABLE data (id INTEGER PRIMARY KEY,name TEXT,value TEXT);";
 
# Creating an Empty db file and filling it with my structure
cat /dev/null > dbname.db
echo $STRUCTURE > /tmp/tmpstructure
sqlite3 dbname.db < /tmp/tmpstructure;
rm -f /tmp/tmpstructure;
 
# Inserting some data into my structure
sqlite3 dbname.db "INSERT INTO data (name,value) VALUES ('MyName','MyValue')";
sqlite3 dbname.db "INSERT INTO data (name,value) VALUES ('MyOtherName','MyOtherValue')";
 
# Getting my data
LIST=`sqlite3 dbname.db "SELECT * FROM data WHERE 1"`;
 
# For each row
for ROW in $LIST; do
 
    # Parsing data (sqlite3 returns a pipe separated string)
    Id=`echo $ROW | awk '{split($0,a,"|"); print a[1]}'`
    Name=`echo $ROW | awk '{split($0,a,"|"); print a[2]}'`
    Value=`echo $ROW | awk '{split($0,a,"|"); print a[3]}'`
     
    # Printing my data
    echo -e "\e[4m$Id\e[m) "$Name" -> "$Value;
     
done
** Peter R code
*** add files with specific pattern to project

# ------------------------------------------------------------------------------
# Add Files with specific pattern e.g.: "AUT", "NOR", ...; to a project
# ------------------------------------------------------------------------------
  # requires: --
  # option: country = ""
  #           --> no country name is given so that all text files containing
  #               a pattern of 4 digits will be selected;
  #               if country name like 'AUT' , 'NOR', and so on is given
  #               all files containing these sequence of letters are selected
  # option: filetype = "txt"
  #                --> only "*.txt" files will be selected, can be changed
  #                    to any kind of file extension

# ??? ... splitt select and write function into two

rqda.add.files <- function(country="", exclude="", filetype="txt", project="",  ENCODING = "ANSI", REPLACE=F){
              # require(RQDA) # obsolete

# settings for debugging
country <- "aut"; exclude="diff"; project="aut19xxVererbung"; filetype="txt"; ENCODING="ANSI" ; REPLACE=F;

              project.rqda <- paste(project,".rqda",sep="")

              files      <- list.files(path = getwd(),pattern=paste(".",filetype,sep=""))
                            files <- grep(paste(country,"[0-9]{4}",sep=""),
                                          files, value=TRUE,ignore.case=T)
                            if(exclude!=""){
                            files <- grep( exclude , files , value=TRUE,ignore.case=T, invert=T)
                            }
              #files
              ftext <- NULL
              #b
              for(i in 1:length(files)){
                  ftext[[length(ftext)+1]] <- readLines(files[i])
                }

              # deleting and recreating the source table if REPLACE==T
              if(REPLACE==T){
                  sql <- c(  " DROP TABLE source; "
                            ," CREATE TABLE source (name text, id integer,"
                            ,"                      file text, memo text,"
                            ,"                      owner text, date text, dateM text, status integer);"
                          )
                  send.sqlite( project.rqda , sql )
               }
              # number of files already within project
                prev <- send.sqlite( "aut19xxVererbung.rqda" , "SELECT id from source;" )
                if(length(prev)>0) prev <- max(as.integer(prev)) else prev <- 0

              # building the sql statements
              sql <- NULL
                  # over files
              for( i in 1:length(files)){
                  sql[length(sql)+1] <- "INSERT INTO source"
                  sql[length(sql)+1] <- "(name, id, file, memo, owner, date, dateM, status)"
                  sql[length(sql)+1] <- paste("VALUES('",files[i],"' , ",sep="")
                  sql[length(sql)+1] <- paste("'",i+prev,"' , ",sep="")
                  sql[length(sql)+1] <- "'"
                        # over textlines
                    for(k in 1:length(ftext[[i]])){
                        if(k==1){
                         sql[length(sql)] <- paste(sql[length(sql)] , ftext[[i]][1] , sep="" )
                         }
                        else{
                         sql[length(sql)+1] <- ftext[[i]][k]
                         }
                        if( k==length(ftext[[i]]) ) sql[length(sql)] <- paste( sql[length(sql)], "' , ",sep="")
                      }
                  sql[length(sql)+1] <- "null , "
                  sql[length(sql)+1] <- "null , "
                  sql[length(sql)+1] <- paste( "'" , date() , "' , " , sep="" )
                  sql[length(sql)+1] <- "null , "
                  sql[length(sql)+1] <- "'1' ) ; "
                }

            # switch encoding to UTF-8 (format of rqda-projects)
            sql <- iconv(sql, from = "WINDOWS-1252", to = "UTF-8", sub = NA, mark = TRUE)

            # sending to sql
            send.sqlite( project.rqda ,sql,keep=T)

 }
              
*** comparing files line by line and reporting differences

# ------------------------------------------------------------------------------
# Comparing files line by line and reporting differences
# ------------------------------------------------------------------------------
  # A Function comparing files with the help of diff
  # diff and its libraries have to be put within the working directory
      # dowload page (DiffUtils for Windows):
          # http://gnuwin32.sourceforge.net/packages/diffutils.htm
      # a description of the 'normal'-format produced by this shell-querry
          # can be found here:
          # http://www.gnu.org/s/diffutils/manual/#Normal
      # Files have to be in a 1 Byte coding (ANSI, ASCII, latin1, ..., not UTF-8 !!!)

  # requires: diff , files with 1 Byte encoding , R-2.13.x  or higher, Windows
  #
  # option: file1
  #           --> first file to compare (the basis)
  # option: file2
  #           --> second file to compare (the file with changes)
  #
  # option: export = T
  #           --> whether or not the differences are exportet into a textfile
  #
  # option: clean = F
  #           --> not implemented


compare <- function(file1,file2,clean=F,export=T){

                        # config for developement #
#                          setwd("d:\\Peter\\Dropbox\\GOs\\Coding Example")
#                          file1   <- "nor1946.txt"
#                          file2   <- "nor1948.txt"
#                          clean   <- F
#                          export  <- T
                        # config for developement  #

  # check coding of files

    # ?! ...


  # read in files
    text1 <- readLines(file1)
    text2 <- readLines(file2)


  # cleaning files ? : unnecessary blanks e.g.
    if(clean==T){
      # ?! ...  placeholder
      }


  # length of lines (+1 for ther return character at the end of line)
    text1.info <- nchar(text1)
    text2.info <- nchar(text2)


  # length and possition of lines (length, from, to)
    text1.info <- cbind(text1.info,NA,NA)
     colnames(text1.info)<-c("length","from","to")
    text2.info <- cbind(text2.info,NA,NA)
     colnames(text2.info)<-c("length","from","to")


    x <- 0  # for text1
      for(i in 1:length(text1.info[,1])){
        x           <- text1.info[i,1]+x # running total
        text1.info[i,3]  <- x
        if(i==1)text1.info[i,2] <- 0 else text1.info[i,2] <- text1.info[i-1,3]
        }
      for(i in 1:length(text1.info[,1])){
        text1.info[i,2] <- text1.info[i,2]+i-1
        text1.info[i,3] <- text1.info[i,3]+i-1
        }

    x <- 0  # for text2
      for(i in 1:length(text2.info[,1])){
        x           <- text2.info[i,1]+x # running total
        text2.info[i,3]  <- x
        if(i==1)text2.info[i,2] <- 0 else text2.info[i,2] <- text2.info[i-1,3]
      }
      for(i in 1:length(text2.info[,1])){
        text2.info[i,2] <- text2.info[i,2]+i-1
        text2.info[i,3] <- text2.info[i,3]+i-1
        }






    # The actual comparison done by a call to diff
      # make sure to have diff and its libraries in the wd
      # make sure to have R-2.13.x  or higher
      comptext <- paste(" diff", file1, file2)
    compare <- shell("cmd", input=comptext,intern=T)
    compare <- compare[-1:-4]  # Begrüßung weg
    compare <- compare[1:(length(compare)-1)]  # letzte zeile weg


    # extracting the change commands
      no.change.lines <- grep("---|<|>",compare)
      change.commands <- compare[-no.change.lines]
      change.commands <- change.commands[change.commands!=""]


      text1.info <- cbind(text1.info,NA,NA,NA)
      colnames(text1.info) <- c("length","from","to","oldline","newline","change")
      statement <- rep("",length(text1)) # statement is the change command for a certain line



    # get line match
      # change : c=1 , d=2 , a=3

      # from CHANGE from
          # all 00c00 types
      if( length(grep("^[0-9]+c[0-9]+$",change.commands))!=0){
        for(i in grep("^[0-9]+c[0-9]+$",change.commands)){
            ch <- c(as.numeric(strsplit(change.commands[i],"c")[[1]]),1)
            text1.info[ch[1],c("oldline","newline","change")] <- ch
            statement[ch[1]] <- change.commands[i]
            }
        }

          # all 00d00 types
      if( length(grep("^[0-9]+d[0-9]+$",change.commands))!=0){
        for(i in grep("^[0-9]+d[0-9]+$",change.commands)){
            ch <- c(as.numeric(strsplit(change.commands[i],"d")[[1]]),2)
            text1.info[ch[1],c("oldline","newline","change")] <- ch
            statement[ch[1]] <- change.commands[i]
            }
        }

          # all 00a00 types
      if( length(grep("^[0-9]+a[0-9]+$",change.commands)!=0)){
        for(i in grep("^[0-9]+a[0-9]+$",change.commands)){
            ch <- c(as.numeric(strsplit(change.commands[i],"a")[[1]]),3)
            text1.info[ch[1],c("oldline","newline","change")] <- ch
            statement[ch[1]] <- change.commands[i]
            }
        }


     # from,to CHANGE from,to
          # all 00,00c00,00 types
      if( length(grep("^[0-9]+,[0-9]+c[0-9]+,[0-9]+$",change.commands))!=0){ # if some exist
        for(i in grep("^[0-9]+,[0-9]+c[0-9]+,[0-9]+$",change.commands)){     # line numbers of them = i
            ol <- as.numeric(strsplit(strsplit(change.commands[i],"c")[[1]],",")[[1]])
            ol <- seq(from =ol[1], to=ol[2])
            nl <- as.numeric(strsplit(strsplit(change.commands[i],"c")[[1]],",")[[2]])
            nl <- seq(from =nl[1], to=nl[2])
            seqk  <- ol-min(ol)+1
            for(k in seqk){
                ch <- c(ol[k],nl[k],1)
                text1.info[ch[1],c("oldline","newline","change")] <- ch
                statement[ch[1]] <- change.commands[i]
                }
            ol.l <- length(ol)
            nl.l <- length(nl)
            ol   <- max(ol)+1
            nl   <- max(nl)+1
            if( ol <= length(text1.info[,1]) & ol.l < nl.l ){
                ch <- c(ol,nl,0)
                text1.info[ol,c("oldline","newline","change")] <- ch
                }
              }
        }



          # all 00,00d00,00 types
      if( length(grep("^[0-9]+,[0-9]+d[0-9]+,[0-9]+$",change.commands))!=0){ # if some exist
        for(i in grep("^[0-9]+,[0-9]+d[0-9]+,[0-9]+$",change.commands)){     # line numbers of them = i
            ol <- as.numeric(strsplit(strsplit(change.commands[i],"d")[[1]],",")[[1]])
            ol <- seq(from =ol[1], to=ol[2])
            nl <- as.numeric(strsplit(strsplit(change.commands[i],"d")[[1]],",")[[2]])
            nl <- seq(from =nl[1], to=nl[2])
            seqk  <- ol-min(ol)+1
            for(k in seqk){
                ch <- c(ol[k],nl[k],2)
                text1.info[ch[1],c("oldline","newline","change")] <- ch
                statement[ch[1]] <- change.commands[i]
                }
              ol.l <- length(ol)
              nl.l <- length(nl)
              ol <- max(ol)+1
              nl <- max(nl)+1
              if( ol <= length(text1.info[,1]) & ol.l < nl.l){
              ch <- c(ol,nl,0)
              text1.info[ol,c("oldline","newline","change")] <- ch
                  }
            }
        }

          # all 00,00d00,00 types
      if( length(grep("^[0-9]+,[0-9]+a[0-9]+,[0-9]+$",change.commands))!=0){ # if some exist
        for(i in grep("^[0-9]+,[0-9]+a[0-9]+,[0-9]+$",change.commands)){     # line numbers of them = i
            ol <- as.numeric(strsplit(strsplit(change.commands[i],"a")[[1]],",")[[1]])
            ol <- seq(from =ol[1], to=ol[2])
            nl <- as.numeric(strsplit(strsplit(change.commands[i],"a")[[1]],",")[[2]])
            nl <- seq(from =nl[1], to=nl[2])
            seqk  <- ol-min(ol)+1
            for(k in seqk){
                ch <- c(ol[k],nl[k],3)
                text1.info[ch[1],c("oldline","newline","change")] <- ch
                statement[ch[1]] <- change.commands[i]
                }
              ol.l <- length(ol)
              nl.l <- length(nl)
              ol <- max(ol)+1
              nl <- max(nl)+1
              if( ol <= length(text1.info[,1]) & ol.l < nl.l){
              ch <- c(ol,nl,0)
              text1.info[ol,c("oldline","newline","change")] <- ch
                  }
            }
        }



    # from CHANGE from,to
                # all 00c00,00 types
      if( length(grep("^[0-9]+c[0-9]+,[0-9]+$",change.commands))!=0){ # if some exist
        for(i in grep("^[0-9]+c[0-9]+,[0-9]+$",change.commands)){     # line numbers of them = i
            ol <- as.numeric(strsplit(strsplit(change.commands[i],"c")[[1]],",")[[1]])
            ol <- seq(from =ol[1], to=ol[1])
            nl <- as.numeric(strsplit(strsplit(change.commands[i],"c")[[1]],",")[[2]])
            nl <- seq(from =nl[1], to=nl[2])
            seqk  <- ol-min(ol)+1
            for(k in seqk){
                ch <- c(ol[k],nl[k],1)
                text1.info[ch[1],c("oldline","newline","change")] <- ch
                statement[ch[1]] <- change.commands[i]
                }
              ol <- max(ol)+1
              nl <- max(nl)+1
              if(ol<=length(text1.info[,1])){
              ch <- c(ol,nl,0)
              text1.info[ol,c("oldline","newline","change")] <- ch
                  }
            }
        }

                # all 00d00,00 types
      if( length(grep("^[0-9]+d[0-9]+,[0-9]+$",change.commands))!=0){ # if some exist
        for(i in grep("^[0-9]+d[0-9]+,[0-9]+$",change.commands)){     # line numbers of them = i
            ol <- as.numeric(strsplit(strsplit(change.commands[i],"d")[[1]],",")[[1]])
            ol <- seq(from =ol[1], to=ol[1])
            nl <- as.numeric(strsplit(strsplit(change.commands[i],"d")[[1]],",")[[2]])
            nl <- seq(from =nl[1], to=nl[2])
            seqk  <- ol-min(ol)+1
            for(k in seqk){
                ch <- c(ol[k],nl[k],2)
                text1.info[ch[1],c("oldline","newline","change")] <- ch
                statement[ch[1]] <- change.commands[i]
                }
              ol <- max(ol)+1
              nl <- max(nl)+1
              if(ol<=length(text1.info[,1])){
              ch <- c(ol,nl,0)
              text1.info[ol,c("oldline","newline","change")] <- ch
                  }
            }
        }

                # all 00a00,00 types
      if( length(grep("^[0-9]+a[0-9]+,[0-9]+$",change.commands))!=0){ # if some exist
        for(i in grep("^[0-9]+a[0-9]+,[0-9]+$",change.commands)){     # line numbers of them = i
            ol <- as.numeric(strsplit(strsplit(change.commands[i],"a")[[1]],",")[[1]])
            ol <- seq(from =ol[1], to=ol[1])
            nl <- as.numeric(strsplit(strsplit(change.commands[i],"a")[[1]],",")[[2]])
            nl <- seq(from =nl[1], to=nl[2])
            seqk  <- ol-min(ol)+1
            for(k in seqk){
                ch <- c(ol[k],nl[k],3)
                text1.info[ch[1],c("oldline","newline","change")] <- ch
                statement[ch[1]] <- change.commands[i]
                }
              ol <- max(ol)+1
              nl <- max(nl)+1
              if(ol<=length(text1.info[,1])){
              ch <- c(ol,nl,0)
              text1.info[ol,c("oldline","newline","change")] <- ch
                  }
            }
        }


    # from,to CHANGE from
                      # all 00,00c00 types
      if( length(grep("[0-9]+,[0-9]+c[0-9]+$",change.commands))!=0){ # if some exist
        for(i in grep("[0-9]+,[0-9]+c[0-9]+$",change.commands)){     # line numbers of them = i
            ol <- as.numeric(strsplit(strsplit(change.commands[i],"c")[[1]],",")[[1]])
            ol <- seq(from =ol[1], to=ol[2])
            nl <- as.numeric(strsplit(strsplit(change.commands[i],"c")[[1]],",")[[2]])
            nl <- seq(from =nl[1], to=nl[1])
            seqk  <- ol-min(ol)+1
            for(k in seqk){
                ch <- c(ol[k],nl[k],1)
                text1.info[ch[1],c("oldline","newline","change")] <- ch
                statement[ch[1]] <- change.commands[i]
                }
              ol <- max(ol)+1
              nl <- max(nl)+1
              if(ol<=length(text1.info[,1])){
              ch <- c(ol,nl,0)
              text1.info[ol,c("oldline","newline","change")] <- ch
                  }
            }
        }

                      # all 00,00d00 types
      if( length(grep("[0-9]+,[0-9]+d[0-9]+$",change.commands))!=0){ # if some exist
        for(i in grep("[0-9]+,[0-9]+d[0-9]+$",change.commands)){     # line numbers of them = i
            ol <- as.numeric(strsplit(strsplit(change.commands[i],"d")[[1]],",")[[1]])
            ol <- seq(from =ol[1], to=ol[2])
            nl <- as.numeric(strsplit(strsplit(change.commands[i],"d")[[1]],",")[[2]])
            nl <- seq(from =nl[1], to=nl[1])
            seqk  <- ol-min(ol)+1
            for(k in seqk){
                ch <- c(ol[k],nl[k],2)
                text1.info[ch[1],c("oldline","newline","change")] <- ch
                statement[ch[1]] <- change.commands[i]
                }
              ol <- max(ol)+1
              nl <- max(nl)+1
              if(ol<=length(text1.info[,2])){
              ch <- c(ol,nl,0)
              text1.info[ol,c("oldline","newline","change")] <- ch
                  }
            }
        }

                      # all 00,00a00 types
      if( length(grep("[0-9]+,[0-9]+a[0-9]+$",change.commands))!=0){ # if some exist
        for(i in grep("[0-9]+,[0-9]+a[0-9]+$",change.commands)){     # line numbers of them = i
            ol <- as.numeric(strsplit(strsplit(change.commands[i],"c")[[1]],",")[[1]])
            ol <- seq(from =ol[1], to=ol[2])
            nl <- as.numeric(strsplit(strsplit(change.commands[i],"c")[[1]],",")[[2]])
            nl <- seq(from =nl[1], to=nl[1])
            seqk  <- ol-min(ol)+1
            for(k in seqk){
                ch <- c(ol[k],nl[k],3)
                text1.info[ch[1],c("oldline","newline","change")] <- ch
                statement[ch[1]] <- change.commands[i]
                }
              ol <- max(ol)+1
              nl <- max(nl)+1
              if(ol<=length(text1.info[,1])){
              ch <- c(ol,nl,0)
              text1.info[ol,c("oldline","newline","change")] <- ch
                  }
            }
        }


  # filling other linenumbers (with no change)
   text1.info[,4]<-1:length(text1.info[,1])                   # fill oldline with index
   if(is.na(text1.info[1,6]))  text1.info[1,4:6] <- c(1,1,0)  # fill first line if empty

     for(i in 2:length(text1.info[,1]) ){
       if( sum(is.na(text1.info[i,5:6])) ==2 ){
           j <- i-1
           text1.info[i,5:6] <- c(text1.info[j,5]+1,0)
          }
        }

      while(length(text1.info[is.na(text1.info[,5]) & text1.info[,6]==0,][,4])!=0){
          iffer0 <- text1.info[is.na(text1.info[,5]) & text1.info[,6]==0,][,4]
          iffer1 <- iffer0 +1
          text1.info[iffer0,5] <- text1.info[iffer1,5] -1
         }


  # exporting the difference

  temp <- as.data.frame(cbind(1:length(text2),text2.info,text2))
    line.iffer <- text1.info[,5][text1.info[,6]!=0 & !is.na(text1.info[,5])]
    #temp[line.iffer,c("V1","aut2")] # all lines that changed
    expo <- temp[line.iffer,c("V1","text2")]
    colnames(expo) <- c("linenumber",file2)

    fshort1 <- substr(file1,1,nchar(file1)-4)
    fshort2 <- substr(file2,1,nchar(file2)-4)
    file.name.export <- paste("diff ",fshort1,fshort2,".txt",sep="")

  if(export==T){
      write.csv(expo,file=file.name.export,row.names=F)
    }



  # RETURN the results as invisible list object
  RETURN        <- list(file1,file2,text1,text2,text1.info,text2.info,expo)
  names(RETURN) <- c("fname1","fname2","text1","text2","info1","info2","diff1")

  invisible(RETURN)
}

*** function writing some coding scheme into a .rqda project

# ------------------------------------------------------------------------------
# a function writing some coding scheme into a .rqda-project  (from a list within R)
# ------------------------------------------------------------------------------

  # requires: OR() , stringr , get.sqllite() , windows (because if the user written into the table)
  #
  # option: coding = ""
  #           --> supply coding as list e.g. c("first coding", "second coding", "...")
  # option: csv = ""
  #           --> supply coding as csv with all columns in the table supplied
  # option: project = ""
  #           --> name of the rqda project file e.g. "test.rqda"
  #
  # option: categories = ""
  #           --> coding categories supplied as list in the form of:
  #               list( c("cat name 1",cid,cid) , c("cat name 2",cid,cid,...) )
  #               each list elements first entry is the category's name followed 
  #               by the codings belonging to this category
  # option: actualdate = T
  #           --> whether or not today's date should be written into table

rqda.write.cs <- function(coding="", csv="", project="", categories="", actualdate=T){
  dat <- coding
  get.sqlite()
  
  if(csv=="" & OR(coding!="")){
    # some checks
      if(exists("OR")==FALSE) return("Error !!! You need function OR() !!! ")
      if(OR(dat=="")) return("Error !!! You have to specify data !!! ")
      if(project=="")      return("Error !!! You have to specify the project !!! ")
  
    # gathering information  from data supplied by user
      if(OR(dat!="")){
          na <- rep(NA,length(dat))
            freecode <- data.frame(cbind(na,na,na,na,na,na,na,na))   # gen dataframe
            names(freecode) <- c("name","memo","owner","date","dateM","id","status","color") # gen column names
            tab.names <- names(freecode)
          freecode$name   <- dat
        }
  
  
    # add further information dat, id, owner
      if(actualdate==T) freecode$date <- date()     # the actual date
      freecode$id      <- 1:length(freecode[,1])  # making sure id is unique
      freecode$status  <- 1                        # 0=temporarily deleted
          k <- length(strsplit(Sys.getenv("HOMEPATH"),"\\\\")[[1]])
          owner <- strsplit(Sys.getenv("HOMEPATH"),"\\\\")[[1]][k]
      freecode$owner <- owner
  
    # writing into project / table
    tab           <- "freecode"
    droptable     <- paste("DROP TABLE ",tab,";",sep="")
    createtable1  <- paste("CREATE TABLE ", tab,sep="")
    createtable2  <- " (name text, memo text, owner text,date text,dateM text, id integer, status integer, color text);"
    tabins        <- paste("INSERT INTO ",tab,sep="")
    colu          <- "(name, memo, owner, date, dateM, id, status, color)"
  
  
    require(stringr)
        tbody <- freecode
        tab.names <- names(freecode)
  
    rqda.tabletosqlvalues <- function(tname=tab, line=1){
        values=""
        for( i in 1:length(tbody) ){
            values        <- paste(values, "'", freecode[line,i], "',",sep="")
            values <- str_replace(values,"('NA')","null")
           }
        values <- substr(values,1,nchar(values)-1)
        values        <-  paste("VALUES(",values,");",sep="")
  
        columnnames <- ""
        for( i in 1:length(tbody) ){
        columnnames <- paste(columnnames," , ",tab.names[i],sep="")
        }
        columnnames <- paste("(", substr(columnnames,4,nchar(values)) , ")",sep="" )
  
        valset <- c(paste("INSERT INTO ",tname,sep=""),columnnames,values)
        return(valset)
      }
  
    values <- rqda.tabletosqlvalues( tab , 1 )
     if( length( tbody[,1] ) > 1 ){
        for(i in 2:length( freecode[,1] )){
            values <- c(values, rqda.tabletosqlvalues( tab , i ) )
          }
      }
  
    sql <- c(droptable, createtable1, createtable2, values)
  }

  if(csv!="" & OR(coding=="")){
    # no date at all or both ???
      if(exists("OR")==FALSE) return("Error !!! You need functioin OR() !!! ")
      if(csv=="") return("Error !!! You have to specify data !!! ")
      if(project=="")      return("Error !!! You have to specify the project !!! ")
    # gathering information  from csv
      if(csv!=""){ # read in csv and save into freecode object
          freecode <- data.frame(cbind(NA,NA,NA,NA,NA,NA,NA,NA))   # gen dataframe
          names(freecode) <- c("name","memo","owner","date","dateM","id","status","color") # gen column names
          tab.names <- names(freecode)
          codetable <- read.csv(file=csv,head=TRUE,sep=";")[,1:8] # code-table data from csv
          freecode <- codetable
        }
  
    # add further information dat, id, owner
      if(actualdate==T) freecode$date <- date()     # the actual date
      freecode$id      <- 1:length(freecode[,1])  # making sure id is unique
      freecode$status  <- 1                        # 0=temporarily deleted
          k <- length(strsplit(Sys.getenv("HOMEPATH"),"\\\\")[[1]])
          owner <- strsplit(Sys.getenv("HOMEPATH"),"\\\\")[[1]][k]
      freecode$owner <- owner
  
    # assembling the sql statement
    tab           <- "freecode"
    droptable     <- paste("DROP TABLE ",tab,";",sep="")
    createtable1  <- paste("CREATE TABLE ", tab,sep="")
    createtable2  <- " (name text, memo text, owner text,date text,dateM text, id integer, status integer, color text);"
    tabins        <- paste("INSERT INTO ",tab,sep="")
    colu          <- "(name, memo, owner, date, dateM, id, status, color)"
  
  
    require(stringr)
        tbody <- freecode
        tab.names <- names(tbody)
    rqda.tabletosqlvalues <- function(tname=tab, line=1){
        values=""
        for( i in 1:length(tbody) ){
            values        <- paste(values, "'", tbody[line,i], "',",sep="")
            values <- str_replace(values,"('NA')","null")
           }
        values <- substr(values,1,nchar(values)-1)
        values        <-  paste("VALUES(",values,");",sep="")
  
        columnnames <- ""
        for( i in 1:length(tbody) ){
        columnnames <- paste(columnnames," , ",tab.names[i],sep="")
        }
        columnnames <- paste("(", substr(columnnames,4,nchar(values)) , ")",sep="" )
  
        valset <- c(paste("INSERT INTO ",tname,sep=""),columnnames,values)
        return(valset)
      }
  
    values <- rqda.tabletosqlvalues( tab , 1 )
    tbody  <- freecode
    if( length( tbody[,1] ) > 1 ){
        for(i in 2:length( freecode[,1] )){
            values <- c(values, rqda.tabletosqlvalues( tab , i ) )
          }
      }
  
    sql <- c(droptable, createtable1, createtable2, values)   # compose sql statements
  }



  writeLines(sql,"sqlCodeTable.sql")                   # export sql statements
   fil      <- "sqlCodeTable.sql"
   temp <- paste("sqlite3 ",project," < ",fil,sep="")  # compose shell statement
   shell(temp)                  # do sql
   unlink("sqlCodeTable.sql")   # delete temporary file
   

   # ------------
   # code categories
   if(OR(categories!="")){

     tab1         <- "codecat"
     droptable1  <- paste("DROP TABLE ",tab1,";",sep="")
     createtable11  <- "CREATE TABLE codecat  (name text, cid integer, catid integer, owner text, date text,"
     createtable12  <- "                      dateM text,memo text, status integer); "
     
     tab2         <- "treecode" 
     droptable2  <- paste("DROP TABLE ",tab2,";",sep="")
     createtable21  <- "CREATE TABLE treecode  (cid integer, catid integer,"
     createtable22  <- "                         owner text, date text, dateM text,"
     createtable23  <- "                         memo text, status integer);"


     # sql statement for code categories
     values1 <- ""
     for(i in 1:length(categories)){
       values1[length(values1)+1]  <- "INSERT INTO codecat"
       values1[length(values1)+1]  <- "(name, cid, catid, owner, date, dateM, memo, status) "
       values1[length(values1)+1]  <- paste("VALUES('",categories[[i]][1],"', null, '", i ,"', 'Peter', '",date(),"', null, null, '1');",sep="")
      }
        values1 <- values1[-1]   # correct for first blank assignement
     
      
     # sql statement for code categories linkage to codes
     values2 <- ""
     for(i in 1:length(categories)){
     for(k in 2:length(categories[[i]])){
       values2[length(values2)+1]  <- "INSERT INTO treecode"
       values2[length(values2)+1]  <- "(cid, catid, date, dateM, memo, status) "
       values2[length(values2)+1]  <- paste("VALUES('", categories[[i]][k] ,"', '", i ,"', '",date(),"', null, null, '1');",sep="")
      }
      }
        values2 <- values2[-1]   # correct for first blank assignement     
     
      
     sql1 <- c(droptable1, createtable11, createtable12, values1)   # compose sql statements
     writeLines(sql1,"sqlCategoriesTable.sql")                   # export sql statements
     fil      <- "sqlCategoriesTable.sql"
     temp <- paste("sqlite3 ",project," < ",fil,sep="")  # compose shell statement
     shell(temp)                  # do sql
     unlink("sqlCodeTable.sql")   # delete temporary file

     sql2 <- c(droptable2, createtable21, createtable22, createtable23, values2)   # compose sql statements
     writeLines(sql2,"sqlCategoriesLinkTable.sql")                   # export sql statements
     fil      <- "sqlCategoriesLinkTable.sql"
     temp <- paste("sqlite3 ",project," < ",fil,sep="")  # compose shell statement
     shell(temp)                  # do sql
     unlink("sqlCodeTable.sql")   # delete temporary file
 } # code categories end

} # funtion end  

*** function extracting the column names from some SQL create table statement

# ------------------------------------------------------------------------------
# a function exracting the column names from some SQL "Create Table"-statement
# ------------------------------------------------------------------------------

  # option: statement = ""
  #           --> some "Create Table"-SQL-Statement
  # option: Table 
  #           --> the name of the table as string

          cleanColnames <- function(statement="", Table){
            tempcol =""
            for(i in 1:length(statement)) {
                tempcol <- paste(tempcol, statement[i],sep="")
               }
            statement <- gsub("  ", "", tempcol)
            statement <- gsub(paste("CREATE TABLE ",Table,sep=""),"",statement)
            statement <- gsub("\\(","",statement)
            statement <- gsub("\\)","",statement)
            statement <- gsub(","," ",statement)
            statement <- gsub(" integer","",statement)
            statement <- gsub(" real","",statement)
            statement <- gsub(" text","",statement)
            statement <- gsub("  ", " ", statement)
            statement <- gsub("  ", " ", statement)
            statement <- gsub("  ", " ", statement)
            statement <- strsplit(statement,split=" ")[[1]]
            statement
          }

*** function selecting from text a part of it within specific margins

# ------------------------------------------------------------------------------
# a function selecting from text a part of it within specific margins
# ------------------------------------------------------------------------------

  # option: TEXT 	= ""
  #           --> the text to choose from
  # option: FROM 	= 0
  #           --> the first character
  # option: TO 		= 0
  # 		  --> the last character	
  # option: fullinfo = F
  # 		  --> should a dataframe be returned 
  #				  with additional information or only the text	
		  
text.select <- function(TEXT="", FROM=0, TO=0, fullinfo = F ){
	TEXT.line    <- 1:length(TEXT)
	TEXT.length  <- nchar(TEXT)
	TEXT.to      <- NULL
	TEXT.from	 <- NULL
	
	x <- 0
	for(i in 1:length(TEXT.line)){
		x <- TEXT.length[i] + x # running total
		TEXT.to[i]  <- x
		if(i==1)TEXT.from[i] <- 0 else TEXT.from[i] <- TEXT.to[i-1]
		}
	for(i in 1:length(TEXT.line)){
		TEXT.from[i] <- TEXT.from[i]+i-1
		TEXT.to[i] <- TEXT.to[i]+i-1
		}

	LINES <- max(TEXT.line[TEXT.from <= FROM]):min(TEXT.line[TEXT.to >= TO])	
	TEXT[min(LINES)] <- substr( TEXT[min(LINES)], (FROM - TEXT.from[min(LINES)]) , (TEXT.length[min(LINES)]) )
	TEXT[max(LINES)] <- substr( TEXT[max(LINES)], 0 , (TO-TEXT.from[max(LINES)]) )
	TEXT 		<- TEXT[LINES]
	
	RETURN <- data.frame(TEXT.line[LINES])
	RETURN$length <- TEXT.length[LINES]
	RETURN$from <- TEXT.from[LINES]
	RETURN$to   <- TEXT.to[LINES]
	RETURN$text <- TEXT
	
	names(RETURN) <- c("line","length","from","to","text")
	
	if(fullinfo==T) return(RETURN) else return(TEXT)
}

*** function returning the coding table of a certain rqda project file

# ------------------------------------------------------------------------------
# a function returning the coding table of a certain rqda-project-file
# ------------------------------------------------------------------------------

  # option: PName 	= ""
  #           --> the name of the rqda project file: e.g. "test.rqda"
  # option: infid 	= ""
  #           --> rtestriction on file ids to include; e.g. c(1,2,3)
  # option: incid 	= ""
  #           --> restriction on coding ids to include; e.g. c(1,2,3,4)
  # option: ctext 	= FALSE
  #           --> whether or not the seltext should be returned too
  # note: 	the function returns a dataframe or a list containing of a dataframe and a text-list 
  #			depending on whether or not ctext = T or F
  
rqda.get.coding <- function(PName="", infid="", incid="", ctext=F ){

		# count how many lines there are 
	
	   nlines 		<- as.integer(send.sqlite(PName, "SELECT count(seltext) FROM coding;"))
	   
	   if( nlines > 0 ){
       
		   # get coding table information
		   cid 		<- as.integer(	send.sqlite( PName, "SELECT cid 		 FROM coding;" 	) )
		   fid		<- as.integer(	send.sqlite( PName, "SELECT fid 		 FROM coding;" 	) )
		   selfirst	<- as.integer(	send.sqlite( PName, "SELECT selfirst   FROM coding;" 	) )
		   selend	<- as.integer(	send.sqlite( PName, "SELECT selend     FROM coding;" 	) )
		   status	<- as.integer(	send.sqlite( PName, "SELECT status 	 FROM coding;" 		) )
		   date		<- 				send.sqlite( PName, "SELECT date 	 FROM coding;" 		)
		   memo		<- 				send.sqlite( PName, "SELECT memo 	 FROM coding;" 		)

		   if(ctext==T){
		   coding.text <- list(NULL)
		   rowids <- as.integer(	send.sqlite( PName, "SELECT ROWID FROM coding;" 		) )
			counter <- 0
			for(x in rowids){
				counter <- counter + 1
				coding.text[[counter]] <- iconv(
						send.sqlite( PName, 
									 paste("SELECT seltext FROM coding WHERE rowid = '",x,"' ;",sep="") 
									 ) , from="UTF-8", to="Windows-1252")
			   }
			}
			
			# a table alike coding with information from the coding table from the rqda project 
			# except the coded text which is saved within coding.text
		   coding <- as.data.frame(cbind(cid,fid,selfirst,selend,status)) 
			coding[,length(coding[1,])+1] <- date  
			coding[,length(coding[1,])+1] <- memo 
			names(coding) <- c("cid","fid","selfirst","selend","status","date","memo")
		
		} else {
		return( NULL )
		}
	
	# applying the restrictions from cid and fid options
	if(AND(infid!=""))	coding <- coding[coding$fid %in% infid,]
	if(AND(incid!="")) 	coding <- coding[coding$cid %in% incid,]
	
	if(ctext==T){
	RETURN <- list("coding"=coding, "text"=coding.text)
	} else {
	RETURN <- coding
	}
	
	invisible(RETURN) 
}

*** function transferring codes from one file to the next if lines are equal according to diff

# ------------------------------------------------------------------------------
# a function tranfering codes from one file to the next 
# if lines are equal according to "diff.exe"
# ------------------------------------------------------------------------------

  # option: PROJECT	= ""
  #           --> 	the name of the rqda project file: e.g. "test.rqda"
  # option: FORWARD	= T
  #           --> 	should the transfer of codings be done from first file to last file 
  # 				in file-list of rqda project or the other way around?
  
rqda.transfer.codings <- function(PROJECT="" , FORWARD=T){

## FILE LIST and NUMBER OF FILES
FILES  <- as.integer(send.sqlite( PROJECT , "select id from source;"))
      if(FORWARD == F ) FILES  <- FILES[length(FILES):1]
	 

      
NFILES <- length(FILES)

## LOOP over Files
   if(NFILES > 1){                      # if(NFiles > 1){
       for(i in 1:(NFILES-1)){          # for(i in 1:(NFiles-1)){

# i <- 1 # Only for Test and DEVELOPEMENT
      # FILE 1 and FILE 2 for LOOPING OVER FILES
         FILE1  	<-  FILES[ 1 :(length(FILES )-1) ][i] #
         FILE2  	<-  FILES[ 2 : length(FILES )    ][i] #
		 
		 FILE1NAME  <-  send.sqlite( PROJECT, paste("SELECT name FROM source WHERE id ='",FILE1,"';",sep="") )
		 FILE2NAME  <-  send.sqlite( PROJECT, paste("SELECT name FROM source WHERE id ='",FILE2,"';",sep="") )

		 # a message
		 print(cat(c("\n","Transfering from",paste(FILE1NAME,".rqda to ",FILE2NAME,".rqda . ",sep=""),"\n")))
         
		 FILE1Text 	<-  iconv(
							send.sqlite(PROJECT, paste("SELECT file FROM source WHERE id = '",FILE1,"';",sep=""))
							,from="UTF-8",to="WINDOWS-1252")
		 writeLines(FILE1Text, "comp.temp1.txt") 
		 
		 FILE2Text 	<-  iconv(
							send.sqlite(PROJECT, paste("SELECT file FROM source WHERE id = '",FILE2,"';",sep=""))
							,from="UTF-8",to="WINDOWS-1252")
		 writeLines(FILE2Text, "comp.temp2.txt")
							
				
       # get coding table information
		coding1 <- rqda.get.coding(PROJECT, infid=FILE1)	
		if(is.null(coding1)) return("ERROR !!! There are no coded lines at all")
		
		# comparison of files
		comp <- compare("comp.temp1.txt","comp.temp2.txt")
		
		# checking for change in coded text
		change.iffer <- NULL
		for(y in 1:length(coding1[,1])){
			startline <- max( comp$info1[comp$info1[,"from"] <= coding1$selfirst[y],"oldline"] )
			endline   <- min( comp$info1[comp$info1[, "to" ] >= coding1$selend[y]  ,"oldline"] )
			change <- comp$info1[ comp$info1[,"oldline"]>=startline & comp$info1[,"oldline"]<=endline ,"change"]
			#print(change)
			change.iffer[length(change.iffer)+1] <- OR(change!=0)
			}
						
		coding1[,"change"] <- change.iffer
	
		# building coding2 (only non-changed codings)
		coding2 <- coding1[coding1$change==FALSE,c("cid","fid","selfirst","selend","status","date","memo")]
		coding2$fid  <- FILE2
		coding2$date <- date()
		coding2$memo <- paste("Coding automaticly copied from ",FILE1NAME," to ",FILE2NAME,sep="")
		
		# building coding2, add correct selfirst and selend
		for(y in 1:length(coding2[,1])){
			coding2[y, "startline"] <- max( comp$info1[comp$info1[,"from"] <= coding2$selfirst[y],"oldline"] )
			coding2[y, "endline"  ] <- min( comp$info1[comp$info1[, "to" ] >= coding2$selend[y]  ,"oldline"] )
			coding2[y, "firstchar"] <- comp$info1[ comp$info1[,"oldline"]==coding2$startline[y],"from"] 
			coding2[y, "lastchar" ] <- comp$info1[ comp$info1[,"oldline"]==coding2$endline[y],"to"]
			}
		
		coding2$startdiff 	<- coding2$firstchar 	- 	coding2$selfirst
		coding2$enddiff 	<- coding2$selend		-	coding2$lastchar
	
			comp$info2 <- as.data.frame(comp$info2)
			comp$info2[,"newline"] <- 1:length(comp$info2[,1])
			
		for(y in 1:length(coding2[,1])){
			coding2[y, "startline"] <- comp$info1[ comp$info1[,"oldline"]==coding2$startline[y],"newline"] 
			coding2[y, "endline"]   <- comp$info1[ comp$info1[,"oldline"]==coding2$endline[y]  ,"newline"]
			coding2[y, "selfirst"]  <- comp$info2[ comp$info2[,"newline"]==coding2$startline[y],"from" ] + coding2$startdiff[y]
			coding2[y, "selend"  ]  <- comp$info2[ comp$info2[,"newline"]==coding2$endline[y]  , "to"  ] - coding2$enddiff[y]
			}
			
		
		# Extract coded text
			coding1.text <- list(NULL)
			temp <- coding1[coding1[,"change"]==FALSE,]
		
			for(u in 1:length(temp[,1])){
				from <- temp[u,"selfirst"]
				to   <- temp[u,"selend"]
				coding1.text[[u]] <- text.select(FILE1Text, FROM = from , TO = to)
				}
		
			coding2.text <- list(NULL)
			temp <- coding2
		
			for(u in 1:length(temp[,1])){
				from <- temp[u,"selfirst"]
				to   <- temp[u,"selend"]
				coding2.text[[u]] <- text.select(FILE2Text, FROM = from , TO = to)
				}
				
			if(AND(unlist(coding1.text) == unlist(coding2.text))==FALSE) return("ERROR !!! The to be transfered text is from file 1 and 2 is not equal !!!")

			
		# clean up coding2
		coding2 <- coding2[,c("cid","fid","selfirst","selend","status","date","memo")]
			
		
           
         # writing back to project
          # be careful to not overwrite anything !!!
        cid2       <- coding2$cid
        fid2       <- coding2$fid
        seltext2   <- coding2.text
        selfirst2  <- coding2$selfirst
        selend2    <- coding2$selend
        status2    <- coding2$status
        owner2     <- rep("Peter", length(cid2))
        date2      <- coding2$date
        memo2      <- coding2$memo

       # querry if these codings already exist or not, if they do they do not have to be written into the file!
        Iffer <- TRUE    # not already there?
          for(u in 1:length(cid2)){
              sql <- paste("SELECT * FROM coding WHERE ",
                      "selfirst = '"    ,selfirst2[u],
                      "' AND selend = '",selend2[u],
                      "' AND fid = '"   ,fid2[u],
                      "' AND cid = '"   ,cid2[u],
                      "' ;",sep="")
                      #print(sql)
              send.sqlite( PROJECT ,sql)
              Iffer[u] <- length(send.sqlite( PROJECT ,sql))==0
            }

         cid2       <- cid2[Iffer]
         fid2       <- fid2[Iffer]
         seltext2   <- seltext2[Iffer]
         selfirst2  <- selfirst2[Iffer]
         selend2    <- selend2[Iffer]
         status2    <- status2[Iffer]
         owner2     <- owner2[Iffer]
         date2      <- date2[Iffer]
         memo2      <- memo2[Iffer]
         

         # writing it all back
         if(length(seltext2) > 0 ){           # if(length(seltext2) > 0 )
            # building the sql-temp file
            sql <- NULL
            insert  <- "INSERT INTO coding"
            columns <- "(cid, fid, seltext, selfirst, selend, status, owner, date, memo)"
            for( u in 1:length(cid2)){
                sql[length(sql)+1] <- insert
                sql[length(sql)+1] <- columns
                sql[length(sql)+1] <- paste("VALUES('",cid2[u],"', '",fid2[u],"',",sep="")
                Text <- seltext2[[u]]
                  for(k in 1:length(Text)){
                    if(k==1)                    sql[length(sql)]    <- paste(sql[length(sql)]," '",Text[k],sep="")
                    if(k!=1)  					sql[length(sql)+1]  <- Text[k]
                    if(k==length(Text))         sql[length(sql)]    <- paste(sql[length(sql)],"', ",sep="")
                  }
                sql[length(sql)+1] <- paste(
                        "'",selfirst2[u],"', ","'",selend2[u],"', "
                        ,"'1', " ,"'",owner2[u],"', " ,
                        "'",date2[u],"', ","'",memo2[u],"');",sep="")
  
              }

   
            # switch encoding to UTF-8 (format of rqda-projects)
            sql <- iconv(sql, from = "WINDOWS-1252", to = "UTF-8", sub = NA, mark = TRUE)
             
            # sending to sql
            send.sqlite( PROJECT ,sql,keep=F)
			
			print(cat("\n",length(seltext2)," codings transfered.\n"))
          
          } else {  # END: if(length(seltext2) > 0 )
			print(cat("\n",length(seltext2)," codings transfered.\n"))	
		  }
        ###     ####
         }                              # END: for(i in 1:(NFiles-1)){
      }                                 # END: if(NFiles > 1){
## END: LOOP over Files
 }
** QDA descriptive stats

#+srcname: pres-docs-qda-test
#+begin_src R
#############################################
#
# Descriptive statistics of Case Attributes
#
#############################################
# load libraries
#
library("gtools")
library("ggplot2")

# remove old objects, close plot windows
#
rm(list = ls())
graphics.off()

# define variables
#
sql <- "select name from attributes where class='numeric'"
temp <- RQDAQuery(sql)
DF.1 <- GetAttr(type = "case", attrs = temp$name)
DF.2 <- subset(DF.1, select=-case)
DF.3 <- melt.data.frame(DF.2, id.vars="caseID", variable_name="case.attr")

#+end_src
